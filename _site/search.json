[
  {
    "objectID": "content/01-content.html",
    "href": "content/01-content.html",
    "title": "Introduction and Set Up",
    "section": "",
    "text": "Garrick Aden-Buie’s macOS set up blogpost\n Matti Vuorre’s macOS set up blogpost\n\n\n\n Installation and Connect Git, GitHub, RStudio sections in Jennifer Bryan’s Happy Git and GitHub for the useR\n\n\n\n Install or upgrade R and RStudio chapter in Jennifer Bryan’s Happy Git and GitHub for the useR"
  },
  {
    "objectID": "content/01-content.html#slides",
    "href": "content/01-content.html#slides",
    "title": "Introduction and Set Up",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "content/01-content.html#in-class",
    "href": "content/01-content.html#in-class",
    "title": "Introduction and Set Up",
    "section": "In-class",
    "text": "In-class"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GVPT622",
    "section": "",
    "text": "Quantitative Methods for Political Science\n        \n        \n            An introduction to research methods and quantitative research in political science.\n        \n        \n            Fall 2023Department of Government and PoliticsUniversity of Maryland, College Park\n        \n    \n    \n      \n        \n        \n        \n      \n    \n\n\n\n\n\n\nProfessor\n\n   Dr David Cunningham\n   dacunnin@umd.edu\n\n\n\nTeaching Assistant\n\n   Harriet Goers\n   Chincoteague Building\n   hgoers@umd.edu\n   hgoers\n\n\n\n\n\nCourse details\n\n   August 28 - 11 December\n   Monday, 12:30 - 3:15 PM\n   Tydings Building, Room 1111\n\n\n\nLab details\n\n   Friday, 3:00 - 5:00 PM\n   LeFrak Hall, Room 0227\n\n\n\n\nContacting me\nE-mail is the best ways to get in contact with me. I will try to respond to all course-related e-mails within 24 hours."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "DATE \n    TITLE \n    CLASS NOTES \n    SESSION SCRIPTS \n    ASSIGNMENTS \n  \n \n\n  \n    X \n    Course Introduction \n     \n     \n     \n  \n  \n    Y \n    Descriptive Statistics"
  },
  {
    "objectID": "content/01-content.html#class-slides",
    "href": "content/01-content.html#class-slides",
    "title": "Course Introduction",
    "section": "Class slides",
    "text": "Class slides"
  },
  {
    "objectID": "content/01-content.html#session-scipts",
    "href": "content/01-content.html#session-scipts",
    "title": "Course Introduction",
    "section": "Session scipts",
    "text": "Session scipts"
  },
  {
    "objectID": "content/02-content.html#class-slides",
    "href": "content/02-content.html#class-slides",
    "title": "Descriptive Statistics",
    "section": "Class slides",
    "text": "Class slides"
  },
  {
    "objectID": "content/02-content.html#session-scripts",
    "href": "content/02-content.html#session-scripts",
    "title": "Descriptive Statistics",
    "section": "Session scripts",
    "text": "Session scripts"
  },
  {
    "objectID": "content/01-content.html#session-scripts",
    "href": "content/01-content.html#session-scripts",
    "title": "Course Introduction",
    "section": "Session scripts",
    "text": "Session scripts"
  },
  {
    "objectID": "content/02-content.html#session",
    "href": "content/02-content.html#session",
    "title": "Descriptive Statistics",
    "section": "Session",
    "text": "Session\n\nPrerequisites\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(skimr)\nlibrary(patchwork)\n\n\n\nDescribing categorical variables\nImagine that we have completed a survey of 10,000 individuals. We asked them their age and level of satisfaction with their job.\n\n\n\nWe store their responses in a data frame called survey_df. It has 10,000 observations (one for each respondent) and three variables: a unique id (id); their age in years (age); and their level of satisfaction (sat), which can take one of four values: very unsatisfied, unsatisfied, satisfied, and very satisfied.\n\nsurvey_df\n\n# A tibble: 10,000 × 3\n      id   age sat             \n   <int> <int> <chr>           \n 1     1    20 Unsatisfied     \n 2     2    27 Very unsatisfied\n 3     3    42 Very satisfied  \n 4     4    20 Very unsatisfied\n 5     5    37 Unsatisfied     \n 6     6    26 Satisfied       \n 7     7    60 Very satisfied  \n 8     8    63 Very unsatisfied\n 9     9    42 Satisfied       \n10    10    40 Very unsatisfied\n# ℹ 9,990 more rows\n\n\n\nFrequency distribution\nWe can take advantage of janitor::tabyl() to quickly calculate the number and proportion of respondents who provided each level of satisfaction.\n\ntabyl(survey_df, sat)\n\n              sat    n percent\n        Satisfied 2579  0.2579\n      Unsatisfied 2414  0.2414\n   Very satisfied 2508  0.2508\n Very unsatisfied 2499  0.2499\n\n\nAlternatively, we can use skimr::skim() to get a useful summary of this categorical variable.\n\nskim(survey_df$sat)\n\n\nData summary\n\n\nName\nsurvey_df$sat\n\n\nNumber of rows\n10000\n\n\nNumber of columns\n1\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ndata\n0\n1\n9\n16\n0\n4\n0\n\n\n\n\n\n\n\nVisualizing this frequency\nWe can easily visualize this using a bar chart.\n\np1 <- survey_df |> \n  tabyl(sat) |> \n  ggplot(aes(x = n, y = sat)) + \n  geom_col() + \n  theme_minimal()\n\np2 <- survey_df |> \n  tabyl(sat) |> \n  ggplot(aes(x = percent, y = sat)) + \n  geom_col() + \n  theme_minimal()\n\np1 | p2\n\n\n\n\n\n\nWorking with factors\nNotice how our categories are ordered: very satisfied sits above satisfied. We can tell R this information factor().\n\nsurvey_df <- survey_df |> \n  mutate(\n    sat = factor(sat, levels = c(\"Very unsatisfied\", \n                                 \"Unsatisfied\", \n                                 \"Satisfied\", \n                                 \"Very satisfied\"))\n  )\n\nNow when we work with our categorical variables, they will be ordered.\n\nsurvey_df |> \n  tabyl(sat) |> \n  ggplot(aes(x = n, y = sat)) + \n  geom_col() + \n  theme_minimal()\n\n\n\n\n\n\n\nDescribing continuous variables\nWe can also get a good sense of our continuous variable age by looking at the center, spread, and shape of its distribution.\n\nFive number summary\nWe can use skimr::skim() to quickly get useful information on our continuous variable.\n\nskim(survey_df$age)\n\n\nData summary\n\n\nName\nsurvey_df$age\n\n\nNumber of rows\n10000\n\n\nNumber of columns\n1\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ndata\n0\n1\n39.96\n14.79\n15\n27\n40\n53\n65\n▇▇▇▇▇\n\n\n\n\n\n\n\nHistogram\n\nggplot(survey_df, aes(x = age)) + \n  geom_histogram() + \n  theme_minimal()\n\n\n\n\n\nggplot(survey_df, aes(x = age)) + \n  geom_histogram(binwidth = 5) + \n  theme_minimal()\n\n\n\n\n\n\nDensity curves\n\nggplot(survey_df, aes(x = age)) + \n  geom_density() + \n  theme_minimal()\n\n\n\n\n\n\nBox and whisker plots\n\nggplot(survey_df, aes(x = age)) + \n  geom_boxplot() + \n  theme_minimal()\n\n\n\n\n\n\nLooking for patterns in our groups\n\nggplot(survey_df, aes(x = age, y = sat)) + \n  geom_boxplot() + \n  theme_minimal()\n\n\n\n\n\nggplot(survey_df, aes(x = age, y = sat)) + \n  geom_violin() + \n  theme_minimal()\n\n\n\n\n\n\n\nUnderstanding distributions\n\nNormal distribution\n\ntibble(z = rnorm(n = 1000)) |> \n  ggplot(aes(x = z)) + \n  geom_histogram() + \n  theme_minimal()\n\n\n\n\n\ntibble(z = rnorm(n = 1e6)) |> \n  ggplot(aes(x = z)) + \n  geom_histogram() + \n  theme_minimal()\n\n\n\n\n\n\nRight skewed distribution\n\ntibble(z = rbeta(10000, 2, 10)) |> \n  ggplot(aes(x = z)) + \n  geom_histogram() + \n  theme_minimal()\n\n\n\n\n\n\nLeft skewed distribution\n\ntibble(z = rbeta(10000, 10, 2)) |> \n  ggplot(aes(x = z)) + \n  geom_histogram() + \n  theme_minimal()\n\n\n\n\n\n\n\nMeasures of central tendency: mean, median, and mode\n\nMean\nThe mean is the average of all values.\n\n\nMedian\nThe median is the mid-point of all values.\n\n\nMode\nThe mode is the most frequent of all values.\n\n\nUsing central tendency to describe and understand distributions\nNormally distributed vectors share their mean and medians.\n\nnorm_dist <- tibble(z = rnorm(n = 1000))\n\nggplot(norm_dist, aes(x = z)) + \n  geom_histogram() + \n  geom_vline(xintercept = mean(norm_dist$z), colour = \"red\") + \n  geom_vline(xintercept = median(norm_dist$z), colour = \"blue\") + \n  theme_minimal()\n\n\n\n\nFor right skewed data, the mean is greater than the median.\n\nright_dist <- tibble(z = rbeta(10000, 2, 10))\n\nggplot(right_dist, aes(x = z)) + \n  geom_histogram() + \n  geom_vline(xintercept = mean(right_dist$z), colour = \"red\") + \n  geom_vline(xintercept = median(right_dist$z), colour = \"blue\") + \n  theme_minimal()\n\n\n\n\nFor left skewed data, the mean is smaller than the median.\n\nleft_dist <- tibble(z = rbeta(10000, 10, 2))\n\nggplot(left_dist, aes(x = z)) + \n  geom_histogram() + \n  geom_vline(xintercept = mean(left_dist$z), colour = \"red\") + \n  geom_vline(xintercept = median(left_dist$z), colour = \"blue\") + \n  theme_minimal()\n\n\n\n\n\n\n\nMeasures of spread: range, variance, and standard deviation\n\nRange\nThe range is the difference between the largest and smallest value.\n\nmax(survey_df$age) - min(survey_df$age)\n\n[1] 50\n\n\n\n\nVariance\nThe variance measures how spread out your values are. Take a look at these two plots. Both have the same center point (0) and number of observations (1,000,000). However, the data are much more spread out around that center point in the top graph.\n\nwide_dist <- tibble(z = rnorm(1e6, sd = 2))\n\np1 <- ggplot(wide_dist, aes(x = z)) + \n  geom_histogram() + \n  geom_vline(xintercept = 0) + \n  theme_minimal() + \n  scale_x_continuous(limits = c(-4, 4))\n\nnarrow_dist <- tibble(z = rnorm(1e6, sd = 1))\n\np2 <- ggplot(narrow_dist, aes(x = z)) + \n  geom_histogram() + \n  geom_vline(xintercept = 0) + \n  theme_minimal() + \n  scale_x_continuous(limits = c(-4, 4))\n\np1 / p2\n\n\n\n\nThe data in the top graph have far more variance than those in the bottom graph. We measure this by calculating the average of the squares of the deviations of the observations from their mean.\n\\[\ns^2 = \\frac{\\Sigma(x_i - \\bar{x})^2}{n - 1}\n\\]\nLet’s step through this. We will first calculate the variance for wide_dist, or the top graph.\n\nwide_var_calc <- wide_dist |> \n  mutate(\n    mean = mean(wide_dist$z),\n    diff = z - mean,\n    diff_2 = diff^2\n  )\n\nwide_var_calc\n\n# A tibble: 1,000,000 × 4\n         z    mean    diff  diff_2\n     <dbl>   <dbl>   <dbl>   <dbl>\n 1 -0.921  0.00268 -0.924  0.853  \n 2  1.63   0.00268  1.63   2.65   \n 3 -0.0416 0.00268 -0.0443 0.00196\n 4  0.779  0.00268  0.777  0.603  \n 5  0.494  0.00268  0.492  0.242  \n 6 -0.738  0.00268 -0.740  0.548  \n 7 -0.130  0.00268 -0.133  0.0176 \n 8 -1.58   0.00268 -1.58   2.51   \n 9  1.82   0.00268  1.82   3.30   \n10 -0.127  0.00268 -0.129  0.0167 \n# ℹ 999,990 more rows\n\n\nWe take the sum of square of the difference between each observation and the mean of our whole sample. We then divide that by one less than our number of observations.\n\nwide_var <- sum(wide_var_calc$diff_2) / (nrow(wide_var_calc) - 1)\n\nwide_var\n\n[1] 4.00025\n\n\nWe can compare this to the variance for our narrower distribution.\n\nnarrow_var_calc <- narrow_dist |> \n  mutate(\n    mean = mean(narrow_dist$z),\n    diff = z - mean,\n    diff_2 = diff^2\n  )\n\nnarrow_var <- sum(narrow_var_calc$diff_2) / (nrow(narrow_var_calc) - 1)\n\nnarrow_var\n\n[1] 1.000964\n\n\nIt is, in fact, smaller!\nWe can use var() to do this in one step:\n\nvar(wide_dist)\n\n        z\nz 4.00025\n\n\n\nvar(narrow_dist)\n\n         z\nz 1.000964\n\n\n\n\nStandard deviation\nA simpler measure of spread is the standard deviation. It is simply the square root of the variance.\n\nsqrt(wide_var)\n\n[1] 2.000063\n\n\n\nsqrt(narrow_var)\n\n[1] 1.000482\n\n\nIf you look back to our graphs, you will see that I set the standard deviations explicitly when I generated the data. rnorm() takes an sd argument. This is great because we can confirm that the standard deviations for the wide and narrow distributions are 2 and 1 respectively (with a little bit of noise).\n\ntibble(\n  n = rnorm(1e6, sd = 1),\n  w = rnorm(1e6, sd = 2)\n) |> \n  ggplot() + \n  geom_density(aes(x = n), colour = \"green\") + \n  geom_density(aes(x = w), colour = \"lightblue\") + \n  theme_minimal()\n\n\n\n\n\n\n\nStandardization\nNotice how our description of each variable depends on its units of measurement. What do we do if we want to compare across different measurements that have different units?\n\nZ scores\nFor normal distributions, we can use the z score. This gives us a standard way of understanding how many standard deviations from the mean of a normally distributed variable a value is.\n\\[\nz_i = \\frac{x_i - \\mu_x}{\\sigma_x}\n\\]"
  },
  {
    "objectID": "content/03-content.html#class-slides",
    "href": "content/03-content.html#class-slides",
    "title": "Bivariate Relationships",
    "section": "Class slides",
    "text": "Class slides"
  },
  {
    "objectID": "content/03-content.html#section",
    "href": "content/03-content.html#section",
    "title": "Bivariate Relationships",
    "section": "Section",
    "text": "Section\n\nPrerequisites\n\nlibrary(tidyverse)\nlibrary(wbstats)\nlibrary(poliscidata)\nlibrary(countrycode)\nlibrary(broom)\nlibrary(janitor)\nlibrary(ggridges)\n\nToday, we will explore the relationship between wealth and health. This question was made popular by Hans Rosling’s Gapminder project.\nFirst, we need to collect our data. We will use wbstats::wb_data() to pull these data directly from the World Bank.\n\ngapminder_df <- wb_data(\n  indicator = c(\"SP.DYN.LE00.IN\", \"NY.GDP.PCAP.CD\"),\n  start_date = 2016,\n  end_date = 2016\n) |> \n  rename(\n    life_exp = SP.DYN.LE00.IN,\n    gdp_per_cap = NY.GDP.PCAP.CD\n  ) |> \n  mutate(\n    log_gdp_per_cap = log(gdp_per_cap),\n    region = countrycode(country, \"country.name\", \"region\", custom_match = c(\"Turkiye\" = \"Europe & Central Asia\"))\n  ) |> \n  relocate(region, .after = country)\n\ngapminder_df\n\n# A tibble: 217 × 8\n   iso2c iso3c country         region  date gdp_per_cap life_exp log_gdp_per_cap\n   <chr> <chr> <chr>           <chr>  <dbl>       <dbl>    <dbl>           <dbl>\n 1 AW    ABW   Aruba           Latin…  2016      28451.     75.6           10.3 \n 2 AF    AFG   Afghanistan     South…  2016        520.     63.1            6.25\n 3 AO    AGO   Angola          Sub-S…  2016       1710.     61.1            7.44\n 4 AL    ALB   Albania         Europ…  2016       4124.     78.9            8.32\n 5 AD    AND   Andorra         Europ…  2016      39932.     NA             10.6 \n 6 AE    ARE   United Arab Em… Middl…  2016      41055.     79.3           10.6 \n 7 AR    ARG   Argentina       Latin…  2016      12790.     76.3            9.46\n 8 AM    ARM   Armenia         Europ…  2016       3680.     74.7            8.21\n 9 AS    ASM   American Samoa  East …  2016      13301.     NA              9.50\n10 AG    ATG   Antigua and Ba… Latin…  2016      15863.     78.2            9.67\n# ℹ 207 more rows\n\n\n\n\nVisualizing bivariate relationships: two continuous variables\n\nggplot(gapminder_df, aes(x = gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  theme_minimal()\n\n\n\n\nThere seems to be a very strong case that there is a relationship between a country’s GDP per capita (wealth) and its average life expectancy (health).\nBecause we want to explore linear relationships at this stage of the course, we will look at the logged GDP per capita variable:\n\nggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  theme_minimal()\n\n\n\n\nI can imagine drawing a straight line between these points that roughly explains how they vary with each other. It appears that as a country’s logged GDP per capita increases, so too does the life expectancy for its population.\n\n\nCorrelations\nWe can measure the strength of this association using correlations. The correlation coefficient tells us how closely variables relate to one another. It tells us both the strength and direction of the association.\n\nStrength: how closely are these values tied to one another. Measured from 0 to |1|, with values closer to 0 indicating a very weak relationship and values closer to |1| indicating a very strong relationship.\nDirection: do both \\(X\\) and \\(Y\\) change in the same direction? Positive correlations show that when \\(X\\) increases (decreases), so too does \\(Y\\). Negative correlations show that when \\(X\\) increases (decreases), \\(Y\\) decreases (increases). In other words, the move in different directions.\n\nWhat is the correlation between logged GDP per capita and life expectancy?\n\ncor(gapminder_df$log_gdp_per_cap, gapminder_df$life_exp, use = \"complete.obs\")\n\n[1] 0.8475933\n\n\nAs expected, the relationship is positive and strong.\n\n\nVisualizing the linear relationship between two continuous variables\nLet’s draw a line across all of these data points and try to understand how life expectancy increases with logged GDP per capita.\n\nggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  theme_minimal()\n\n\n\n\nWe can, of course, draw many different lines across these points. Which is the best line to draw?\nThis course focuses on ordinary least squares (OLS) regression. Simply put, OLS regression draws the line that minimizes the distance between itself and all of the dots.\nLet’s step through this. Look at the graph above.\n\nDraw a line through those dots.\nCalculate the distance between each dot and the line.\nSum up the absolute values of those distances. Remember, we just care about the distance, so we don’t need to worry about whether or not the dots are above or below the line.\nRepeat steps 1 - 3 many, many, many times.\nPick the line with the smallest sum of distances.\n\nPhew, this seems tedious and imprecise. Happily, maths and R are to the rescue. Here is the line that minimizes those distances (all with the addition of one extra line of code).\n\nggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = F) + \n  theme_minimal()\n\n\n\n\n\n\nEstimating a linear model in R\nHow can we find this line? To answer this, we will first do some review.\nRemember the general equation for a line:\n\\[\ny = a + mx\n\\]\nRead this as: the value of \\(y\\) is the sum of some constant, \\(a\\), and some \\(x\\) variable that has been transformed by some slope value \\(m\\).\n\nRemember that the slope constant, \\(m\\), tells you how much \\(y\\) changes for every one unit increase in \\(x\\).\n\nSo, if:\n\\[\ny = 10 + 2x\n\\]\nThen, when \\(x = 20\\):\n\\[\ny = 10 + 2*20 = 50\n\\]\nFor many values of \\(x\\):\n\nggplot(tibble(x = 1:50, y = 10 + 2*x), aes(x = x, y = y)) + \n  geom_line(colour = \"lightgrey\", size = 3) + \n  geom_point() + \n  theme_minimal()\n\n\n\n\nWell, let’s substitute in our variables of interest. Our \\(y\\) variable is the life expectancy and our \\(x\\) variable is the logged GDP per capita.\n\\[\nlife Exp_x = \\beta_0 + \\beta_1 logGdpPerCap_x + \\epsilon\n\\]\nRead this as: the life expectancy of some country, \\(x\\), is a function of some constant (\\(\\beta_0\\)) and its logged GDP per capita transformed by some value \\(\\beta_1\\) with some random error (\\(\\epsilon\\)).\nLet’s imagine that this relationship is accurately described by the following formula:\n\\[\nlife Exp_x = 30 + 4 * logGdpPerCap_x\n\\]\n\nWe will get to that pesky error term in a bit.\n\nThen, our model would predict the following average life expectancy for countries with log GDPs per capita between 0 and 20:\n\nggplot(\n  tibble(log_gdp_per_cap = 1:20, life_exp = 30 + 4*log_gdp_per_cap), \n  aes(x = log_gdp_per_cap, y = life_exp)\n) + \n  geom_point() + \n  theme_minimal()\n\n\n\n\nA country with a logged GDP per capita of 5 (the equivalent of a GDP per capita of $148.41) has a predicted average life expectancy of 50 years, or \\(30 + 4*5\\).\nA country with a logged GDP per capita of 10 (the equivalent of a GDP per capita of $22,026.47) has a predicted average life expectancy of 70 years, or \\(30 + 4*10\\).\nDoes this accurately describe what we see in our data? What is the average life expectancy for countries with roughly $22,000 GDP per capita?\n\ncountries_10 <- gapminder_df |> \n  filter(gdp_per_cap > 21000 & gdp_per_cap < 23000)\n\ncountries_10\n\n# A tibble: 3 × 8\n  iso2c iso3c country          region  date gdp_per_cap life_exp log_gdp_per_cap\n  <chr> <chr> <chr>            <chr>  <dbl>       <dbl>    <dbl>           <dbl>\n1 BH    BHR   Bahrain          Middl…  2016      22867.     79.6           10.0 \n2 KN    KNA   St. Kitts and N… Latin…  2016      21095.     71.7            9.96\n3 SI    SVN   Slovenia         Europ…  2016      21678.     81.2            9.98\n\n\nWe predicted 70 years, but our data suggest that these countries have closer to an average of 77 years. Why do we have this difference?\nWell, we probably haven’t produced the best model we can. We just picked those numbers out of thin air. We will find the OLS regression model shortly. We might get closer to the observed value with that model. Let’s see.\n\nHow do we calculate the constant (\\(\\beta_0\\)) using OLS regression?\nRemember, OLS regression simply finds the line that minimizes the distance between itself and all the data points.\nIt turns out that the constant that minimizes this distance is the mean of \\(Y\\) minus \\(\\beta_1\\) times the mean of \\(X\\).\nSo, the constant that best predicts the life expectancy of a country based on its logged GDP per capita is equal to the average life expectancy across our sample minus the average logged GDP per capita transformed by \\(\\beta_1\\).\nSo…\n\n\nHow do we calculate the coefficient \\(\\beta_1\\)?\nThe regression slope is the correlation coefficient between \\(X\\) and \\(Y\\) multiplied by the standard deviation of \\(Y\\) divided by the standard deviation of \\(X\\).\nEw… Let’s step through that.\nRemember, the correlation coefficient simply measures how \\(X\\) and \\(Y\\) change together. Does \\(Y\\) increase when \\(X\\) increases? How strong is this relationship?\nThe standard deviations of \\(X\\) and \\(Y\\) just measure how spread out they are.\nBringing these together, we are interested in how much \\(X\\) and \\(Y\\) change together moderated by how much they change independently.\nFormally:\n\\[\n\\beta_1 = (\\frac{\\Sigma(\\frac{x_i - \\bar{x}}{s_X})(\\frac{y_i - \\bar{y}}{s_Y})}{n - 1})(\\frac{s_Y}{s_X}) = \\frac{\\Sigma(x_i - \\bar{x})(y_i - \\bar{y})}{\\Sigma(x_i - \\bar{x})^2}\n\\]\nHappily R does all of this for us:\n\nm <- lm(life_exp ~ log_gdp_per_cap, data = gapminder_df)\n\nm\n\n\nCall:\nlm(formula = life_exp ~ log_gdp_per_cap, data = gapminder_df)\n\nCoefficients:\n    (Intercept)  log_gdp_per_cap  \n         32.927            4.509  \n\n\nOkay, so the line of best fit describing the relationship between life expectancy and logged GDP per capita is:\n\\[\nlife Exp_x = 32.9 + 4.5 * logGdpPerCap_x + \\epsilon\n\\]\n\n\n\nPrediction and performance\nWe can use this model to predict a country’s life expectancy given its GDP per capita.\nbroom::tidy(m) makes this model object a lot easier to work with.\n\ntidy(m)\n\n# A tibble: 2 × 5\n  term            estimate std.error statistic  p.value\n  <chr>              <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)        32.9      1.77       18.6 1.24e-45\n2 log_gdp_per_cap     4.51     0.200      22.6 6.07e-57\n\n\nWhat is the life expectancy for a country with a GDP per capita of $10,000? First, let’s find the estimated constant (or intercept or \\(\\beta_0\\)).\n\nm_res <- tidy(m)\n\nbeta_0 <- m_res |> \n  filter(term == \"(Intercept)\") |> \n  pull(estimate)\n\nbeta_0\n\n[1] 32.92739\n\n\nThen we need to find the estimated coefficient for (logged) GDP per capita:\n\nbeta_1 <- m_res |> \n  filter(term == \"log_gdp_per_cap\") |> \n  pull(estimate)\n\nbeta_1\n\n[1] 4.508875\n\n\nFinally, we can plug this in to our model:\n\nlife_exp_10000 <- beta_0 + beta_1 * log(10000)\nlife_exp_10000\n\n[1] 74.45566\n\n\nA country with a GDP per capita of $10,000 is predicted to have an average life expectancy of 74 years. Does this make sense with our data?\n\nggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  geom_vline(xintercept = log(10000)) + \n  geom_hline(yintercept = life_exp_10000) + \n  geom_smooth(method = \"lm\", se = F) + \n  theme_minimal()\n\n\n\n\nWe can predict values from a model using broom::augment():\n\naugment(m, newdata = tibble(log_gdp_per_cap = log(10000)))\n\n# A tibble: 1 × 2\n  log_gdp_per_cap .fitted\n            <dbl>   <dbl>\n1            9.21    74.5\n\n\nWe can do this across a number of different values for GDP per capita:\n\nnew_data <- tibble(\n  gdp_per_cap = seq(from = 1000, to = 50000, by = 1000),\n  log_gdp_per_cap = log(gdp_per_cap)\n)\n\naugment(m, newdata = new_data)\n\n# A tibble: 50 × 3\n   gdp_per_cap log_gdp_per_cap .fitted\n         <dbl>           <dbl>   <dbl>\n 1        1000            6.91    64.1\n 2        2000            7.60    67.2\n 3        3000            8.01    69.0\n 4        4000            8.29    70.3\n 5        5000            8.52    71.3\n 6        6000            8.70    72.2\n 7        7000            8.85    72.8\n 8        8000            8.99    73.4\n 9        9000            9.10    74.0\n10       10000            9.21    74.5\n# ℹ 40 more rows\n\n\nLet’s look at that line:\n\nggplot(augment(m, newdata = new_data), aes(x = log_gdp_per_cap, y = .fitted)) + \n  geom_line(colour = \"lightgrey\", size = 2) + \n  geom_point() + \n  theme_minimal()\n\n\n\n\nBut how well does this model fit our observed data?\nLet’s fit it with our observed values of each country’s logged GDP per capita, instead of the ones we provided above.\n\naugment(m)\n\n# A tibble: 202 × 9\n   .rownames life_exp log_gdp_per_cap .fitted .resid    .hat .sigma   .cooksd\n   <chr>        <dbl>           <dbl>   <dbl>  <dbl>   <dbl>  <dbl>     <dbl>\n 1 1             75.6           10.3     79.2 -3.55  0.0104    4.13 0.00395  \n 2 2             63.1            6.25    61.1  2.01  0.0192    4.14 0.00237  \n 3 3             61.1            7.44    66.5 -5.40  0.00879   4.12 0.00766  \n 4 4             78.9            8.32    70.5  8.40  0.00533   4.09 0.0111   \n 5 6             79.3           10.6     80.8 -1.49  0.0134    4.14 0.000893 \n 6 7             76.3            9.46    75.6  0.743 0.00620   4.14 0.000102 \n 7 8             74.7            8.21    69.9  4.72  0.00557   4.12 0.00368  \n 8 10            78.2            9.67    76.5  1.62  0.00704   4.14 0.000548 \n 9 11            82.4           10.8     81.7  0.748 0.0152    4.14 0.000257 \n10 12            81.6           10.7     81.3  0.373 0.0143    4.14 0.0000601\n# ℹ 192 more rows\n# ℹ 1 more variable: .std.resid <dbl>\n\n\nHow did it do? What is the difference between what our model predicted and the country’s observed life expectancy?\nHere, we have the predicted values for life expectancy for all of our countries in our sample. Compare .fitted (the predicted life expectancy) to life_exp (the actual observed average life expectancy).\n\nm_eval <- augment(m) |> \n  transmute(\n    life_exp, \n    .fitted,\n    diff = life_exp - .fitted\n  )\n\nm_eval\n\n# A tibble: 202 × 3\n   life_exp .fitted   diff\n      <dbl>   <dbl>  <dbl>\n 1     75.6    79.2 -3.55 \n 2     63.1    61.1  2.01 \n 3     61.1    66.5 -5.40 \n 4     78.9    70.5  8.40 \n 5     79.3    80.8 -1.49 \n 6     76.3    75.6  0.743\n 7     74.7    69.9  4.72 \n 8     78.2    76.5  1.62 \n 9     82.4    81.7  0.748\n10     81.6    81.3  0.373\n# ℹ 192 more rows\n\n\nNote that broom::augment() already did this calculation and stored it in the .resid variable.\n\naugment(m) |> \n  transmute(\n    life_exp, \n    .fitted,\n    diff = life_exp - .fitted,\n    .resid\n  )\n\n# A tibble: 202 × 4\n   life_exp .fitted   diff .resid\n      <dbl>   <dbl>  <dbl>  <dbl>\n 1     75.6    79.2 -3.55  -3.55 \n 2     63.1    61.1  2.01   2.01 \n 3     61.1    66.5 -5.40  -5.40 \n 4     78.9    70.5  8.40   8.40 \n 5     79.3    80.8 -1.49  -1.49 \n 6     76.3    75.6  0.743  0.743\n 7     74.7    69.9  4.72   4.72 \n 8     78.2    76.5  1.62   1.62 \n 9     82.4    81.7  0.748  0.748\n10     81.6    81.3  0.373  0.373\n# ℹ 192 more rows\n\n\nOkay, so there are some differences. Let’s look at those difference a bit more closely:\n\nggplot(augment(m), aes(x = .resid)) + \n  geom_density() + \n  geom_vline(xintercept = 0) + \n  theme_minimal()\n\n\n\n\nIf our model perfectly predicted each country’s life expectancy, we would see no difference between the predicted and observed values. There would just be a very tall straight line at 0 on the graph above.\nOur model hasn’t predicted life expectancy perfectly. Whilst most predictions are within a couple of years of the country’s true life expectancy, there are some that are very different (up to 10 or 15 years!). Where the model has got it wrong, it has tended to overestimate life expectancy (note that the peak of the density curve sits above 0).\n\nCan you see for which points these large differences exist?\n\n\nggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = F) + \n  theme_minimal()\n\n\n\n\nWhat is causing these differences? A lot of your work as a political scientist is trying to answer this very question!\n\n\n(Random) error\nThe world is a messy and complicated place. Things often vary in completely random ways. That’s okay! It means that your observational data are going to move in funny and random ways that you cannot capture. That’s okay too! As long as your model includes all those systematic drivers of the thing you are interested in measuring (such as life expectancy), we can accept a bit of random error.\nIn fact, we have already accounted for this. Remember that error term?\n\\[\nlife Exp_x = \\beta_0 + \\beta_1 logGdpPerCap_x + \\epsilon\n\\]\nWe run into issues when this isn’t the case. We will discuss this more in later classes.\n\n\nModelling relationships among categorical variables\nSometimes we want to know whether our outcome of interest changes based where our observation sits within a categorical variable. For example, do levels of support for abortion access differ between Democrats, Republicans, and Independents? Do the number of women elected to parliament change based on whether or not the country has a formal quota?\nLet’s return to the American National Election Survey first explored last week. We will focus on that first question: do levels of support for abortion access differ between Democrats, Republicans, and Independents?\nWe can easily access the 2012 survey through R using the poliscidata package.\npoliscidata::nes\n\nCross tabs\nA simple cross tab can provide a nice summary of differences in your outcome of interest across your categories.\nFor example, let’s look at differences in the number of individuals who identified as Democrat, Republican, or Independent and whether they do not support access to abortions, support access with some conditions, with more conditions, or always.\n\ntabyl(nes, abort4, pid_3)\n\n     abort4  Dem Ind Rep NA_\n      Never  187 229 252   4\n Some conds  499 583 519   6\n More conds  332 337 227   2\n     Always 1325 964 381  10\n       <NA>   15  36   6   2\n\n\nWe can visualise this:\n\n\n\nAre these differences meaningful or significant? We will chat about that next week."
  },
  {
    "objectID": "content/03-content.html#visualizing-the-linear-relationship-between-two-continuous-variables",
    "href": "content/03-content.html#visualizing-the-linear-relationship-between-two-continuous-variables",
    "title": "Bivariate Relationships",
    "section": "Visualizing the linear relationship between two continuous variables",
    "text": "Visualizing the linear relationship between two continuous variables\n\nggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = F) + \n  theme_minimal()\n\n\n\n\nWe have started to unpack our estimated model for the linear relationship between a country’s (logged) GDP per capita and its average life expectancy.\n\\[\nlife Exp_x = \\beta_0 + \\beta_1 logGdpPerCap_x\n\\]\nRead this as: the life expectancy of some country, \\(x\\), is a function of some constant (\\(\\beta_0\\)) and the its logged GDP per capita transformed by some value \\(\\beta_1\\).\nHow do we calculate the constant (\\(\\beta_0\\)) and \\(\\beta_1\\)?\n\nEstimating a linear model in R\n\nm <- lm(life_exp ~ log_gdp_per_cap, data = gapminder_df)\n\ntidy(m)\n\n# A tibble: 2 × 5\n  term            estimate std.error statistic  p.value\n  <chr>              <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)        32.8      1.72       19.0 2.38e-46\n2 log_gdp_per_cap     4.54     0.195      23.3 2.36e-58"
  },
  {
    "objectID": "content/02-content.html#section",
    "href": "content/02-content.html#section",
    "title": "Descriptive Statistics",
    "section": "Section",
    "text": "Section\n\nPrerequisites\n\n\n\n\nlibrary(tidyverse)\nlibrary(poliscidata)\nlibrary(wbstats)\nlibrary(janitor)\nlibrary(skimr)\nlibrary(countrycode)\nlibrary(ggridges)\nlibrary(ggdist)\nlibrary(MetBrewer)\nlibrary(patchwork)\n\n\n\nHow will what you learn this week help your research?\nYou have an interesting question that you want to explore. You have some data that relate to that question, including those that describe the outcome in which you are interested and the things that you think determine or drive that outcome. You think that one (or more) of the drivers is particularly important, but no one has yet written about it or proven its importance. Brilliant! What do you do now?\nThe first step in any empirical analysis is getting to know your data. I mean, really getting to know your data. You want to dig into it with a critical eye. You want to understand any patterns lurking beneath the surface.\nUltimately, you want to get a really good understanding of the data generation process. This has two parts. First, you want to understand how, out there in the real world, your outcome and drivers come to be. For example, if you are interested in political voting patterns, you want to know the nitty gritty process of how people actually vote. Do they have to travel long distances, stand in long queues, fill out a lot of paperwork? Are there age restrictions on their ability to vote? Are there more insidious restrictions that might suppress voting for one particular group in the electorate? To answer these questions, you can use those patterns and summaries descibed above. Are there relatively few young voters compared to older voters? Why might that be? In turn, your growing expertise in and understanding of the data generation process should inform your exploration of the data. You might note that people have to wait in long queues on a Tuesday to vote. Does this impact the number of workers vs. retirees who vote?\nNow, this is made slightly more tricky by the second part of your exploration of the data generation process. You need to understand how your variables are actually measured. How do we know who turns out to vote? Did you get access to the voter file, which records each individual who voted and some interesting and potentially relevant demographic information about them? Or are you relying on exit polls, that only include a portion of those who voted? Were the people included in the polls reflective of the total voting population? What or whom is missing from this survey? Of course, if your sample is not representative, you might find some patterns that appear to be very important to your outcome of interest but are, in fact, just an artifact of a poorly drawn sample.\nThis week you will be introduced to the first part of this process: data exploration. We use descriptive statistics to describe patterns in our data. These are incredibly powerful tools that will arm you with an intimate knowledge of the shape of your factors of interest. With this knowledge, you will be able to start to answer your important question and potentially identify new ones. You will also be able to sense-check your more complex models and pick up on odd or incorrect relationships that they may find.\nAs you make frequency tables and histograms and very elaborate dot plots and box charts, keep in mind that these are tools useful for your interrogation of the data generation process. Be critical. Continue to ask whether your data allow you to detect true relationships between your variables of interest. Build your intuition for what is really going on and what factors are really driving your outcome of interest.\nLet’s get started.\n\n\nDescribing your data\nDescriptive statistics can be used to understand single variables and how any number of variables of interest relate to one another. The level of complexity involved in understanding these data and their relationships tend to increase with the number of variables you include.\nYour variables are defined in terms of a unit of observation or analysis. These could include individuals, households, congressional districts, states, or countries. The unit of analysis you adopt should be relevant to your theory. For example, if you are interested in understanding which strategies rebel groups use to successfully recruit individuals to fight, you probably want individual-level data. If you are interested in determining what drives countries to war with one another, you probably want country-level data (or leader-level data, or voter-level data?).\nAlso, the ways of describing your data depend on the type of variable. You can have categorical or continuous variables.\nCategorical variables are discrete. They can be unordered (nominal) - for example, the colour of cars - or ordered (ordinal) - for example, whether you strongly dislike, dislike, are neutral about, like, or strongly like Taylor Swift.\n\nDichotomous (or binary) variables are a special type of categorical variable. They take on one of two values. For example: yes or no; at war or not at war; is a Swifty, or is not a Swifty.\n\nContinuous variables are, well, continuous. For example, your height or weight, a country’s GDP or population, or the number of fatalities in a battle.\n\nContinuous variables can be made into (usually ordered) categorical variables. This process is called binning. For example, you can take peoples ages and reduce them to 0 - 18 years old, 18 - 45 years old, 45 - 65 years old, and 65+ years old.\nYou lose information in this process: you cannot go from 45 - 65 years old back to the individuals’ precise age. In other words, you cannot go from a categorical to continuous variable.\n\nLet’s take a look at how you can describe these different types of variables in turn, using real-world political science examples.\n\n\nDescribing categorical variables\nSimple put, for categorical variables we are interested in the count and/or percentage of cases that fall into each category.\n\nLater, we will ask interesting questions using these summaries. These include whether differences between the counts and/or percentages of cases that fall into each category are meaningfully (and/or statistically significantly) different from one another. This deceptively simple question serves as the foundation for a lot of political science (particularly comparative) research.\n\n\nCase study: American National Election Survey\nFor this section, we will be working with the American National Election Survey to explore how to produce useful descriptive statistics for categorical variables using R. The ANES polls annually individual Americans about their political beliefs and behavior.\nWe can access the 2012 survey using the poliscidata package:\n\npoliscidata::nes\n\n\nTake a look at the many different bits of information collected about each respondent using ?nes.\n\nLet’s look at how many individuals of different ages took part in the survey in 2012. The survey records the age of each respondent within six different brackets and reports that information in the dem_age6 variable.\n\n\nFrequency distribution\nWe can take advantage of janitor::tabyl() to quickly calculate the number and proportion of respondents in each age bracket.\n\ntabyl(nes, dem_age6)\n\n dem_age6    n    percent valid_percent\n    17-29  936 0.15821501     0.1598634\n    30-39  862 0.14570656     0.1472246\n    40-49  948 0.16024341     0.1619129\n    50-59 1312 0.22177147     0.2240820\n    60-69 1105 0.18678161     0.1887276\n 70-older  692 0.11697093     0.1181896\n     <NA>   61 0.01031102            NA\n\n\n\nNote: valid_percent provides the proportion of respondents in each age bracket with missing values removed from the denominator. For example, the NES survey had 5,916 respondents in 2012, but only 5,855 of them provided their age. 936 responded that they are 17 - 29 years old. Therefore, the 17-29 proportion (which is bounded by 0 and 1, whereas percents are bounded by 0 and 100) is 936 / 5,916 and its valid proportion is 936 / 5,855.\n\n\n\nVisualizing this frequency\nIt is a bit difficult to quickly determine relative counts. Which age bracket has the most respondents? Which has the least? Are these counts very different from each other.\nI highly recommend visualizing your data. You will get a much better sense of it. We can easily visualize this frequency table. I recommend using a bar chart to show clearly relative counts.\n\nnes |> \n  tabyl(dem_age6) |> \n  ggplot(aes(x = n, y = dem_age6)) + \n  geom_col() +\n  theme_minimal() + \n  labs(\n    x = \"Count of respondents\",\n    y = \"Age bracket\"\n  )\n\n\n\n\n\n\n\nDescribing continuous variables\nWe need to treat continuous variables differently from categorical ones because they cannot meaningfully be bounded together and compared. For example, imagine making a frequency table or bar chart that counts the number of countries with each observed GDP. You would have 193 different counts of one. Useless!\nWe can get a much better sense of our continuous variables by looking at characteristics of the distribution of these variables across the range of all possible values they could take on. Phew! Let’s make sense of this using some real-world data.\n\nCase study: Comparing countries’ spending on education\nFor this section, we will look at each country’s spending on education as a percent of their gross domestic product. We will use wbstats::wb_data() to collect these data.\n\nperc_edu <- wb_data(\n  \"SE.XPD.TOTL.GD.ZS\", start_date = 2020, end_date = 2020, return_wide = F\n) |> \n  transmute(\n    country, \n    region = countrycode(country, \"country.name\", \"region\", \n                         custom_match = c(\"Turkiye\" = \"Europe & Central Asia\")),\n    year = date, \n    value = value / 100\n  )\n\nperc_edu\n\n# A tibble: 217 × 4\n   country             region                      year   value\n   <chr>               <chr>                      <dbl>   <dbl>\n 1 Afghanistan         South Asia                  2020  0.0286\n 2 Albania             Europe & Central Asia       2020  0.0310\n 3 Algeria             Middle East & North Africa  2020  0.0704\n 4 American Samoa      East Asia & Pacific         2020 NA     \n 5 Andorra             Europe & Central Asia       2020 NA     \n 6 Angola              Sub-Saharan Africa          2020  0.0242\n 7 Antigua and Barbuda Latin America & Caribbean   2020  0.0345\n 8 Argentina           Latin America & Caribbean   2020  0.0502\n 9 Armenia             Europe & Central Asia       2020  0.0271\n10 Aruba               Latin America & Caribbean   2020 NA     \n# ℹ 207 more rows\n\n\nI have converted these percentages (0 - 100) to proportions (0 - 1) for ease of interpretation. I have also added each country’s region (using countrycode::countrycode()) so that we can explore regional trends in our data.\n\nTurkiye’s recent name change has not yet been included in the package. We need to correct for this using the custom_match argument.\n\nWe can get a good sense of how expenditure varied by country by looking at the center, spread, and shape of the distribution.\n\n\nHistogram\n\nggplot(perc_edu, aes(x = value)) + \n  geom_histogram() + \n  theme_minimal() + \n  labs(\n    x = \"Expenditure on education as a proportion of GDP\",\n    y = \"Count\"\n  )\n\n\n\n\n\nTake a look at ?geom_histogram to find the arguments needed to change the bin width of your histograms.\n\n\n\nDensity curves\n\nggplot(perc_edu, aes(x = value)) + \n  geom_density() + \n  theme_minimal() + \n  labs(\n    x = \"Expenditure on education as a proportion of GDP\",\n    y = \"Count\"\n  )\n\n\n\n\n\n\nUnderstanding distributions\nBecause continuous variables are best described through their distribution, we can use the shape of that distribution to better understand our individual variables and compare them to others. Is the distribution symmetric or skewed? Where are the majority of observations clustered? Are there multiple distinct clusters, or high points, in the distribution?\n\nNormal distribution\n\ntibble(x = rnorm(n = 1e6)) |> \n  ggplot(aes(x = x)) + \n  geom_histogram() + \n  theme_minimal()\n\n\n\n\n\n\nRight skewed distribution\n\ntibble(x = rbeta(1e6, 1, 10)) |> \n  ggplot(aes(x = x)) + \n  geom_histogram() + \n  theme_minimal()\n\n\n\n\n\n\nLeft skewed distribution\n\ntibble(x = rbeta(1e6, 10, 1)) |> \n  ggplot(aes(x = x)) + \n  geom_histogram() + \n  theme_minimal()\n\n\n\n\n\n\n\nMeasures of central tendency: mean, median, and mode\nWe can also use measures of central tendency to quickly describe and compare our variables.\n\nMean\nThe mean is the average of all values:\n\\[\n\\bar{x} = \\frac{\\Sigma x_i}{n}\n\\]\nIn other words, add all of your values together and then divide that total by the number of values you have.\nIn R:\n\nmean(perc_edu$value, na.rm = T)\n\n[1] 0.04639309\n\n\n\nNote: if you do not use the argument na.rm (read NA remove!), you will get an NA if any exist in your vector of values. This is a good default! You should be very aware of missing data points.\n\n\n\nMedian\nThe median is the mid-point of all values.\nTo calculate it, put all of your values in order from smallest to largest. Identify the value in the middle. That’s your median.\nIn R:\n\nmedian(perc_edu$value, na.rm = T)\n\n[1] 0.04461425\n\n\n\n\nMode\nThe mode is the most frequent of all values.\nTo calculate it, count how many times each value occurs in your data set. The one that occurs the most is your mode.\n\nThis is usually a more useful summary statistic for categorical variables than continuous ones. For example, which colour of car is most popular? Which political party has the most members?\n\nIn R:\n\nx <- c(1, 1, 2, 4, 5, 32, 5, 1, 10, 3, 4, 6, 10)\n\ntable(x)\n\nx\n 1  2  3  4  5  6 10 32 \n 3  1  1  2  2  1  2  1 \n\n\n\n\nUsing central tendency to describe and understand distributions\nNormally distributed values have the same mean and median.\n\nnorm_dist <- tibble(x = rnorm(n = 1e6))\n\nggplot(norm_dist, aes(x = x)) + \n  geom_histogram() + \n  geom_vline(xintercept = mean(norm_dist$x), colour = \"#FB8F86\", size = 2) + \n  geom_vline(xintercept = median(norm_dist$x), colour = \"#819FE3\", size = 2) + \n  theme_minimal()\n\n\n\n\nFor right skewed data, the mean is greater than the median.\n\nright_dist <- tibble(x = rbeta(1e6, 2, 10))\n\nggplot(right_dist, aes(x = x)) + \n  geom_histogram() + \n  geom_vline(xintercept = mean(right_dist$x), colour = \"#FB8F86\", size = 2) + \n  geom_vline(xintercept = median(right_dist$x), colour = \"#819FE3\", size = 2) + \n  theme_minimal()\n\n\n\n\nFor left skewed data, the mean is smaller than the median.\n\nleft_dist <- tibble(x = rbeta(1e6, 10, 2))\n\nggplot(left_dist, aes(x = x)) + \n  geom_histogram() + \n  geom_vline(xintercept = mean(left_dist$x), colour = \"#FB8F86\", size = 2) + \n  geom_vline(xintercept = median(left_dist$x), colour = \"#819FE3\", size = 2) + \n  theme_minimal()\n\n\n\n\n\n\n\nFive number summary\nAs you can see, we are attempting to summarise our continuous data to give us a meaningful but manageable sense of it. Means and medians are useful for continuous data.\nWe can provide more context to our understanding using more summary statistics. A common approach is the five number summary. This includes:\n\nThe smallest value;\nThe 25th percentile value, or the median of the lower half of the data;\nThe mean;\nThe 75th percentile value, or the median of the upper half of the data;\nThe largest value.\n\nWe can use skimr::skim() to quickly get useful information about our continuous variable.\n\nskim(perc_edu$value)\n\n\nData summary\n\n\nName\nperc_edu$value\n\n\nNumber of rows\n217\n\n\nNumber of columns\n1\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ndata\n59\n0.73\n0.05\n0.02\n0\n0.03\n0.04\n0.06\n0.14\n▂▇▃▁▁\n\n\n\n\n\nWe have 217 rows (because our unit of observation is a country, we can read this as 217 countries). We are missing education spending values for 59 of those countries (see n_missing), giving us a complete rate of 73% (see complete_rate).\nThe country that spent the least on education as a percent of its GDP in 2020 was Cuba, which spent 0.000% (see p0). The country that spent the most was the Marshall Islands, which spent 14% (see p100). The average percent of GDP spent on education in 2020 was 4.64% (see p50).\nThis description was a bit unwieldy. To get a better sense of our data, we can visualize it.\n\n\nBox plots\nBox plots (sometimes referred to as box and whisker plots) visualize the five number summary (with bonus features) nicely.\n\nggplot(perc_edu, aes(x = value)) + \n  geom_boxplot() + \n  theme_minimal() + \n  theme(\n    axis.text.y = element_blank()\n  ) + \n  labs(\n    x = \"Expenditure on education as a proportion of GDP\",\n    y = NULL\n  )\n\n\n\n\nNote that some values are displayed as dots. The box plot is providing you with a bit more information than the five number summary alone. The box itself displays the 25th percentile, the mean, and the 75th percentile values. The tails show you all the data up to a range 1.5 times the mean. If the smallest or largest values fall below or above (respectively) 1.5 times the mean, the tail ends at that value. If, however, these values fall outside that range, they are displayed as dots. These are (very rule of thumb, take with a grain of salt, please rely on your theory instead!) candidates for outliers.\n\n\nOutliers\nOutliers fall so far away from the majority of the other values that they should be examined closely and perhaps excluded from your analysis. Outliers can distort your mean. They do not, howver, distort your median.\n\nWe will talk more about how to deal with outliers appropriately later in the course.\n\n\n\nMeasures of spread: range, variance, and standard deviation\nWe now have a good sense of some of the features of our data. Another useful thing to know is the shape of the distribution. Here, measures of spread are useful.\n\nRange\nThe range is the difference between the largest and smallest value.\n\\[\nrange = min - max\n\\]\n\nmax(perc_edu$value, na.rm = T) - min(perc_edu$value, na.rm = T)\n\n[1] 0.1362499\n\n\n\n\nVariance\nThe variance measures how spread out your values are. Take a look at these two plots. Both have the same center point (0) and number of observations (1,000,000). However, the data are much more spread out around that center point in the top graph.\n\nwide_dist <- tibble(x = rnorm(1e6, sd = 2))\n\np1 <- ggplot(wide_dist, aes(x = x)) + \n  geom_histogram() + \n  geom_vline(xintercept = 0) + \n  theme_minimal() + \n  scale_x_continuous(limits = c(-4, 4))\n\nnarrow_dist <- tibble(x = rnorm(1e6, sd = 1))\n\np2 <- ggplot(narrow_dist, aes(x = x)) + \n  geom_histogram() + \n  geom_vline(xintercept = 0) + \n  theme_minimal() + \n  scale_x_continuous(limits = c(-4, 4))\n\np1 / p2\n\n\n\n\nThe data in the top graph have higher variance (are more spread out) than those in the bottom graph. We measure this by calculating the average of the squares of the deviations of the observations from their mean.\n\\[\ns^2 = \\frac{\\Sigma(x_i - \\bar{x})^2}{n - 1}\n\\]\nLet’s step through this. We will first calculate the variance for wide_dist, or the top graph. To do this:\n\nCalculate the mean of your values.\nCalculate the difference between each individual value and that mean.\nSquare those differences.\n\nWe do not care whether the value is higher or lower than the mean. We only care how far from the mean it is. Squaring a value removes its sign (positive or negative) allowing us to concentrate on this difference.\n\nAdd all of those squared differences to get a single number.\nDivide that single number by the number of observations you have minus 1.\n\nYou now have your variance!\nIn R:\n\nwide_var_calc <- wide_dist |> \n  mutate(\n    mean = mean(wide_dist$x),\n    diff = x - mean,\n    diff_2 = diff^2\n  )\n\nwide_var_calc\n\n# A tibble: 1,000,000 × 4\n         x      mean    diff    diff_2\n     <dbl>     <dbl>   <dbl>     <dbl>\n 1  1.88   -0.000428  1.88    3.53    \n 2 -2.30   -0.000428 -2.30    5.30    \n 3  0.272  -0.000428  0.272   0.0742  \n 4 -1.55   -0.000428 -1.55    2.40    \n 5 -1.49   -0.000428 -1.49    2.22    \n 6 -3.53   -0.000428 -3.53   12.4     \n 7  2.86   -0.000428  2.86    8.18    \n 8 -2.00   -0.000428 -2.00    3.99    \n 9 -0.0164 -0.000428 -0.0160  0.000257\n10 -0.969  -0.000428 -0.969   0.938   \n# ℹ 999,990 more rows\n\n\nWe take the sum of square of the difference between each observation and the mean of our whole sample. We then divide that by one less than our number of observations.\n\nwide_var <- sum(wide_var_calc$diff_2) / (nrow(wide_var_calc) - 1)\n\nwide_var\n\n[1] 3.989814\n\n\nWe can compare this to the variance for our narrower distribution.\n\nnarrow_var_calc <- narrow_dist |> \n  mutate(\n    mean = mean(narrow_dist$x),\n    diff = x - mean,\n    diff_2 = diff^2\n  )\n\nnarrow_var <- sum(narrow_var_calc$diff_2) / (nrow(narrow_var_calc) - 1)\n\nnarrow_var\n\n[1] 1.001298\n\n\nIt is, in fact, smaller!\nThat was painful. Happily we can use var() to do this in one step:\n\nvar(wide_dist)\n\n         x\nx 3.989814\n\n\n\nvar(narrow_dist)\n\n         x\nx 1.001298\n\n\n\nvar(wide_dist) > var(narrow_dist)\n\n     x\nx TRUE\n\n\n\n\nStandard deviation\nA simpler measure of spread is the standard deviation. It is simply the square root of the variance.\n\nsqrt(wide_var)\n\n[1] 1.997452\n\n\n\nsqrt(narrow_var)\n\n[1] 1.000649\n\n\nYou can get this directly using sd():\n\nsd(wide_dist$x)\n\n[1] 1.997452\n\n\n\nsd(narrow_dist$x)\n\n[1] 1.000649\n\n\nIf you look back to our graphs, you will see that I set the standard deviations explicitly when I generated the data: rnorm() takes an sd argument. This is great because we can confirm that the standard deviations for the wide and narrow distributions are 2 and 1 respectively (with a little bit of noise).\n\ntibble(\n  n = rnorm(1e6, sd = 1),\n  w = rnorm(1e6, sd = 2)\n) |> \n  ggplot() + \n  geom_density(aes(x = n), colour = \"green\", size = 2) + \n  geom_density(aes(x = w), colour = \"lightblue\", size = 2) + \n  theme_minimal()\n\n\n\n\n\nRemember that the standard deviation is a measure of how spread out our data are. Therefore, data with no spread (are all the exact same number) will have a standard deviation of 0.\n\n\n\n\nNormal distributions\nRemember that normal distributions share a mean and median. This has very cool and useful consequences.\n\nnorm_5_2 <- tibble(x = rnorm(n = 1e6, mean = 5, sd = 2))\n\nggplot(norm_5_2, aes(x = x)) + \n  stat_slab(\n    aes(fill_ramp = after_stat(cut_cdf_qi(cdf))), fill = met.brewer(\"Egypt\")[2]\n  ) + \n  scale_fill_ramp_discrete(range = c(1, 0.2), guide = \"none\") + \n  theme_minimal()\n\n\n\n\n\nApproximately 68% of the data fall within one standard deviation of the mean (the dark blue).\nApproximately 95% of the data fall within two standard deviations of the mean (the medium blue).\nApproximately 99.7% of the data fall within three standard deviations of the mean (the light blue).\n\n\n\nStandardization\nNotice how our description of each variable depends on its units of measurement. What do we do if we want to compare across different measurements that have different units?\n\nZ scores\nFor normal distributions, we can use the z score. This gives us a standard way of understanding how many standard deviations from the mean of a normally distributed variable a value is.\n\\[\nz_i = \\frac{x_i - \\mu_x}{\\sigma_x}\n\\]\nWe are just transforming our data. We want to center it around 0 and reshape it so that roughly 68% of the data fall within one standard deviation of the mean, 95% of the data fall within two standard deviations of the mean, and 99.7% of the data fall within three standard deviations of the mean.\nLet’s standardize our data from above.\n\nstandard_5_2 <- norm_5_2 |> \n  mutate(mean = mean(x),\n         sd = sd(x),\n         z_score = (x - mean) / sd)\n\nhead(standard_5_2)\n\n# A tibble: 6 × 4\n      x  mean    sd z_score\n  <dbl> <dbl> <dbl>   <dbl>\n1  6.46  5.00  2.00  0.728 \n2  4.02  5.00  2.00 -0.493 \n3  7.91  5.00  2.00  1.45  \n4  5.66  5.00  2.00  0.329 \n5  6.37  5.00  2.00  0.682 \n6  5.09  5.00  2.00  0.0430\n\n\nWe can confirm this:\n\nggplot(standard_5_2, aes(x = z_score)) + \n  stat_slab(\n    aes(fill_ramp = after_stat(cut_cdf_qi(cdf))), fill = met.brewer(\"Egypt\")[2]\n  ) + \n  scale_fill_ramp_discrete(range = c(1, 0.2), guide = \"none\") + \n  theme_minimal() + \n  scale_x_continuous(breaks = seq(-5, 5, 1))\n\n\n\n\n\n\n\n\nNext week\nA focus on techniques for examining relationships between variables."
  }
]