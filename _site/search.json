[
  {
    "objectID": "content/01-content.html",
    "href": "content/01-content.html",
    "title": "Introduction and Set Up",
    "section": "",
    "text": "Garrick Aden-Buie’s macOS set up blogpost\n Matti Vuorre’s macOS set up blogpost\n\n\n\n Installation and Connect Git, GitHub, RStudio sections in Jennifer Bryan’s Happy Git and GitHub for the useR\n\n\n\n Install or upgrade R and RStudio chapter in Jennifer Bryan’s Happy Git and GitHub for the useR"
  },
  {
    "objectID": "content/01-content.html#slides",
    "href": "content/01-content.html#slides",
    "title": "Introduction and Set Up",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "content/01-content.html#in-class",
    "href": "content/01-content.html#in-class",
    "title": "Introduction and Set Up",
    "section": "In-class",
    "text": "In-class"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GVPT622",
    "section": "",
    "text": "Quantitative Methods for Political Science\n        \n        \n            An introduction to research methods and quantitative research in political science.\n        \n        \n            Fall 2023Department of Government and PoliticsUniversity of Maryland, College Park\n        \n    \n    \n      \n        \n        \n        \n      \n    \n\n\n\n\n\nTeaching Assistant\n\n   Harriet Goers\n   Chincoteague Building\n   hgoers@umd.edu\n   harrietgoers\n\n\n\nCourse details\n\n   January 9–May 2, 2023\n   7:15–9:45 PM\n   Tydings Building\n   Slack\n\n\n\nContacting me\nE-mail and Slack are the best ways to get in contact with me. I will try to respond to all course-related e-mails and Slack messages within 24 hours."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "DATE \n    TITLE \n    CLASS SLIDES \n    SESSION SCRIPTS \n    ASSIGNMENTS \n  \n \n\n  \n    1 \n    Course Introduction \n     \n     \n     \n  \n  \n    2 \n    Descriptive Statistics"
  },
  {
    "objectID": "content/01-content.html#class-slides",
    "href": "content/01-content.html#class-slides",
    "title": "Course Introduction",
    "section": "Class slides",
    "text": "Class slides"
  },
  {
    "objectID": "content/01-content.html#session-scipts",
    "href": "content/01-content.html#session-scipts",
    "title": "Course Introduction",
    "section": "Session scipts",
    "text": "Session scipts"
  },
  {
    "objectID": "content/02-content.html#class-slides",
    "href": "content/02-content.html#class-slides",
    "title": "Descriptive Statistics",
    "section": "Class slides",
    "text": "Class slides"
  },
  {
    "objectID": "content/02-content.html#session-scripts",
    "href": "content/02-content.html#session-scripts",
    "title": "Descriptive Statistics",
    "section": "Session scripts",
    "text": "Session scripts"
  },
  {
    "objectID": "content/01-content.html#session-scripts",
    "href": "content/01-content.html#session-scripts",
    "title": "Course Introduction",
    "section": "Session scripts",
    "text": "Session scripts"
  },
  {
    "objectID": "content/02-content.html#session",
    "href": "content/02-content.html#session",
    "title": "Descriptive Statistics",
    "section": "Session",
    "text": "Session\n\nPrerequisites\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(skimr)\nlibrary(patchwork)\n\n\n\nDescribing categorical variables\nImagine that we have completed a survey of 10,000 individuals. We asked them their age and level of satisfaction with their job.\n\n\n\nWe store their responses in a data frame called survey_df. It has 10,000 observations (one for each respondent) and three variables: a unique id (id); their age in years (age); and their level of satisfaction (sat), which can take one of four values: very unsatisfied, unsatisfied, satisfied, and very satisfied.\n\nsurvey_df\n\n# A tibble: 10,000 × 3\n      id   age sat             \n   <int> <int> <chr>           \n 1     1    20 Unsatisfied     \n 2     2    27 Very unsatisfied\n 3     3    42 Very satisfied  \n 4     4    20 Very unsatisfied\n 5     5    37 Unsatisfied     \n 6     6    26 Satisfied       \n 7     7    60 Very satisfied  \n 8     8    63 Very unsatisfied\n 9     9    42 Satisfied       \n10    10    40 Very unsatisfied\n# ℹ 9,990 more rows\n\n\n\nFrequency distribution\nWe can take advantage of janitor::tabyl() to quickly calculate the number and proportion of respondents who provided each level of satisfaction.\n\ntabyl(survey_df, sat)\n\n              sat    n percent\n        Satisfied 2579  0.2579\n      Unsatisfied 2414  0.2414\n   Very satisfied 2508  0.2508\n Very unsatisfied 2499  0.2499\n\n\nAlternatively, we can use skimr::skim() to get a useful summary of this categorical variable.\n\nskim(survey_df$sat)\n\n\nData summary\n\n\nName\nsurvey_df$sat\n\n\nNumber of rows\n10000\n\n\nNumber of columns\n1\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ndata\n0\n1\n9\n16\n0\n4\n0\n\n\n\n\n\n\n\nVisualizing this frequency\nWe can easily visualize this using a bar chart.\n\np1 <- survey_df |> \n  tabyl(sat) |> \n  ggplot(aes(x = n, y = sat)) + \n  geom_col() + \n  theme_minimal()\n\np2 <- survey_df |> \n  tabyl(sat) |> \n  ggplot(aes(x = percent, y = sat)) + \n  geom_col() + \n  theme_minimal()\n\np1 | p2\n\n\n\n\n\n\nWorking with factors\nNotice how our categories are ordered: very satisfied sits above satisfied. We can tell R this information factor().\n\nsurvey_df <- survey_df |> \n  mutate(\n    sat = factor(sat, levels = c(\"Very unsatisfied\", \n                                 \"Unsatisfied\", \n                                 \"Satisfied\", \n                                 \"Very satisfied\"))\n  )\n\nNow when we work with our categorical variables, they will be ordered.\n\nsurvey_df |> \n  tabyl(sat) |> \n  ggplot(aes(x = n, y = sat)) + \n  geom_col() + \n  theme_minimal()\n\n\n\n\n\n\n\nDescribing continuous variables\nWe can also get a good sense of our continuous variable age by looking at the center, spread, and shape of its distribution.\n\nFive number summary\nWe can use skimr::skim() to quickly get useful information on our continuous variable.\n\nskim(survey_df$age)\n\n\nData summary\n\n\nName\nsurvey_df$age\n\n\nNumber of rows\n10000\n\n\nNumber of columns\n1\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ndata\n0\n1\n39.96\n14.79\n15\n27\n40\n53\n65\n▇▇▇▇▇\n\n\n\n\n\n\n\nHistogram\n\nggplot(survey_df, aes(x = age)) + \n  geom_histogram() + \n  theme_minimal()\n\n\n\n\n\nggplot(survey_df, aes(x = age)) + \n  geom_histogram(binwidth = 5) + \n  theme_minimal()\n\n\n\n\n\n\nDensity curves\n\nggplot(survey_df, aes(x = age)) + \n  geom_density() + \n  theme_minimal()\n\n\n\n\n\n\nBox and whisker plots\n\nggplot(survey_df, aes(x = age)) + \n  geom_boxplot() + \n  theme_minimal()\n\n\n\n\n\n\nLooking for patterns in our groups\n\nggplot(survey_df, aes(x = age, y = sat)) + \n  geom_boxplot() + \n  theme_minimal()\n\n\n\n\n\nggplot(survey_df, aes(x = age, y = sat)) + \n  geom_violin() + \n  theme_minimal()\n\n\n\n\n\n\n\nUnderstanding distributions\n\nNormal distribution\n\ntibble(z = rnorm(n = 1000)) |> \n  ggplot(aes(x = z)) + \n  geom_histogram() + \n  theme_minimal()\n\n\n\n\n\ntibble(z = rnorm(n = 1e6)) |> \n  ggplot(aes(x = z)) + \n  geom_histogram() + \n  theme_minimal()\n\n\n\n\n\n\nRight skewed distribution\n\ntibble(z = rbeta(10000, 2, 10)) |> \n  ggplot(aes(x = z)) + \n  geom_histogram() + \n  theme_minimal()\n\n\n\n\n\n\nLeft skewed distribution\n\ntibble(z = rbeta(10000, 10, 2)) |> \n  ggplot(aes(x = z)) + \n  geom_histogram() + \n  theme_minimal()\n\n\n\n\n\n\n\nMeasures of central tendency: mean, median, and mode\n\nMean\nThe mean is the average of all values.\n\n\nMedian\nThe median is the mid-point of all values.\n\n\nMode\nThe mode is the most frequent of all values.\n\n\nUsing central tendency to describe and understand distributions\nNormally distributed vectors share their mean and medians.\n\nnorm_dist <- tibble(z = rnorm(n = 1000))\n\nggplot(norm_dist, aes(x = z)) + \n  geom_histogram() + \n  geom_vline(xintercept = mean(norm_dist$z), colour = \"red\") + \n  geom_vline(xintercept = median(norm_dist$z), colour = \"blue\") + \n  theme_minimal()\n\n\n\n\nFor right skewed data, the mean is greater than the median.\n\nright_dist <- tibble(z = rbeta(10000, 2, 10))\n\nggplot(right_dist, aes(x = z)) + \n  geom_histogram() + \n  geom_vline(xintercept = mean(right_dist$z), colour = \"red\") + \n  geom_vline(xintercept = median(right_dist$z), colour = \"blue\") + \n  theme_minimal()\n\n\n\n\nFor left skewed data, the mean is smaller than the median.\n\nleft_dist <- tibble(z = rbeta(10000, 10, 2))\n\nggplot(left_dist, aes(x = z)) + \n  geom_histogram() + \n  geom_vline(xintercept = mean(left_dist$z), colour = \"red\") + \n  geom_vline(xintercept = median(left_dist$z), colour = \"blue\") + \n  theme_minimal()\n\n\n\n\n\n\n\nMeasures of spread: range, variance, and standard deviation\n\nRange\nThe range is the difference between the largest and smallest value.\n\nmax(survey_df$age) - min(survey_df$age)\n\n[1] 50\n\n\n\n\nVariance\nThe variance measures how spread out your values are. Take a look at these two plots. Both have the same center point (0) and number of observations (1,000,000). However, the data are much more spread out around that center point in the top graph.\n\nwide_dist <- tibble(z = rnorm(1e6, sd = 2))\n\np1 <- ggplot(wide_dist, aes(x = z)) + \n  geom_histogram() + \n  geom_vline(xintercept = 0) + \n  theme_minimal() + \n  scale_x_continuous(limits = c(-4, 4))\n\nnarrow_dist <- tibble(z = rnorm(1e6, sd = 1))\n\np2 <- ggplot(narrow_dist, aes(x = z)) + \n  geom_histogram() + \n  geom_vline(xintercept = 0) + \n  theme_minimal() + \n  scale_x_continuous(limits = c(-4, 4))\n\np1 / p2\n\n\n\n\nThe data in the top graph have far more variance than those in the bottom graph. We measure this by calculating the average of the squares of the deviations of the observations from their mean.\n\\[\ns^2 = \\frac{\\Sigma(x_i - \\bar{x})^2}{n - 1}\n\\]\nLet’s step through this. We will first calculate the variance for wide_dist, or the top graph.\n\nwide_var_calc <- wide_dist |> \n  mutate(\n    mean = mean(wide_dist$z),\n    diff = z - mean,\n    diff_2 = diff^2\n  )\n\nwide_var_calc\n\n# A tibble: 1,000,000 × 4\n         z    mean    diff  diff_2\n     <dbl>   <dbl>   <dbl>   <dbl>\n 1 -0.921  0.00268 -0.924  0.853  \n 2  1.63   0.00268  1.63   2.65   \n 3 -0.0416 0.00268 -0.0443 0.00196\n 4  0.779  0.00268  0.777  0.603  \n 5  0.494  0.00268  0.492  0.242  \n 6 -0.738  0.00268 -0.740  0.548  \n 7 -0.130  0.00268 -0.133  0.0176 \n 8 -1.58   0.00268 -1.58   2.51   \n 9  1.82   0.00268  1.82   3.30   \n10 -0.127  0.00268 -0.129  0.0167 \n# ℹ 999,990 more rows\n\n\nWe take the sum of square of the difference between each observation and the mean of our whole sample. We then divide that by one less than our number of observations.\n\nwide_var <- sum(wide_var_calc$diff_2) / (nrow(wide_var_calc) - 1)\n\nwide_var\n\n[1] 4.00025\n\n\nWe can compare this to the variance for our narrower distribution.\n\nnarrow_var_calc <- narrow_dist |> \n  mutate(\n    mean = mean(narrow_dist$z),\n    diff = z - mean,\n    diff_2 = diff^2\n  )\n\nnarrow_var <- sum(narrow_var_calc$diff_2) / (nrow(narrow_var_calc) - 1)\n\nnarrow_var\n\n[1] 1.000964\n\n\nIt is, in fact, smaller!\nWe can use var() to do this in one step:\n\nvar(wide_dist)\n\n        z\nz 4.00025\n\n\n\nvar(narrow_dist)\n\n         z\nz 1.000964\n\n\n\n\nStandard deviation\nA simpler measure of spread is the standard deviation. It is simply the square root of the variance.\n\nsqrt(wide_var)\n\n[1] 2.000063\n\n\n\nsqrt(narrow_var)\n\n[1] 1.000482\n\n\nIf you look back to our graphs, you will see that I set the standard deviations explicitly when I generated the data. rnorm() takes an sd argument. This is great because we can confirm that the standard deviations for the wide and narrow distributions are 2 and 1 respectively (with a little bit of noise).\n\ntibble(\n  n = rnorm(1e6, sd = 1),\n  w = rnorm(1e6, sd = 2)\n) |> \n  ggplot() + \n  geom_density(aes(x = n), colour = \"green\") + \n  geom_density(aes(x = w), colour = \"lightblue\") + \n  theme_minimal()\n\n\n\n\n\n\n\nStandardization\nNotice how our description of each variable depends on its units of measurement. What do we do if we want to compare across different measurements that have different units?\n\nZ scores\nFor normal distributions, we can use the z score. This gives us a standard way of understanding how many standard deviations from the mean of a normally distributed variable a value is.\n\\[\nz_i = \\frac{x_i - \\mu_x}{\\sigma_x}\n\\]"
  },
  {
    "objectID": "content/03-content.html#class-slides",
    "href": "content/03-content.html#class-slides",
    "title": "Bivariate Relationships",
    "section": "Class slides",
    "text": "Class slides"
  },
  {
    "objectID": "content/03-content.html#section",
    "href": "content/03-content.html#section",
    "title": "Bivariate Relationships",
    "section": "Section",
    "text": "Section\n\nPrerequisites\n\nlibrary(tidyverse)\nlibrary(wbstats)\nlibrary(countrycode)\nlibrary(broom)\nlibrary(janitor)\nlibrary(ggridges)\n\nToday, we will explore the relationship between wealth and health. This question was made popular by Hans Rosling’s Gapminder project.\nFirst, we need to collect our data. We will use wbstats::wb_data() to pull these data directly from the World Bank.\n\ngapminder_df <- wb_data(\n  indicator = c(\"SP.DYN.LE00.IN\", \"NY.GDP.PCAP.CD\"),\n  start_date = 2016,\n  end_date = 2016\n) |> \n  rename(\n    life_exp = SP.DYN.LE00.IN,\n    gdp_per_cap = NY.GDP.PCAP.CD\n  ) |> \n  mutate(\n    log_gdp_per_cap = log(gdp_per_cap),\n    region = countrycode(country, \"country.name\", \"region\", custom_match = c(\"Turkiye\" = \"Europe & Central Asia\"))\n  ) |> \n  relocate(region, .after = country)\n\ngapminder_df\n\n# A tibble: 217 × 8\n   iso2c iso3c country         region  date gdp_per_cap life_exp log_gdp_per_cap\n   <chr> <chr> <chr>           <chr>  <dbl>       <dbl>    <dbl>           <dbl>\n 1 AW    ABW   Aruba           Latin…  2016      28451.     75.6           10.3 \n 2 AF    AFG   Afghanistan     South…  2016        520.     63.1            6.25\n 3 AO    AGO   Angola          Sub-S…  2016       1710.     61.1            7.44\n 4 AL    ALB   Albania         Europ…  2016       4124.     78.9            8.32\n 5 AD    AND   Andorra         Europ…  2016      39932.     NA             10.6 \n 6 AE    ARE   United Arab Em… Middl…  2016      41055.     79.3           10.6 \n 7 AR    ARG   Argentina       Latin…  2016      12790.     76.3            9.46\n 8 AM    ARM   Armenia         Europ…  2016       3680.     74.7            8.21\n 9 AS    ASM   American Samoa  East …  2016      13301.     NA              9.50\n10 AG    ATG   Antigua and Ba… Latin…  2016      15863.     78.2            9.67\n# ℹ 207 more rows\n\n\n\n\nVisualizing bivariate relationships: two continuous variables\n\nggplot(gapminder_df, aes(x = gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  theme_minimal()\n\n\n\n\nThere seems to be a very strong case that there is a relationship between a country’s GDP per capita (wealth) and its average life expectancy (health).\nBecause we want to explore linear relationships at this stage of the course, we will look at the logged GDP per capita variable:\n\nggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  theme_minimal()\n\n\n\n\n\n\nVisualizing the linear relationship between two continuous variables\n\nggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = F) + \n  theme_minimal()\n\n\n\n\nWe have started to unpack our estimated model for the linear relationship between a country’s (logged) GDP per capita and its average life expectancy.\n\\[\nlife Exp_x = \\beta_0 + \\beta_1 logGdpPerCap_x + \\epsilon\n\\]\nRead this as: the life expectancy of some country, \\(x\\), is a function of some constant (\\(\\beta_0\\)) and its logged GDP per capita transformed by some value \\(\\beta_1\\) with some random error (\\(\\epsilon\\)).\nHow do we calculate the constant (\\(\\beta_0\\)) and \\(\\beta_1\\)?\n\n\nEstimating a linear model in R\n\nm <- lm(life_exp ~ log_gdp_per_cap, data = gapminder_df)\n\ntidy(m)\n\n# A tibble: 2 × 5\n  term            estimate std.error statistic  p.value\n  <chr>              <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)        32.8      1.72       19.0 2.38e-46\n2 log_gdp_per_cap     4.54     0.195      23.3 2.36e-58\n\n\nIt’s all about the error! What line can I draw through all of my data points that minimizes the distance between the line and every data point? R has done this work for us.\n\n\nPrediction\nWe can use this model to predict a country’s life expectancy given its GDP per capita.\nWhat is the life expectancy for a country with a GDP per capita of $10,000? First, let’s find the estimated constant (or intercept or \\(\\beta_0\\)).\n\nm_res <- tidy(m)\n\nbeta_0 <- m_res |> \n  filter(term == \"(Intercept)\") |> \n  pull(estimate)\n\nbeta_0\n\n[1] 32.77775\n\n\nThen we need to find the estimated coefficient for (logged) GDP per capita:\n\nbeta_1 <- m_res |> \n  filter(term == \"log_gdp_per_cap\") |> \n  pull(estimate)\n\nbeta_1\n\n[1] 4.543066\n\n\nFinally, we can plug this in to our model:\n\nlife_exp_10000 <- beta_0 + beta_1 * log(10000)\nlife_exp_10000\n\n[1] 74.62093\n\n\nA country with a GDP per capita of $10,000 is predicted to have an average life expectancy of 75 years. Does this make sense with our data?\n\nggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  geom_vline(xintercept = log(10000)) + \n  geom_hline(yintercept = life_exp_10000) + \n  geom_smooth(method = \"lm\", se = F) + \n  theme_minimal()\n\n\n\n\nWe can predict values from a model using broom::augment():\n\naugment(m, newdata = tibble(log_gdp_per_cap = log(10000)))\n\n# A tibble: 1 × 2\n  log_gdp_per_cap .fitted\n            <dbl>   <dbl>\n1            9.21    74.6\n\n\nWe can do this across a number of different values for GDP per capita:\n\nnew_data <- tibble(\n  gdp_per_cap = seq(from = 10000, to = 50000, by = 1000),\n  log_gdp_per_cap = log(gdp_per_cap)\n)\n\naugment(m, newdata = new_data)\n\n# A tibble: 41 × 3\n   gdp_per_cap log_gdp_per_cap .fitted\n         <dbl>           <dbl>   <dbl>\n 1       10000            9.21    74.6\n 2       11000            9.31    75.1\n 3       12000            9.39    75.4\n 4       13000            9.47    75.8\n 5       14000            9.55    76.1\n 6       15000            9.62    76.5\n 7       16000            9.68    76.8\n 8       17000            9.74    77.0\n 9       18000            9.80    77.3\n10       19000            9.85    77.5\n# ℹ 31 more rows\n\n\nWe can also use this function to see how well our model predicts for our own data:\n\naugment(m)\n\n# A tibble: 198 × 9\n   .rownames life_exp log_gdp_per_cap .fitted .resid    .hat .sigma   .cooksd\n   <chr>        <dbl>           <dbl>   <dbl>  <dbl>   <dbl>  <dbl>     <dbl>\n 1 1             75.6           10.3     79.4 -3.75  0.0106    4.03 0.00471  \n 2 2             63.1            6.25    61.2  1.94  0.0193    4.03 0.00234  \n 3 3             61.1            7.44    66.6 -5.50  0.00886   4.02 0.00844  \n 4 4             78.9            8.32    70.6  8.26  0.00541   3.99 0.0115   \n 5 6             79.3           10.6     81.0 -1.70  0.0136    4.03 0.00125  \n 6 7             76.3            9.46    75.7  0.569 0.00633   4.04 0.0000641\n 7 8             74.7            8.21    70.1  4.58  0.00566   4.02 0.00371  \n 8 10            78.2            9.67    76.7  1.43  0.00719   4.03 0.000463 \n 9 11            82.4           10.8     81.9  0.527 0.0154    4.04 0.000136 \n10 12            81.6           10.7     81.5  0.156 0.0145    4.04 0.0000113\n# ℹ 188 more rows\n# ℹ 1 more variable: .std.resid <dbl>\n\n\nHere, we have the predicted values for life expectancy for all of our countries in our sample. Compare .fitted (the predicted life expectancy) to life_exp (the actual observed average life expectancy).\n\nm_eval <- augment(m) |> \n  transmute(\n    life_exp, \n    .fitted,\n    diff = life_exp - .fitted\n  )\n\nm_eval\n\n# A tibble: 198 × 3\n   life_exp .fitted   diff\n      <dbl>   <dbl>  <dbl>\n 1     75.6    79.4 -3.75 \n 2     63.1    61.2  1.94 \n 3     61.1    66.6 -5.50 \n 4     78.9    70.6  8.26 \n 5     79.3    81.0 -1.70 \n 6     76.3    75.7  0.569\n 7     74.7    70.1  4.58 \n 8     78.2    76.7  1.43 \n 9     82.4    81.9  0.527\n10     81.6    81.5  0.156\n# ℹ 188 more rows\n\n\nNote that broom::augment() does this calculation and stores it in the .resid variable.\n\naugment(m) |> \n  transmute(\n    life_exp, \n    .fitted,\n    diff = life_exp - .fitted,\n    .resid\n  )\n\n# A tibble: 198 × 4\n   life_exp .fitted   diff .resid\n      <dbl>   <dbl>  <dbl>  <dbl>\n 1     75.6    79.4 -3.75  -3.75 \n 2     63.1    61.2  1.94   1.94 \n 3     61.1    66.6 -5.50  -5.50 \n 4     78.9    70.6  8.26   8.26 \n 5     79.3    81.0 -1.70  -1.70 \n 6     76.3    75.7  0.569  0.569\n 7     74.7    70.1  4.58   4.58 \n 8     78.2    76.7  1.43   1.43 \n 9     82.4    81.9  0.527  0.527\n10     81.6    81.5  0.156  0.156\n# ℹ 188 more rows\n\n\n\n\nPerformance\nIs this the best model we can produce to predict life expectancy?\n\nggplot(augment(m), aes(x = .resid)) + \n  geom_density() +\n  geom_vline(xintercept = 0) + \n  theme_minimal()\n\n\n\n\nOur model appears to be doing mildly well, with most predictions within one or two years of the observed value. However, some are as far away as 16 years! That’s quite a lot.\n\nCan you see for which points these large differences exist?\n\n\nggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = F) + \n  theme_minimal()\n\n\n\n\nDoes this model perform well? This depends a lot on the question you are asking.\n\n\nModelling the relationship between many variables\n\nggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp, colour = region)) + \n  geom_point()\n\n\n\n\n\n\nCross tabs\nDoes the average life expectancy vary by region?\n\nxtabs(life_exp ~ region, data = gapminder_df)\n\nregion\n       East Asia & Pacific      Europe & Central Asia \n                 2368.2936                  4280.8130 \n Latin America & Caribbean Middle East & North Africa \n                 2919.8775                  1574.3790 \n             North America                 South Asia \n                  241.6661                   564.8040 \n        Sub-Saharan Africa \n                 2967.2066 \n\n\nThis is nice to visualise:\n\ngapminder_df |> \n  group_by(region) |> \n  summarise(avg = mean(life_exp, na.rm = T)) |> \n  ggplot(aes(x = avg, y = reorder(region, avg))) + \n  geom_col() + \n  theme_minimal()\n\n\n\n\nOr even better:\n\nggplot(gapminder_df, aes(x = life_exp, y = region)) + \n  stat_density_ridges(quantile_lines = T, quantiles = 2) + \n  theme_minimal()\n\n\n\n\nAre these differences meaningful or significant? We will chat about that next week."
  },
  {
    "objectID": "content/03-content.html#visualizing-the-linear-relationship-between-two-continuous-variables",
    "href": "content/03-content.html#visualizing-the-linear-relationship-between-two-continuous-variables",
    "title": "Bivariate Relationships",
    "section": "Visualizing the linear relationship between two continuous variables",
    "text": "Visualizing the linear relationship between two continuous variables\n\nggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = F) + \n  theme_minimal()\n\n\n\n\nWe have started to unpack our estimated model for the linear relationship between a country’s (logged) GDP per capita and its average life expectancy.\n\\[\nlife Exp_x = \\beta_0 + \\beta_1 logGdpPerCap_x\n\\]\nRead this as: the life expectancy of some country, \\(x\\), is a function of some constant (\\(\\beta_0\\)) and the its logged GDP per capita transformed by some value \\(\\beta_1\\).\nHow do we calculate the constant (\\(\\beta_0\\)) and \\(\\beta_1\\)?\n\nEstimating a linear model in R\n\nm <- lm(life_exp ~ log_gdp_per_cap, data = gapminder_df)\n\ntidy(m)\n\n# A tibble: 2 × 5\n  term            estimate std.error statistic  p.value\n  <chr>              <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)        32.8      1.72       19.0 2.38e-46\n2 log_gdp_per_cap     4.54     0.195      23.3 2.36e-58"
  },
  {
    "objectID": "content/02-content.html#section",
    "href": "content/02-content.html#section",
    "title": "Descriptive Statistics",
    "section": "Section",
    "text": "Section\n\nPrerequisites\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(skimr)\nlibrary(patchwork)\n\n\n\nDescribing categorical variables\nImagine that we have completed a survey of 10,000 individuals. We asked them their age and level of satisfaction with their job.\n\n\n\nWe store their responses in a data frame called survey_df. It has 10,000 observations (one for each respondent) and three variables: a unique id (id); their age in years (age); and their level of satisfaction (sat), which can take one of four values: very unsatisfied, unsatisfied, satisfied, and very satisfied.\n\nsurvey_df\n\n# A tibble: 10,000 × 3\n      id   age sat             \n   <int> <int> <chr>           \n 1     1    31 Satisfied       \n 2     2    43 Unsatisfied     \n 3     3    23 Unsatisfied     \n 4     4    47 Satisfied       \n 5     5    33 Very unsatisfied\n 6     6    64 Unsatisfied     \n 7     7    51 Very unsatisfied\n 8     8    64 Unsatisfied     \n 9     9    26 Very unsatisfied\n10    10    38 Very unsatisfied\n# ℹ 9,990 more rows\n\n\n\nFrequency distribution\nWe can take advantage of janitor::tabyl() to quickly calculate the number and proportion of respondents who provided each level of satisfaction.\n\ntabyl(survey_df, sat)\n\n              sat    n percent\n        Satisfied 2495  0.2495\n      Unsatisfied 2497  0.2497\n   Very satisfied 2513  0.2513\n Very unsatisfied 2495  0.2495\n\n\nAlternatively, we can use skimr::skim() to get a useful summary of this categorical variable.\n\nskim(survey_df$sat)\n\n\nData summary\n\n\nName\nsurvey_df$sat\n\n\nNumber of rows\n10000\n\n\nNumber of columns\n1\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ndata\n0\n1\n9\n16\n0\n4\n0\n\n\n\n\n\n\n\nVisualizing this frequency\nWe can easily visualize this using a bar chart.\n\np1 <- survey_df |> \n  tabyl(sat) |> \n  ggplot(aes(x = n, y = sat)) + \n  geom_col() + \n  theme_minimal()\n\np2 <- survey_df |> \n  tabyl(sat) |> \n  ggplot(aes(x = percent, y = sat)) + \n  geom_col() + \n  theme_minimal()\n\np1 | p2\n\n\n\n\n\n\nWorking with factors\nNotice how our categories are ordered: very satisfied sits above satisfied. We can tell R this information factor().\n\nsurvey_df <- survey_df |> \n  mutate(\n    sat = factor(sat, levels = c(\"Very unsatisfied\", \n                                 \"Unsatisfied\", \n                                 \"Satisfied\", \n                                 \"Very satisfied\"))\n  )\n\nNow when we work with our categorical variables, they will be ordered.\n\nsurvey_df |> \n  tabyl(sat) |> \n  ggplot(aes(x = n, y = sat)) + \n  geom_col() + \n  theme_minimal()\n\n\n\n\n\n\n\nDescribing continuous variables\nWe can also get a good sense of our continuous variable age by looking at the center, spread, and shape of its distribution.\n\nFive number summary\nWe can use skimr::skim() to quickly get useful information on our continuous variable.\n\nskim(survey_df$age)\n\n\nData summary\n\n\nName\nsurvey_df$age\n\n\nNumber of rows\n10000\n\n\nNumber of columns\n1\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ndata\n0\n1\n39.92\n14.67\n15\n27\n40\n53\n65\n▇▇▇▇▇\n\n\n\n\n\n\n\nHistogram\n\nggplot(survey_df, aes(x = age)) + \n  geom_histogram() + \n  theme_minimal()\n\n\n\n\n\nggplot(survey_df, aes(x = age)) + \n  geom_histogram(binwidth = 5) + \n  theme_minimal()\n\n\n\n\n\n\nDensity curves\n\nggplot(survey_df, aes(x = age)) + \n  geom_density() + \n  theme_minimal()\n\n\n\n\n\n\nBox and whisker plots\n\nggplot(survey_df, aes(x = age)) + \n  geom_boxplot() + \n  theme_minimal()\n\n\n\n\n\n\nLooking for patterns in our groups\n\nggplot(survey_df, aes(x = age, y = sat)) + \n  geom_boxplot() + \n  theme_minimal()\n\n\n\n\n\nggplot(survey_df, aes(x = age, y = sat)) + \n  geom_violin() + \n  theme_minimal()\n\n\n\n\n\n\n\nUnderstanding distributions\n\nNormal distribution\n\ntibble(z = rnorm(n = 1000)) |> \n  ggplot(aes(x = z)) + \n  geom_histogram() + \n  theme_minimal()\n\n\n\n\n\ntibble(z = rnorm(n = 1e6)) |> \n  ggplot(aes(x = z)) + \n  geom_histogram() + \n  theme_minimal()\n\n\n\n\n\n\nRight skewed distribution\n\ntibble(z = rbeta(10000, 2, 10)) |> \n  ggplot(aes(x = z)) + \n  geom_histogram() + \n  theme_minimal()\n\n\n\n\n\n\nLeft skewed distribution\n\ntibble(z = rbeta(10000, 10, 2)) |> \n  ggplot(aes(x = z)) + \n  geom_histogram() + \n  theme_minimal()\n\n\n\n\n\n\n\nMeasures of central tendency: mean, median, and mode\n\nMean\nThe mean is the average of all values.\n\n\nMedian\nThe median is the mid-point of all values.\n\n\nMode\nThe mode is the most frequent of all values.\n\n\nUsing central tendency to describe and understand distributions\nNormally distributed vectors share their mean and medians.\n\nnorm_dist <- tibble(z = rnorm(n = 1000))\n\nggplot(norm_dist, aes(x = z)) + \n  geom_histogram() + \n  geom_vline(xintercept = mean(norm_dist$z), colour = \"red\") + \n  geom_vline(xintercept = median(norm_dist$z), colour = \"blue\") + \n  theme_minimal()\n\n\n\n\nFor right skewed data, the mean is greater than the median.\n\nright_dist <- tibble(z = rbeta(10000, 2, 10))\n\nggplot(right_dist, aes(x = z)) + \n  geom_histogram() + \n  geom_vline(xintercept = mean(right_dist$z), colour = \"red\") + \n  geom_vline(xintercept = median(right_dist$z), colour = \"blue\") + \n  theme_minimal()\n\n\n\n\nFor left skewed data, the mean is smaller than the median.\n\nleft_dist <- tibble(z = rbeta(10000, 10, 2))\n\nggplot(left_dist, aes(x = z)) + \n  geom_histogram() + \n  geom_vline(xintercept = mean(left_dist$z), colour = \"red\") + \n  geom_vline(xintercept = median(left_dist$z), colour = \"blue\") + \n  theme_minimal()\n\n\n\n\n\n\n\nMeasures of spread: range, variance, and standard deviation\n\nRange\nThe range is the difference between the largest and smallest value.\n\nmax(survey_df$age) - min(survey_df$age)\n\n[1] 50\n\n\n\n\nVariance\nThe variance measures how spread out your values are. Take a look at these two plots. Both have the same center point (0) and number of observations (1,000,000). However, the data are much more spread out around that center point in the top graph.\n\nwide_dist <- tibble(z = rnorm(1e6, sd = 2))\n\np1 <- ggplot(wide_dist, aes(x = z)) + \n  geom_histogram() + \n  geom_vline(xintercept = 0) + \n  theme_minimal() + \n  scale_x_continuous(limits = c(-4, 4))\n\nnarrow_dist <- tibble(z = rnorm(1e6, sd = 1))\n\np2 <- ggplot(narrow_dist, aes(x = z)) + \n  geom_histogram() + \n  geom_vline(xintercept = 0) + \n  theme_minimal() + \n  scale_x_continuous(limits = c(-4, 4))\n\np1 / p2\n\n\n\n\nThe data in the top graph have far more variance than those in the bottom graph. We measure this by calculating the average of the squares of the deviations of the observations from their mean.\n\\[\ns^2 = \\frac{\\Sigma(x_i - \\bar{x})^2}{n - 1}\n\\]\nLet’s step through this. We will first calculate the variance for wide_dist, or the top graph.\n\nwide_var_calc <- wide_dist |> \n  mutate(\n    mean = mean(wide_dist$z),\n    diff = z - mean,\n    diff_2 = diff^2\n  )\n\nwide_var_calc\n\n# A tibble: 1,000,000 × 4\n        z      mean   diff  diff_2\n    <dbl>     <dbl>  <dbl>   <dbl>\n 1 -0.373 -0.000426 -0.372  0.139 \n 2 -1.90  -0.000426 -1.90   3.62  \n 3 -1.46  -0.000426 -1.46   2.14  \n 4 -1.03  -0.000426 -1.03   1.06  \n 5 -3.53  -0.000426 -3.52  12.4   \n 6  2.63  -0.000426  2.63   6.91  \n 7  2.72  -0.000426  2.72   7.38  \n 8  0.268 -0.000426  0.269  0.0721\n 9  2.89  -0.000426  2.89   8.34  \n10 -1.45  -0.000426 -1.45   2.11  \n# ℹ 999,990 more rows\n\n\nWe take the sum of square of the difference between each observation and the mean of our whole sample. We then divide that by one less than our number of observations.\n\nwide_var <- sum(wide_var_calc$diff_2) / (nrow(wide_var_calc) - 1)\n\nwide_var\n\n[1] 3.997788\n\n\nWe can compare this to the variance for our narrower distribution.\n\nnarrow_var_calc <- narrow_dist |> \n  mutate(\n    mean = mean(narrow_dist$z),\n    diff = z - mean,\n    diff_2 = diff^2\n  )\n\nnarrow_var <- sum(narrow_var_calc$diff_2) / (nrow(narrow_var_calc) - 1)\n\nnarrow_var\n\n[1] 0.9990178\n\n\nIt is, in fact, smaller!\nWe can use var() to do this in one step:\n\nvar(wide_dist)\n\n         z\nz 3.997788\n\n\n\nvar(narrow_dist)\n\n          z\nz 0.9990178\n\n\n\n\nStandard deviation\nA simpler measure of spread is the standard deviation. It is simply the square root of the variance.\n\nsqrt(wide_var)\n\n[1] 1.999447\n\n\n\nsqrt(narrow_var)\n\n[1] 0.9995088\n\n\nIf you look back to our graphs, you will see that I set the standard deviations explicitly when I generated the data. rnorm() takes an sd argument. This is great because we can confirm that the standard deviations for the wide and narrow distributions are 2 and 1 respectively (with a little bit of noise).\n\ntibble(\n  n = rnorm(1e6, sd = 1),\n  w = rnorm(1e6, sd = 2)\n) |> \n  ggplot() + \n  geom_density(aes(x = n), colour = \"green\") + \n  geom_density(aes(x = w), colour = \"lightblue\") + \n  theme_minimal()\n\n\n\n\n\n\n\nStandardization\nNotice how our description of each variable depends on its units of measurement. What do we do if we want to compare across different measurements that have different units?\n\nZ scores\nFor normal distributions, we can use the z score. This gives us a standard way of understanding how many standard deviations from the mean of a normally distributed variable a value is.\n\\[\nz_i = \\frac{x_i - \\mu_x}{\\sigma_x}\n\\]"
  }
]