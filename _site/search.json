[
  {
    "objectID": "content/01-content.html",
    "href": "content/01-content.html",
    "title": "Introduction and Set Up",
    "section": "",
    "text": "Garrick Aden-Buie’s macOS set up blogpost\n Matti Vuorre’s macOS set up blogpost\n\n\n\n Installation and Connect Git, GitHub, RStudio sections in Jennifer Bryan’s Happy Git and GitHub for the useR\n\n\n\n Install or upgrade R and RStudio chapter in Jennifer Bryan’s Happy Git and GitHub for the useR"
  },
  {
    "objectID": "content/01-content.html#slides",
    "href": "content/01-content.html#slides",
    "title": "Introduction and Set Up",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "content/01-content.html#in-class",
    "href": "content/01-content.html#in-class",
    "title": "Introduction and Set Up",
    "section": "In-class",
    "text": "In-class"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GVPT622",
    "section": "",
    "text": "Quantitative Methods for Political Science\n        \n        \n            An introduction to research methods and quantitative research in political science.\n        \n        \n            Fall 2023Department of Government and PoliticsUniversity of Maryland, College Park\n        \n    \n    \n      \n        \n        \n        \n      \n    \n\n\n\n\n\nTeaching Assistant\n\n   Harriet Goers\n   Chincoteague Building\n   hgoers@umd.edu\n   hgoers\n\n\n\nCourse details\n\n   January 9–May 2, 2023\n   7:15–9:45 PM\n   Tydings Building\n   Slack\n\n\n\nContacting me\nE-mail and Slack are the best ways to get in contact with me. I will try to respond to all course-related e-mails and Slack messages within 24 hours."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "DATE \n    TITLE \n    CLASS SLIDES \n    SESSION SCRIPTS \n    ASSIGNMENTS \n  \n \n\n  \n    1 \n    Course Introduction \n     \n     \n     \n  \n  \n    2 \n    Descriptive Statistics"
  },
  {
    "objectID": "content/01-content.html#class-slides",
    "href": "content/01-content.html#class-slides",
    "title": "Course Introduction",
    "section": "Class slides",
    "text": "Class slides"
  },
  {
    "objectID": "content/01-content.html#session-scipts",
    "href": "content/01-content.html#session-scipts",
    "title": "Course Introduction",
    "section": "Session scipts",
    "text": "Session scipts"
  },
  {
    "objectID": "content/02-content.html#class-slides",
    "href": "content/02-content.html#class-slides",
    "title": "Descriptive Statistics",
    "section": "Class slides",
    "text": "Class slides"
  },
  {
    "objectID": "content/02-content.html#session-scripts",
    "href": "content/02-content.html#session-scripts",
    "title": "Descriptive Statistics",
    "section": "Session scripts",
    "text": "Session scripts"
  },
  {
    "objectID": "content/01-content.html#session-scripts",
    "href": "content/01-content.html#session-scripts",
    "title": "Course Introduction",
    "section": "Session scripts",
    "text": "Session scripts"
  },
  {
    "objectID": "content/02-content.html#session",
    "href": "content/02-content.html#session",
    "title": "Descriptive Statistics",
    "section": "Session",
    "text": "Session\n\nPrerequisites\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(skimr)\nlibrary(patchwork)\n\n\n\nDescribing categorical variables\nImagine that we have completed a survey of 10,000 individuals. We asked them their age and level of satisfaction with their job.\n\n\n\nWe store their responses in a data frame called survey_df. It has 10,000 observations (one for each respondent) and three variables: a unique id (id); their age in years (age); and their level of satisfaction (sat), which can take one of four values: very unsatisfied, unsatisfied, satisfied, and very satisfied.\n\nsurvey_df\n\n# A tibble: 10,000 × 3\n      id   age sat             \n   <int> <int> <chr>           \n 1     1    20 Unsatisfied     \n 2     2    27 Very unsatisfied\n 3     3    42 Very satisfied  \n 4     4    20 Very unsatisfied\n 5     5    37 Unsatisfied     \n 6     6    26 Satisfied       \n 7     7    60 Very satisfied  \n 8     8    63 Very unsatisfied\n 9     9    42 Satisfied       \n10    10    40 Very unsatisfied\n# ℹ 9,990 more rows\n\n\n\nFrequency distribution\nWe can take advantage of janitor::tabyl() to quickly calculate the number and proportion of respondents who provided each level of satisfaction.\n\ntabyl(survey_df, sat)\n\n              sat    n percent\n        Satisfied 2579  0.2579\n      Unsatisfied 2414  0.2414\n   Very satisfied 2508  0.2508\n Very unsatisfied 2499  0.2499\n\n\nAlternatively, we can use skimr::skim() to get a useful summary of this categorical variable.\n\nskim(survey_df$sat)\n\n\nData summary\n\n\nName\nsurvey_df$sat\n\n\nNumber of rows\n10000\n\n\nNumber of columns\n1\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ndata\n0\n1\n9\n16\n0\n4\n0\n\n\n\n\n\n\n\nVisualizing this frequency\nWe can easily visualize this using a bar chart.\n\np1 <- survey_df |> \n  tabyl(sat) |> \n  ggplot(aes(x = n, y = sat)) + \n  geom_col() + \n  theme_minimal()\n\np2 <- survey_df |> \n  tabyl(sat) |> \n  ggplot(aes(x = percent, y = sat)) + \n  geom_col() + \n  theme_minimal()\n\np1 | p2\n\n\n\n\n\n\nWorking with factors\nNotice how our categories are ordered: very satisfied sits above satisfied. We can tell R this information factor().\n\nsurvey_df <- survey_df |> \n  mutate(\n    sat = factor(sat, levels = c(\"Very unsatisfied\", \n                                 \"Unsatisfied\", \n                                 \"Satisfied\", \n                                 \"Very satisfied\"))\n  )\n\nNow when we work with our categorical variables, they will be ordered.\n\nsurvey_df |> \n  tabyl(sat) |> \n  ggplot(aes(x = n, y = sat)) + \n  geom_col() + \n  theme_minimal()\n\n\n\n\n\n\n\nDescribing continuous variables\nWe can also get a good sense of our continuous variable age by looking at the center, spread, and shape of its distribution.\n\nFive number summary\nWe can use skimr::skim() to quickly get useful information on our continuous variable.\n\nskim(survey_df$age)\n\n\nData summary\n\n\nName\nsurvey_df$age\n\n\nNumber of rows\n10000\n\n\nNumber of columns\n1\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ndata\n0\n1\n39.96\n14.79\n15\n27\n40\n53\n65\n▇▇▇▇▇\n\n\n\n\n\n\n\nHistogram\n\nggplot(survey_df, aes(x = age)) + \n  geom_histogram() + \n  theme_minimal()\n\n\n\n\n\nggplot(survey_df, aes(x = age)) + \n  geom_histogram(binwidth = 5) + \n  theme_minimal()\n\n\n\n\n\n\nDensity curves\n\nggplot(survey_df, aes(x = age)) + \n  geom_density() + \n  theme_minimal()\n\n\n\n\n\n\nBox and whisker plots\n\nggplot(survey_df, aes(x = age)) + \n  geom_boxplot() + \n  theme_minimal()\n\n\n\n\n\n\nLooking for patterns in our groups\n\nggplot(survey_df, aes(x = age, y = sat)) + \n  geom_boxplot() + \n  theme_minimal()\n\n\n\n\n\nggplot(survey_df, aes(x = age, y = sat)) + \n  geom_violin() + \n  theme_minimal()\n\n\n\n\n\n\n\nUnderstanding distributions\n\nNormal distribution\n\ntibble(z = rnorm(n = 1000)) |> \n  ggplot(aes(x = z)) + \n  geom_histogram() + \n  theme_minimal()\n\n\n\n\n\ntibble(z = rnorm(n = 1e6)) |> \n  ggplot(aes(x = z)) + \n  geom_histogram() + \n  theme_minimal()\n\n\n\n\n\n\nRight skewed distribution\n\ntibble(z = rbeta(10000, 2, 10)) |> \n  ggplot(aes(x = z)) + \n  geom_histogram() + \n  theme_minimal()\n\n\n\n\n\n\nLeft skewed distribution\n\ntibble(z = rbeta(10000, 10, 2)) |> \n  ggplot(aes(x = z)) + \n  geom_histogram() + \n  theme_minimal()\n\n\n\n\n\n\n\nMeasures of central tendency: mean, median, and mode\n\nMean\nThe mean is the average of all values.\n\n\nMedian\nThe median is the mid-point of all values.\n\n\nMode\nThe mode is the most frequent of all values.\n\n\nUsing central tendency to describe and understand distributions\nNormally distributed vectors share their mean and medians.\n\nnorm_dist <- tibble(z = rnorm(n = 1000))\n\nggplot(norm_dist, aes(x = z)) + \n  geom_histogram() + \n  geom_vline(xintercept = mean(norm_dist$z), colour = \"red\") + \n  geom_vline(xintercept = median(norm_dist$z), colour = \"blue\") + \n  theme_minimal()\n\n\n\n\nFor right skewed data, the mean is greater than the median.\n\nright_dist <- tibble(z = rbeta(10000, 2, 10))\n\nggplot(right_dist, aes(x = z)) + \n  geom_histogram() + \n  geom_vline(xintercept = mean(right_dist$z), colour = \"red\") + \n  geom_vline(xintercept = median(right_dist$z), colour = \"blue\") + \n  theme_minimal()\n\n\n\n\nFor left skewed data, the mean is smaller than the median.\n\nleft_dist <- tibble(z = rbeta(10000, 10, 2))\n\nggplot(left_dist, aes(x = z)) + \n  geom_histogram() + \n  geom_vline(xintercept = mean(left_dist$z), colour = \"red\") + \n  geom_vline(xintercept = median(left_dist$z), colour = \"blue\") + \n  theme_minimal()\n\n\n\n\n\n\n\nMeasures of spread: range, variance, and standard deviation\n\nRange\nThe range is the difference between the largest and smallest value.\n\nmax(survey_df$age) - min(survey_df$age)\n\n[1] 50\n\n\n\n\nVariance\nThe variance measures how spread out your values are. Take a look at these two plots. Both have the same center point (0) and number of observations (1,000,000). However, the data are much more spread out around that center point in the top graph.\n\nwide_dist <- tibble(z = rnorm(1e6, sd = 2))\n\np1 <- ggplot(wide_dist, aes(x = z)) + \n  geom_histogram() + \n  geom_vline(xintercept = 0) + \n  theme_minimal() + \n  scale_x_continuous(limits = c(-4, 4))\n\nnarrow_dist <- tibble(z = rnorm(1e6, sd = 1))\n\np2 <- ggplot(narrow_dist, aes(x = z)) + \n  geom_histogram() + \n  geom_vline(xintercept = 0) + \n  theme_minimal() + \n  scale_x_continuous(limits = c(-4, 4))\n\np1 / p2\n\n\n\n\nThe data in the top graph have far more variance than those in the bottom graph. We measure this by calculating the average of the squares of the deviations of the observations from their mean.\n\\[\ns^2 = \\frac{\\Sigma(x_i - \\bar{x})^2}{n - 1}\n\\]\nLet’s step through this. We will first calculate the variance for wide_dist, or the top graph.\n\nwide_var_calc <- wide_dist |> \n  mutate(\n    mean = mean(wide_dist$z),\n    diff = z - mean,\n    diff_2 = diff^2\n  )\n\nwide_var_calc\n\n# A tibble: 1,000,000 × 4\n         z    mean    diff  diff_2\n     <dbl>   <dbl>   <dbl>   <dbl>\n 1 -0.921  0.00268 -0.924  0.853  \n 2  1.63   0.00268  1.63   2.65   \n 3 -0.0416 0.00268 -0.0443 0.00196\n 4  0.779  0.00268  0.777  0.603  \n 5  0.494  0.00268  0.492  0.242  \n 6 -0.738  0.00268 -0.740  0.548  \n 7 -0.130  0.00268 -0.133  0.0176 \n 8 -1.58   0.00268 -1.58   2.51   \n 9  1.82   0.00268  1.82   3.30   \n10 -0.127  0.00268 -0.129  0.0167 \n# ℹ 999,990 more rows\n\n\nWe take the sum of square of the difference between each observation and the mean of our whole sample. We then divide that by one less than our number of observations.\n\nwide_var <- sum(wide_var_calc$diff_2) / (nrow(wide_var_calc) - 1)\n\nwide_var\n\n[1] 4.00025\n\n\nWe can compare this to the variance for our narrower distribution.\n\nnarrow_var_calc <- narrow_dist |> \n  mutate(\n    mean = mean(narrow_dist$z),\n    diff = z - mean,\n    diff_2 = diff^2\n  )\n\nnarrow_var <- sum(narrow_var_calc$diff_2) / (nrow(narrow_var_calc) - 1)\n\nnarrow_var\n\n[1] 1.000964\n\n\nIt is, in fact, smaller!\nWe can use var() to do this in one step:\n\nvar(wide_dist)\n\n        z\nz 4.00025\n\n\n\nvar(narrow_dist)\n\n         z\nz 1.000964\n\n\n\n\nStandard deviation\nA simpler measure of spread is the standard deviation. It is simply the square root of the variance.\n\nsqrt(wide_var)\n\n[1] 2.000063\n\n\n\nsqrt(narrow_var)\n\n[1] 1.000482\n\n\nIf you look back to our graphs, you will see that I set the standard deviations explicitly when I generated the data. rnorm() takes an sd argument. This is great because we can confirm that the standard deviations for the wide and narrow distributions are 2 and 1 respectively (with a little bit of noise).\n\ntibble(\n  n = rnorm(1e6, sd = 1),\n  w = rnorm(1e6, sd = 2)\n) |> \n  ggplot() + \n  geom_density(aes(x = n), colour = \"green\") + \n  geom_density(aes(x = w), colour = \"lightblue\") + \n  theme_minimal()\n\n\n\n\n\n\n\nStandardization\nNotice how our description of each variable depends on its units of measurement. What do we do if we want to compare across different measurements that have different units?\n\nZ scores\nFor normal distributions, we can use the z score. This gives us a standard way of understanding how many standard deviations from the mean of a normally distributed variable a value is.\n\\[\nz_i = \\frac{x_i - \\mu_x}{\\sigma_x}\n\\]"
  },
  {
    "objectID": "content/03-content.html#class-slides",
    "href": "content/03-content.html#class-slides",
    "title": "Bivariate Relationships",
    "section": "Class slides",
    "text": "Class slides"
  },
  {
    "objectID": "content/03-content.html#section",
    "href": "content/03-content.html#section",
    "title": "Bivariate Relationships",
    "section": "Section",
    "text": "Section\n\nPrerequisites\n\nlibrary(tidyverse)\nlibrary(wbstats)\nlibrary(poliscidata)\nlibrary(countrycode)\nlibrary(broom)\nlibrary(janitor)\nlibrary(ggridges)\n\nToday, we will explore the relationship between wealth and health. This question was made popular by Hans Rosling’s Gapminder project.\nFirst, we need to collect our data. We will use wbstats::wb_data() to pull these data directly from the World Bank.\n\ngapminder_df <- wb_data(\n  indicator = c(\"SP.DYN.LE00.IN\", \"NY.GDP.PCAP.CD\"),\n  start_date = 2016,\n  end_date = 2016\n) |> \n  rename(\n    life_exp = SP.DYN.LE00.IN,\n    gdp_per_cap = NY.GDP.PCAP.CD\n  ) |> \n  mutate(\n    log_gdp_per_cap = log(gdp_per_cap),\n    region = countrycode(country, \"country.name\", \"region\", custom_match = c(\"Turkiye\" = \"Europe & Central Asia\"))\n  ) |> \n  relocate(region, .after = country)\n\ngapminder_df\n\n# A tibble: 217 × 8\n   iso2c iso3c country         region  date gdp_per_cap life_exp log_gdp_per_cap\n   <chr> <chr> <chr>           <chr>  <dbl>       <dbl>    <dbl>           <dbl>\n 1 AW    ABW   Aruba           Latin…  2016      28451.     75.6           10.3 \n 2 AF    AFG   Afghanistan     South…  2016        520.     63.1            6.25\n 3 AO    AGO   Angola          Sub-S…  2016       1710.     61.1            7.44\n 4 AL    ALB   Albania         Europ…  2016       4124.     78.9            8.32\n 5 AD    AND   Andorra         Europ…  2016      39932.     NA             10.6 \n 6 AE    ARE   United Arab Em… Middl…  2016      41055.     79.3           10.6 \n 7 AR    ARG   Argentina       Latin…  2016      12790.     76.3            9.46\n 8 AM    ARM   Armenia         Europ…  2016       3680.     74.7            8.21\n 9 AS    ASM   American Samoa  East …  2016      13301.     NA              9.50\n10 AG    ATG   Antigua and Ba… Latin…  2016      15863.     78.2            9.67\n# ℹ 207 more rows\n\n\n\n\nVisualizing bivariate relationships: two continuous variables\n\nggplot(gapminder_df, aes(x = gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  theme_minimal()\n\n\n\n\nThere seems to be a very strong case that there is a relationship between a country’s GDP per capita (wealth) and its average life expectancy (health).\nBecause we want to explore linear relationships at this stage of the course, we will look at the logged GDP per capita variable:\n\nggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  theme_minimal()\n\n\n\n\nI can imagine drawing a straight line between these points that roughly explains how they vary with each other. It appears that as a country’s logged GDP per capita increases, so too does the life expectancy for its population.\n\n\nCorrelations\nWe can measure the strength of this association using correlations. The correlation coefficient tells us how closely variables relate to one another. It tells us both the strength and direction of the association.\n\nStrength: how closely are these values tied to one another. Measured from 0 to |1|, with values closer to 0 indicating a very weak relationship and values closer to |1| indicating a very strong relationship.\nDirection: do both \\(X\\) and \\(Y\\) change in the same direction? Positive correlations show that when \\(X\\) increases (decreases), so too does \\(Y\\). Negative correlations show that when \\(X\\) increases (decreases), \\(Y\\) decreases (increases). In other words, the move in different directions.\n\nWhat is the correlation between logged GDP per capita and life expectancy?\n\ncor(gapminder_df$log_gdp_per_cap, gapminder_df$life_exp, use = \"complete.obs\")\n\n[1] 0.8475933\n\n\nAs expected, the relationship is positive and strong.\n\n\nVisualizing the linear relationship between two continuous variables\nLet’s draw a line across all of these data points and try to understand how life expectancy increases with logged GDP per capita.\n\nggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  theme_minimal()\n\n\n\n\nWe can, of course, draw many different lines across these points. Which is the best line to draw?\nThis course focuses on ordinary least squares (OLS) regression. Simply put, OLS regression draws the line that minimizes the distance between itself and all of the dots.\nLet’s step through this. Look at the graph above.\n\nDraw a line through those dots.\nCalculate the distance between each dot and the line.\nSum up the absolute values of those distances. Remember, we just care about the distance, so we don’t need to worry about whether or not the dots are above or below the line.\nRepeat steps 1 - 3 many, many, many times.\nPick the line with the smallest sum of distances.\n\nPhew, this seems tedious and imprecise. Happily, maths and R are to the rescue. Here is the line that minimizes those distances (all with the addition of one extra line of code).\n\nggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = F) + \n  theme_minimal()\n\n\n\n\n\n\nEstimating a linear model in R\nHow can we find this line? To answer this, we will first do some review.\nRemember the general equation for a line:\n\\[\ny = a + mx\n\\]\nRead this as: the value of \\(y\\) is the sum of some constant, \\(a\\), and some \\(x\\) variable that has been transformed by some slope value \\(m\\).\n\nRemember that the slope constant, \\(m\\), tells you how much \\(y\\) changes for every one unit increase in \\(x\\).\n\nSo, if:\n\\[\ny = 10 + 2x\n\\]\nThen, when \\(x = 20\\):\n\\[\ny = 10 + 2*20 = 50\n\\]\nFor many values of \\(x\\):\n\nggplot(tibble(x = 1:50, y = 10 + 2*x), aes(x = x, y = y)) + \n  geom_line(colour = \"lightgrey\", size = 3) + \n  geom_point() + \n  theme_minimal()\n\n\n\n\nWell, let’s substitute in our variables of interest. Our \\(y\\) variable is the life expectancy and our \\(x\\) variable is the logged GDP per capita.\n\\[\nlife Exp_x = \\beta_0 + \\beta_1 logGdpPerCap_x + \\epsilon\n\\]\nRead this as: the life expectancy of some country, \\(x\\), is a function of some constant (\\(\\beta_0\\)) and its logged GDP per capita transformed by some value \\(\\beta_1\\) with some random error (\\(\\epsilon\\)).\nLet’s imagine that this relationship is accurately described by the following formula:\n\\[\nlife Exp_x = 30 + 4 * logGdpPerCap_x\n\\]\n\nWe will get to that pesky error term in a bit.\n\nThen, our model would predict the following average life expectancy for countries with log GDPs per capita between 0 and 20:\n\nggplot(\n  tibble(log_gdp_per_cap = 1:20, life_exp = 30 + 4*log_gdp_per_cap), \n  aes(x = log_gdp_per_cap, y = life_exp)\n) + \n  geom_point() + \n  theme_minimal()\n\n\n\n\nA country with a logged GDP per capita of 5 (the equivalent of a GDP per capita of $148.41) has a predicted average life expectancy of 50 years, or \\(30 + 4*5\\).\nA country with a logged GDP per capita of 10 (the equivalent of a GDP per capita of $22,026.47) has a predicted average life expectancy of 70 years, or \\(30 + 4*10\\).\nDoes this accurately describe what we see in our data? What is the average life expectancy for countries with roughly $22,000 GDP per capita?\n\ncountries_10 <- gapminder_df |> \n  filter(gdp_per_cap > 21000 & gdp_per_cap < 23000)\n\ncountries_10\n\n# A tibble: 3 × 8\n  iso2c iso3c country          region  date gdp_per_cap life_exp log_gdp_per_cap\n  <chr> <chr> <chr>            <chr>  <dbl>       <dbl>    <dbl>           <dbl>\n1 BH    BHR   Bahrain          Middl…  2016      22867.     79.6           10.0 \n2 KN    KNA   St. Kitts and N… Latin…  2016      21095.     71.7            9.96\n3 SI    SVN   Slovenia         Europ…  2016      21678.     81.2            9.98\n\n\nWe predicted 70 years, but our data suggest that these countries have closer to an average of 77 years. Why do we have this difference?\nWell, we probably haven’t produced the best model we can. We just picked those numbers out of thin air. We will find the OLS regression model shortly. We might get closer to the observed value with that model. Let’s see.\n\nHow do we calculate the constant (\\(\\beta_0\\)) using OLS regression?\nRemember, OLS regression simply finds the line that minimizes the distance between itself and all the data points.\nIt turns out that the constant that minimizes this distance is the mean of \\(Y\\) minus \\(\\beta_1\\) times the mean of \\(X\\).\nSo, the constant that best predicts the life expectancy of a country based on its logged GDP per capita is equal to the average life expectancy across our sample minus the average logged GDP per capita transformed by \\(\\beta_1\\).\nSo…\n\n\nHow do we calculate the coefficient \\(\\beta_1\\)?\nThe regression slope is the correlation coefficient between \\(X\\) and \\(Y\\) multiplied by the standard deviation of \\(Y\\) divided by the standard deviation of \\(X\\).\nEw… Let’s step through that.\nRemember, the correlation coefficient simply measures how \\(X\\) and \\(Y\\) change together. Does \\(Y\\) increase when \\(X\\) increases? How strong is this relationship?\nThe standard deviations of \\(X\\) and \\(Y\\) just measure how spread out they are.\nBringing these together, we are interested in how much \\(X\\) and \\(Y\\) change together moderated by how much they change independently.\nFormally:\n\\[\n\\beta_1 = (\\frac{\\Sigma(\\frac{x_i - \\bar{x}}{s_X})(\\frac{y_i - \\bar{y}}{s_Y})}{n - 1})(\\frac{s_Y}{s_X}) = \\frac{\\Sigma(x_i - \\bar{x})(y_i - \\bar{y})}{\\Sigma(x_i - \\bar{x})^2}\n\\]\nHappily R does all of this for us:\n\nm <- lm(life_exp ~ log_gdp_per_cap, data = gapminder_df)\n\nm\n\n\nCall:\nlm(formula = life_exp ~ log_gdp_per_cap, data = gapminder_df)\n\nCoefficients:\n    (Intercept)  log_gdp_per_cap  \n         32.927            4.509  \n\n\nOkay, so the line of best fit describing the relationship between life expectancy and logged GDP per capita is:\n\\[\nlife Exp_x = 32.9 + 4.5 * logGdpPerCap_x + \\epsilon\n\\]\n\n\n\nPrediction and performance\nWe can use this model to predict a country’s life expectancy given its GDP per capita.\nbroom::tidy(m) makes this model object a lot easier to work with.\n\ntidy(m)\n\n# A tibble: 2 × 5\n  term            estimate std.error statistic  p.value\n  <chr>              <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)        32.9      1.77       18.6 1.24e-45\n2 log_gdp_per_cap     4.51     0.200      22.6 6.07e-57\n\n\nWhat is the life expectancy for a country with a GDP per capita of $10,000? First, let’s find the estimated constant (or intercept or \\(\\beta_0\\)).\n\nm_res <- tidy(m)\n\nbeta_0 <- m_res |> \n  filter(term == \"(Intercept)\") |> \n  pull(estimate)\n\nbeta_0\n\n[1] 32.92739\n\n\nThen we need to find the estimated coefficient for (logged) GDP per capita:\n\nbeta_1 <- m_res |> \n  filter(term == \"log_gdp_per_cap\") |> \n  pull(estimate)\n\nbeta_1\n\n[1] 4.508875\n\n\nFinally, we can plug this in to our model:\n\nlife_exp_10000 <- beta_0 + beta_1 * log(10000)\nlife_exp_10000\n\n[1] 74.45566\n\n\nA country with a GDP per capita of $10,000 is predicted to have an average life expectancy of 74 years. Does this make sense with our data?\n\nggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  geom_vline(xintercept = log(10000)) + \n  geom_hline(yintercept = life_exp_10000) + \n  geom_smooth(method = \"lm\", se = F) + \n  theme_minimal()\n\n\n\n\nWe can predict values from a model using broom::augment():\n\naugment(m, newdata = tibble(log_gdp_per_cap = log(10000)))\n\n# A tibble: 1 × 2\n  log_gdp_per_cap .fitted\n            <dbl>   <dbl>\n1            9.21    74.5\n\n\nWe can do this across a number of different values for GDP per capita:\n\nnew_data <- tibble(\n  gdp_per_cap = seq(from = 1000, to = 50000, by = 1000),\n  log_gdp_per_cap = log(gdp_per_cap)\n)\n\naugment(m, newdata = new_data)\n\n# A tibble: 50 × 3\n   gdp_per_cap log_gdp_per_cap .fitted\n         <dbl>           <dbl>   <dbl>\n 1        1000            6.91    64.1\n 2        2000            7.60    67.2\n 3        3000            8.01    69.0\n 4        4000            8.29    70.3\n 5        5000            8.52    71.3\n 6        6000            8.70    72.2\n 7        7000            8.85    72.8\n 8        8000            8.99    73.4\n 9        9000            9.10    74.0\n10       10000            9.21    74.5\n# ℹ 40 more rows\n\n\nLet’s look at that line:\n\nggplot(augment(m, newdata = new_data), aes(x = log_gdp_per_cap, y = .fitted)) + \n  geom_line(colour = \"lightgrey\", size = 2) + \n  geom_point() + \n  theme_minimal()\n\n\n\n\nBut how well does this model fit our observed data?\nLet’s fit it with our observed values of each country’s logged GDP per capita, instead of the ones we provided above.\n\naugment(m)\n\n# A tibble: 202 × 9\n   .rownames life_exp log_gdp_per_cap .fitted .resid    .hat .sigma   .cooksd\n   <chr>        <dbl>           <dbl>   <dbl>  <dbl>   <dbl>  <dbl>     <dbl>\n 1 1             75.6           10.3     79.2 -3.55  0.0104    4.13 0.00395  \n 2 2             63.1            6.25    61.1  2.01  0.0192    4.14 0.00237  \n 3 3             61.1            7.44    66.5 -5.40  0.00879   4.12 0.00766  \n 4 4             78.9            8.32    70.5  8.40  0.00533   4.09 0.0111   \n 5 6             79.3           10.6     80.8 -1.49  0.0134    4.14 0.000893 \n 6 7             76.3            9.46    75.6  0.743 0.00620   4.14 0.000102 \n 7 8             74.7            8.21    69.9  4.72  0.00557   4.12 0.00368  \n 8 10            78.2            9.67    76.5  1.62  0.00704   4.14 0.000548 \n 9 11            82.4           10.8     81.7  0.748 0.0152    4.14 0.000257 \n10 12            81.6           10.7     81.3  0.373 0.0143    4.14 0.0000601\n# ℹ 192 more rows\n# ℹ 1 more variable: .std.resid <dbl>\n\n\nHow did it do? What is the difference between what our model predicted and the country’s observed life expectancy?\nHere, we have the predicted values for life expectancy for all of our countries in our sample. Compare .fitted (the predicted life expectancy) to life_exp (the actual observed average life expectancy).\n\nm_eval <- augment(m) |> \n  transmute(\n    life_exp, \n    .fitted,\n    diff = life_exp - .fitted\n  )\n\nm_eval\n\n# A tibble: 202 × 3\n   life_exp .fitted   diff\n      <dbl>   <dbl>  <dbl>\n 1     75.6    79.2 -3.55 \n 2     63.1    61.1  2.01 \n 3     61.1    66.5 -5.40 \n 4     78.9    70.5  8.40 \n 5     79.3    80.8 -1.49 \n 6     76.3    75.6  0.743\n 7     74.7    69.9  4.72 \n 8     78.2    76.5  1.62 \n 9     82.4    81.7  0.748\n10     81.6    81.3  0.373\n# ℹ 192 more rows\n\n\nNote that broom::augment() already did this calculation and stored it in the .resid variable.\n\naugment(m) |> \n  transmute(\n    life_exp, \n    .fitted,\n    diff = life_exp - .fitted,\n    .resid\n  )\n\n# A tibble: 202 × 4\n   life_exp .fitted   diff .resid\n      <dbl>   <dbl>  <dbl>  <dbl>\n 1     75.6    79.2 -3.55  -3.55 \n 2     63.1    61.1  2.01   2.01 \n 3     61.1    66.5 -5.40  -5.40 \n 4     78.9    70.5  8.40   8.40 \n 5     79.3    80.8 -1.49  -1.49 \n 6     76.3    75.6  0.743  0.743\n 7     74.7    69.9  4.72   4.72 \n 8     78.2    76.5  1.62   1.62 \n 9     82.4    81.7  0.748  0.748\n10     81.6    81.3  0.373  0.373\n# ℹ 192 more rows\n\n\nOkay, so there are some differences. Let’s look at those difference a bit more closely:\n\nggplot(augment(m), aes(x = .resid)) + \n  geom_density() + \n  geom_vline(xintercept = 0) + \n  theme_minimal()\n\n\n\n\nIf our model perfectly predicted each country’s life expectancy, we would see no difference between the predicted and observed values. There would just be a very tall straight line at 0 on the graph above.\nOur model hasn’t predicted life expectancy perfectly. Whilst most predictions are within a couple of years of the country’s true life expectancy, there are some that are very different (up to 10 or 15 years!). Where the model has got it wrong, it has tended to overestimate life expectancy (note that the peak of the density curve sits above 0).\n\nCan you see for which points these large differences exist?\n\n\nggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = F) + \n  theme_minimal()\n\n\n\n\nWhat is causing these differences? A lot of your work as a political scientist is trying to answer this very question!\n\n\n(Random) error\nThe world is a messy and complicated place. Things often vary in completely random ways. That’s okay! It means that your observational data are going to move in funny and random ways that you cannot capture. That’s okay too! As long as your model includes all those systematic drivers of the thing you are interested in measuring (such as life expectancy), we can accept a bit of random error.\nIn fact, we have already accounted for this. Remember that error term?\n\\[\nlife Exp_x = \\beta_0 + \\beta_1 logGdpPerCap_x + \\epsilon\n\\]\nWe run into issues when this isn’t the case. We will discuss this more in later classes.\n\n\nModelling relationships among categorical variables\nSometimes we want to know whether our outcome of interest changes based where our observation sits within a categorical variable. For example, do levels of support for abortion access differ between Democrats, Republicans, and Independents? Do the number of women elected to parliament change based on whether or not the country has a formal quota?\nLet’s return to the American National Election Survey first explored last week. We will focus on that first question: do levels of support for abortion access differ between Democrats, Republicans, and Independents?\nWe can easily access the 2012 survey through R using the poliscidata package.\npoliscidata::nes\n\nCross tabs\nA simple cross tab can provide a nice summary of differences in your outcome of interest across your categories.\nFor example, let’s look at differences in the number of individuals who identified as Democrat, Republican, or Independent and whether they do not support access to abortions, support access with some conditions, with more conditions, or always.\n\ntabyl(nes, abort4, pid_3)\n\n     abort4  Dem Ind Rep NA_\n      Never  187 229 252   4\n Some conds  499 583 519   6\n More conds  332 337 227   2\n     Always 1325 964 381  10\n       <NA>   15  36   6   2\n\n\nWe can visualise this:\n\n\n\nAre these differences meaningful or significant? We will chat about that next week."
  },
  {
    "objectID": "content/03-content.html#visualizing-the-linear-relationship-between-two-continuous-variables",
    "href": "content/03-content.html#visualizing-the-linear-relationship-between-two-continuous-variables",
    "title": "Bivariate Relationships",
    "section": "Visualizing the linear relationship between two continuous variables",
    "text": "Visualizing the linear relationship between two continuous variables\n\nggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = F) + \n  theme_minimal()\n\n\n\n\nWe have started to unpack our estimated model for the linear relationship between a country’s (logged) GDP per capita and its average life expectancy.\n\\[\nlife Exp_x = \\beta_0 + \\beta_1 logGdpPerCap_x\n\\]\nRead this as: the life expectancy of some country, \\(x\\), is a function of some constant (\\(\\beta_0\\)) and the its logged GDP per capita transformed by some value \\(\\beta_1\\).\nHow do we calculate the constant (\\(\\beta_0\\)) and \\(\\beta_1\\)?\n\nEstimating a linear model in R\n\nm <- lm(life_exp ~ log_gdp_per_cap, data = gapminder_df)\n\ntidy(m)\n\n# A tibble: 2 × 5\n  term            estimate std.error statistic  p.value\n  <chr>              <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)        32.8      1.72       19.0 2.38e-46\n2 log_gdp_per_cap     4.54     0.195      23.3 2.36e-58"
  },
  {
    "objectID": "content/02-content.html#section",
    "href": "content/02-content.html#section",
    "title": "Descriptive Statistics",
    "section": "Section",
    "text": "Section\n\nPrerequisites\n\nlibrary(tidyverse)\nlibrary(poliscidata)\nlibrary(wbstats)\nlibrary(janitor)\nlibrary(skimr)\nlibrary(countrycode)\nlibrary(ggridges)\nlibrary(ggdist)\nlibrary(MetBrewer)\nlibrary(patchwork)\n\n\n\nDescribing categorical variables\nFor this section, we will be working with the American National Election Survey to explore how to produce useful descriptive statistics for categorical variables using R. The ANES polls annually individual Americans about their political beliefs and behavior.\nWe can access the 2012 survey using the poliscidata package:\npoliscidata::nes\n\nTake a look at the many different pieces of information collected via the survey at ?nes.\n\nWe are interested in understanding how many individuals of different ages took part in the survey. The survey records the age of each respondent within six different brackets in the dem_age6 variable.\n\nFrequency distribution\nWe can take advantage of janitor::tabyl() to quickly calculate the number and proportion of respondents in each age bracket.\n\ntabyl(nes, dem_age6)\n\n dem_age6    n    percent valid_percent\n    17-29  936 0.15821501     0.1598634\n    30-39  862 0.14570656     0.1472246\n    40-49  948 0.16024341     0.1619129\n    50-59 1312 0.22177147     0.2240820\n    60-69 1105 0.18678161     0.1887276\n 70-older  692 0.11697093     0.1181896\n     <NA>   61 0.01031102            NA\n\n\n\n\nVisualizing this frequency\nIt is a bit difficult to quickly determine relative counts. Which age bracket has the most respondents? Which has the least? Are these counts very different from each other.\nI highly recommend visualizing your data. You will get a much better sense of it. We can easily visualize this frequency table. I recommend using a bar chart to clearly show relative counts.\n\nnes |> \n  tabyl(dem_age6) |> \n  ggplot(aes(x = n, y = dem_age6)) + \n  geom_col() +\n  theme_minimal() + \n  labs(\n    x = \"Count of respondents\",\n    y = \"Age bracket\"\n  )\n\n\n\n\n\n\n\nDescribing continuous variables\nFor this section, we will be looking at each country’s expenditure on education, as a percent of their gross domestic product. We will use the wbstats::wb_data() function to collect these data.\n\nperc_edu <- wb_data(\n  \"SE.XPD.TOTL.GD.ZS\", start_date = 2020, end_date = 2020, return_wide = F\n) |> \n  mutate(\n    value = value / 100,\n    region = countrycode(country, \"country.name\", \"region\", \n                         custom_match = c(\"Turkiye\" = \"Europe & Central Asia\"))\n  )\n\nI have converted these percentages (0 - 100) to proportions (0 - 1) for ease of interpretation. I have also added each country’s region (using countrycode::countrycode()) so that we can explore regional trends in our data.\nWe can get a good sense of how expenditure varied by country looking at the center, spread, and shape of the distribution.\n\nFive number summary\nWe can use skimr::skim() to quickly get useful information on our continuous variable.\n\nskim(perc_edu$value)\n\n\nData summary\n\n\nName\nperc_edu$value\n\n\nNumber of rows\n217\n\n\nNumber of columns\n1\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ndata\n59\n0.73\n0.05\n0.02\n0\n0.03\n0.04\n0.06\n0.14\n▂▇▃▁▁\n\n\n\n\n\nThe country that spent the least on education as a percent of its GDP in 2020 was Cuba, which spent 0.000%. The country that spent the most in 2020 was the Marshall Islands, which spent 14%. The average percent of GDP spent on education in 2020 was 5%.\nThis description was a bit unwieldy. To get a better sense of our data, we can visualize it.\n\n\nHistogram\n\nggplot(perc_edu, aes(x = value)) + \n  geom_histogram() + \n  theme_minimal()\n\n\n\n\n\nTake a look at ?geom_histogram to find the arguments needed to change the bin width of your histograms.\n\n\n\nDensity curves\n\nggplot(perc_edu, aes(x = value)) + \n  geom_density() + \n  theme_minimal()\n\n\n\n\n\n\nBox and whisker plots\n\nggplot(perc_edu, aes(x = value)) + \n  geom_boxplot() + \n  theme_minimal()\n\n\n\n\n\n\nLooking for patterns in our groups\n\nggplot(perc_edu, aes(x = value, y = region)) + \n  geom_boxplot() + \n  theme_minimal()\n\n\n\n\nLet’s make a very cool grouped density plot using ggridges::geom_density_ridges().\n\nggplot(perc_edu, aes(x = value, y = region, fill = region)) + \n  geom_density_ridges(jittered_points = T, point_shape = \"|\", position = position_points_jitter(height = 0)) + \n  scale_fill_brewer(palette = 4) + \n  theme_minimal() + \n  theme(legend.position = \"none\")\n\n\n\n\n\n\nUnderstanding distributions\n\nNormal distribution\n\ntibble(z = rnorm(n = 1000)) |> \n  ggplot(aes(x = z)) + \n  geom_histogram() + \n  theme_minimal()\n\n\n\n\n\ntibble(z = rnorm(n = 1e6)) |> \n  ggplot(aes(x = z)) + \n  geom_histogram() + \n  theme_minimal()\n\n\n\n\n\n\nRight skewed distribution\n\ntibble(z = rbeta(10000, 2, 10)) |> \n  ggplot(aes(x = z)) + \n  geom_histogram() + \n  theme_minimal()\n\n\n\n\n\n\nLeft skewed distribution\n\ntibble(z = rbeta(10000, 10, 2)) |> \n  ggplot(aes(x = z)) + \n  geom_histogram() + \n  theme_minimal()\n\n\n\n\n\n\n\nMeasures of central tendency: mean, median, and mode\n\nMean\nThe mean is the average of all values.\n\n\nMedian\nThe median is the mid-point of all values.\n\n\nMode\nThe mode is the most frequent of all values.\n\n\nUsing central tendency to describe and understand distributions\nNormally distributed vectors share their mean and medians.\n\nnorm_dist <- tibble(z = rnorm(n = 1000))\n\nggplot(norm_dist, aes(x = z)) + \n  geom_histogram() + \n  geom_vline(xintercept = mean(norm_dist$z), colour = \"red\") + \n  geom_vline(xintercept = median(norm_dist$z), colour = \"blue\") + \n  theme_minimal()\n\n\n\n\nFor right skewed data, the mean is greater than the median.\n\nright_dist <- tibble(z = rbeta(10000, 2, 10))\n\nggplot(right_dist, aes(x = z)) + \n  geom_histogram() + \n  geom_vline(xintercept = mean(right_dist$z), colour = \"red\") + \n  geom_vline(xintercept = median(right_dist$z), colour = \"blue\") + \n  theme_minimal()\n\n\n\n\nFor left skewed data, the mean is smaller than the median.\n\nleft_dist <- tibble(z = rbeta(10000, 10, 2))\n\nggplot(left_dist, aes(x = z)) + \n  geom_histogram() + \n  geom_vline(xintercept = mean(left_dist$z), colour = \"red\") + \n  geom_vline(xintercept = median(left_dist$z), colour = \"blue\") + \n  theme_minimal()\n\n\n\n\n\n\n\nMeasures of spread: range, variance, and standard deviation\n\nRange\nThe range is the difference between the largest and smallest value.\n\nmax(perc_edu$value, na.rm = T) - min(perc_edu$value, na.rm = T)\n\n[1] 0.1362499\n\n\n\n\nVariance\nThe variance measures how spread out your values are. Take a look at these two plots. Both have the same center point (0) and number of observations (1,000,000). However, the data are much more spread out around that center point in the top graph.\n\nwide_dist <- tibble(z = rnorm(1e6, sd = 2))\n\np1 <- ggplot(wide_dist, aes(x = z)) + \n  geom_histogram() + \n  geom_vline(xintercept = 0) + \n  theme_minimal() + \n  scale_x_continuous(limits = c(-4, 4))\n\nnarrow_dist <- tibble(z = rnorm(1e6, sd = 1))\n\np2 <- ggplot(narrow_dist, aes(x = z)) + \n  geom_histogram() + \n  geom_vline(xintercept = 0) + \n  theme_minimal() + \n  scale_x_continuous(limits = c(-4, 4))\n\np1 / p2\n\n\n\n\nThe data in the top graph have far more variance than those in the bottom graph. We measure this by calculating the average of the squares of the deviations of the observations from their mean.\n\\[\ns^2 = \\frac{\\Sigma(x_i - \\bar{x})^2}{n - 1}\n\\]\nLet’s step through this. We will first calculate the variance for wide_dist, or the top graph.\n\nwide_var_calc <- wide_dist |> \n  mutate(\n    mean = mean(wide_dist$z),\n    diff = z - mean,\n    diff_2 = diff^2\n  )\n\nwide_var_calc\n\n# A tibble: 1,000,000 × 4\n         z    mean     diff     diff_2\n     <dbl>   <dbl>    <dbl>      <dbl>\n 1 -0.210  0.00138 -0.211    0.0447   \n 2 -1.68   0.00138 -1.69     2.84     \n 3  0.360  0.00138  0.359    0.129    \n 4  0.0106 0.00138  0.00920  0.0000846\n 5  2.26   0.00138  2.25     5.08     \n 6  1.75   0.00138  1.75     3.07     \n 7  0.312  0.00138  0.310    0.0963   \n 8  3.03   0.00138  3.03     9.20     \n 9 -1.05   0.00138 -1.05     1.10     \n10  4.53   0.00138  4.53    20.5      \n# ℹ 999,990 more rows\n\n\nWe take the sum of square of the difference between each observation and the mean of our whole sample. We then divide that by one less than our number of observations.\n\nwide_var <- sum(wide_var_calc$diff_2) / (nrow(wide_var_calc) - 1)\n\nwide_var\n\n[1] 4.009858\n\n\nWe can compare this to the variance for our narrower distribution.\n\nnarrow_var_calc <- narrow_dist |> \n  mutate(\n    mean = mean(narrow_dist$z),\n    diff = z - mean,\n    diff_2 = diff^2\n  )\n\nnarrow_var <- sum(narrow_var_calc$diff_2) / (nrow(narrow_var_calc) - 1)\n\nnarrow_var\n\n[1] 1.000617\n\n\nIt is, in fact, smaller!\nWe can use var() to do this in one step:\n\nvar(wide_dist)\n\n         z\nz 4.009858\n\n\n\nvar(narrow_dist)\n\n         z\nz 1.000617\n\n\n\n\nStandard deviation\nA simpler measure of spread is the standard deviation. It is simply the square root of the variance.\n\nsqrt(wide_var)\n\n[1] 2.002463\n\n\n\nsqrt(narrow_var)\n\n[1] 1.000308\n\n\nYou can get this directly using sd():\n\nsd(wide_dist$z)\n\n[1] 2.002463\n\n\n\nsd(narrow_dist$z)\n\n[1] 1.000308\n\n\nIf you look back to our graphs, you will see that I set the standard deviations explicitly when I generated the data. rnorm() takes an sd argument. This is great because we can confirm that the standard deviations for the wide and narrow distributions are 2 and 1 respectively (with a little bit of noise).\n\ntibble(\n  n = rnorm(1e6, sd = 1),\n  w = rnorm(1e6, sd = 2)\n) |> \n  ggplot() + \n  geom_density(aes(x = n), colour = \"green\") + \n  geom_density(aes(x = w), colour = \"lightblue\") + \n  theme_minimal()\n\n\n\n\n\n\n\nNormal distributions\nRemember that normal distributions share a mean and median. This has very cool and useful consequences.\n\nnorm_5_2 <- tibble(x = rnorm(n = 1e6, mean = 5, sd = 2))\n\nggplot(norm_5_2, aes(x = x)) + \n  stat_slab(\n    aes(fill_ramp = after_stat(cut_cdf_qi(cdf))), fill = met.brewer(\"Egypt\")[2]\n  ) + \n  scale_fill_ramp_discrete(range = c(1, 0.2), guide = \"none\") + \n  theme_minimal()\n\n\n\n\n\nApproximately 68% of the data fall within one standard deviation of the mean.\nApproximately 95% of the data fall within two standard deviations of the mean.\nApproximately 99.7% of the data fall within three standard deviations of the mean.\n\n\n\nStandardization\nNotice how our description of each variable depends on its units of measurement. What do we do if we want to compare across different measurements that have different units?\n\nZ scores\nFor normal distributions, we can use the z score. This gives us a standard way of understanding how many standard deviations from the mean of a normally distributed variable a value is.\n\\[\nz_i = \\frac{x_i - \\mu_x}{\\sigma_x}\n\\]\nWe are just transforming our data. We want to center it around 0 and reshape it so that roughly 68% of the data fall within one standard deviation of the mean, 95% of the data fall within two standard deviations of the mean, and 99.7% of the data fall within three standard deviations of the mean.\nLet’s standardize our data from above.\n\nstandard_5_2 <- norm_5_2 |> \n  mutate(mean = mean(x),\n         sd = sd(x),\n         z_score = (x - mean) / sd)\n\nhead(standard_5_2)\n\n# A tibble: 6 × 4\n       x  mean    sd z_score\n   <dbl> <dbl> <dbl>   <dbl>\n1  6.46   5.00  2.00   0.730\n2  5.93   5.00  2.00   0.466\n3  5.37   5.00  2.00   0.185\n4 10.4    5.00  2.00   2.71 \n5 -0.578  5.00  2.00  -2.79 \n6  3.30   5.00  2.00  -0.853\n\n\nWe can confirm this:\n\nggplot(standard_5_2, aes(x = z_score)) + \n  stat_slab(\n    aes(fill_ramp = after_stat(cut_cdf_qi(cdf))), fill = met.brewer(\"Egypt\")[2]\n  ) + \n  scale_fill_ramp_discrete(range = c(1, 0.2), guide = \"none\") + \n  theme_minimal()"
  }
]