[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\n            Quantitative Methods for Political Science\n        ",
    "section": "",
    "text": "Quantitative Methods for Political Science\n        \n        \n            An introduction to research methods and quantitative research in political science.\n        \n        \n            Fall 2023Department of Government and PoliticsUniversity of Maryland, College Park\n        \n    \n    \n      \n        \n        \n        \n      \n    \n\n\n\n\n\n\nProfessor\n\n   Dr David Cunningham\n   dacunnin@umd.edu\n\n\n\nTeaching Assistant\n\n   Harriet Goers\n   Chincoteague Building\n   hgoers@umd.edu\n   hgoers\n\n\n\n\n\nCourse details\n\n   August 26 - 11 December\n   Thursday, 2:00pm - 4:45pm\n   TYD1111\n\n\n\nLab details\n\n   Friday, 10:00am - 12:00pm\n   TYD1111\n\n\n\nOffice hours\n\n   Tuesday, 11:00am - 12:00pm\n   Zoom\n\n\n\n\nContacting me\nE-mail is the best ways to get in contact with me. I will try to respond to all course-related e-mails within 24 hours."
  },
  {
    "objectID": "content/03-bivariate_relationships.html",
    "href": "content/03-bivariate_relationships.html",
    "title": "Relationships Between Two Variables",
    "section": "",
    "text": "Installing packages\n\n\n\n\n\nIf you have not already done so, please install or update these packages by running the following in your console:\n\ninstall.packages(c(\"broom\", \"ggridges\", \"modelsummary\"))\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(wbstats)\nlibrary(poliscidata)\nlibrary(countrycode)\nlibrary(broom)\nlibrary(janitor)\nlibrary(ggridges)\nlibrary(modelsummary)\n\n\n\nAs usual, we start with an interesting question. You have some outcome of interest and you think that there is an important determinant of that outcome that no one has yet identified. Or perhaps the relationship between some heavily chewed over determinant and the outcome of interest is misunderstood. We are steadily building up your ability to determine empirically the relationship between that determinant and the outcome of interest.\nSimply put (and there really is no need to over-complicate this), we have two or more variables: an outcome of interest (the dependent variable) and a set of independent variables that we theorize are important determinants of that outcome. We can use empirical analysis to understand 1) how the dependent and independent variables change in relation to one another, and 2) whether this relationship is strong enough that we should declare (though 12,000-word journal articles or by shouting from rooftops) that whenever we want to change or understand that outcome, we must consider these important independent variables.\nPreviously, we discussed various tools that you can use to explore your data. This is very important for building your intuition and, by extension, your expertise in the question at hand. These tools also allow you to identify unusual data points (or outliers).\nThis week, we will make the next step. We will explore how two variables relate to each other. How do they move with each other: when one goes up, does the other go down, up, or not really move? How strong is this association?\nThis exploration is particularly important for you to do with regard to the independent variable(s) that are the focal point of your theory and, therefore, your contribution to our understanding of the messy spaghetti bowl of things that determine your outcome of interest. Just as it is important for you to spend some time understanding the shape of your variables (using the tools we discussed last week), you must also start to understand the shape of the relationship between the outcome you are trying to understand or predict and the factors you think are important determinants of that outcome.\nLet’s begin!\n\n\n\nHow do two variables move with one another? When when goes up, does the other go down, up, or not really move at all? How dramatic is this shift?\nThe type of variables we have determines how we can answer this question. To begin, we will explore the relationship between two continuous variables. Later in the class, we will look at how to explore the relationship between a continuous and categorical variable.\nTo start, we will explore the relationship between wealth and health. This question was made popular by Hans Rosling’s Gapminder project.\n\n\n\n\nFirst, we need to collect our data. Following Rosling, we will use each country’s average life expectancy to measure its health and the country’s GDP per capita to measure its wealth. We will use wbstats::wb_data() to pull these data directly from the World Bank.\n\ngapminder_df &lt;- wb_data(\n  indicator = c(\"SP.DYN.LE00.IN\", \"NY.GDP.PCAP.CD\"),\n  start_date = 2016,\n  end_date = 2016\n) |&gt; \n  rename(\n    life_exp = SP.DYN.LE00.IN,\n    gdp_per_cap = NY.GDP.PCAP.CD\n  ) |&gt; \n  mutate(\n    log_gdp_per_cap = log(gdp_per_cap),\n    region = countrycode(country, \"country.name\", \"region\", custom_match = c(\"Turkiye\" = \"Europe & Central Asia\"))\n  ) |&gt; \n  relocate(region, .after = country)\n\ngapminder_df\n\n# A tibble: 217 × 8\n   iso2c iso3c country         region  date gdp_per_cap life_exp log_gdp_per_cap\n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;           &lt;chr&gt;  &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;           &lt;dbl&gt;\n 1 AW    ABW   Aruba           Latin…  2016      28450.     75.6           10.3 \n 2 AF    AFG   Afghanistan     South…  2016        523.     63.1            6.26\n 3 AO    AGO   Angola          Sub-S…  2016       1810.     61.1            7.50\n 4 AL    ALB   Albania         Europ…  2016       4124.     78.9            8.32\n 5 AD    AND   Andorra         Europ…  2016      39931.     NA             10.6 \n 6 AE    ARE   United Arab Em… Middl…  2016      41055.     79.3           10.6 \n 7 AR    ARG   Argentina       Latin…  2016      12790.     76.3            9.46\n 8 AM    ARM   Armenia         Europ…  2016       3680.     74.7            8.21\n 9 AS    ASM   American Samoa  East …  2016      13301.     NA              9.50\n10 AG    ATG   Antigua and Ba… Latin…  2016      16449.     78.2            9.71\n# ℹ 207 more rows\n\n\n\n\n\nWhat is the relationship between a country’s average life expectancy and its GDP per capita? The easiest way to determine this is to visualize these two variables.\n\nggplot(gapminder_df, aes(x = gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  theme_minimal() + \n  labs(x = \"GDP per capita (USD current)\",\n       y = \"Average life expectancy (years)\") + \n  scale_x_continuous(labels = label_dollar())\n\n\n\n\nThere seems to be a good case that there is a strong relationship between a country’s GDP per capita (wealth) and its average life expectancy (health). It appears that we expect citizens that live in countries that have larger GPD per capita to live longer, on average. But this relationship is not linear (a straight line drawn through them will not summarise this very well).\nBecause we want to explore linear relationships at this stage of the course, we will look at the logged GDP per capita variable:\n\nggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  theme_minimal() + \n  labs(x = \"Logged GDP per capita\",\n       y = \"Average life expectancy (years)\") + \n  scale_x_continuous(labels = label_dollar())\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can transform your data to make it easier to work with. Just remember that you now need to talk in terms of logged GDP per capita instead of GDP per capita.\n\n\nI can imagine drawing a straight line among these points that summarises how they vary with each other. It appears that as a country’s logged GDP per capita increases, so too does the average life expectancy of its population. As wealth increases, so too does health.\nWell, that was easy! What is the relationship between health and wealth? They increase with each other.\n\n\n\nNow we need some way of measuring the strength of the relationship. In other words, what amount of the variation in countries’ average life expectancy is associated with variation in their GDP per capita? We can measure the strength of this association using correlations. The correlation coefficient tells us how closely variables relate to one another. It tells us both the strength and direction of the association.\n\nStrength: how closely are these values tied to one another. Measured from 0 to |1|, with values closer to 0 indicating a very weak relationship and values closer to |1| indicating a very strong relationship.\n\n\n\n\n\n\n\nTip\n\n\n\nWhat are those funny looking |s? They represent the absolute value, which is shorthand for the number regardless of its sign. To demonstrate, |1| is the absolute value of 1 and -1.\n\n\n\nDirection: do both \\(X\\) and \\(Y\\) change in the same direction? Positive correlations show that when \\(X\\) increases (decreases), so does \\(Y\\). Negative correlations show that when \\(X\\) increases (decreases), \\(Y\\) decreases (increases). In other words, they move in different directions.\n\nWhat is the correlation between logged GDP per capita and life expectancy?\n\ncor(gapminder_df$log_gdp_per_cap, gapminder_df$life_exp, use = \"complete.obs\")\n\n[1] 0.8492004\n\n\nAs expected, the relationship is positive and strong.\n\n\n\nWe have very quickly gained the skills to determine whether the relationship between two variables is positive, negative, or non-existent. We have also learnt how to describe the strength of that relationship. To that end, we are now able to describe the bivariate relationship between health and wealth as a positive and strong one.\nThis is useful, but we tend to need a more concrete way of describing the relationship between two variables. For example, what if a policy-maker comes up to you and asks what you think the effect of a $1,000 increase in a country’s GDP per capita will do to its average life expectancy? We can build simple models of this relationship to provide that policy-maker with a prediction of what we might expect to happen on average. Further, we can use the model to describe the relationship between these two variables in a generalized way. If a new country were to spring into existence, we can use our knowledge of its GDP per capita to determine how long we might expect its citizens to live.\n\n\n\nLooking back at our data, we can image a straight line running between each country’s plotted average life expectancy and GDP per capita. Let’s draw that line.\n\nggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  theme_minimal() + \n  labs(x = \"Logged GDP per capita\",\n       y = \"Average life expectancy (years)\") + \n  scale_x_continuous(labels = label_dollar())\n\n\n\n\nWe can, of course, draw many different lines through these points. Each of us has probably drawn a slightly different line in our heads. Which is the best line? Ordinary least squares (OLS) regression provides an answer. Simply put, OLS regression draws the line that minimizes the distance between itself and all of the data points. That line can take many shapes, including a straight line, an S, a frowney face, and smiley face, etc.\nLooking at our data above, it appears that a straight line is the best line to draw.\n\n\n\n\n\n\nNote\n\n\n\nOverfitting involves fitting a model (or drawing a line through our data) that misses the forest for the trees. You can draw all kinds of shapes through those data that perhaps result in a smaller distance between itself and each dot. In fact, if you draw a line that connects all of those dots there will be no difference between your line and the data points. However, this model will be too focused on the data we have at hand. Our model will have no idea what to do with any new data points we introduce. This is bad! Your aim here is to produce a generalizable model of the relationship between these two variables, not to draw a line that connects this particular constellation of dots.\n\n\nOkay, so a straight line is the best type of line to draw. But there are still many, many different straight lines that we can draw. Which straight line is best? Remember, OLS regression finds the line that minimizes the distance between itself and all of the data points. Let’s step through this. Look at the graph above.\n\nDraw a line through those dots. Pick a line, any line!\nCalculate the distance between each dot and the line.\nSum up the absolute values of those distances. Remember, we just care about the distance, so we don’t need to worry about whether or not the dots are above or below the line.\nRepeat steps 1 - 3 many, many, many times.\nPick the line with the smallest sum of distances (or results from step 3).\n\nPhew, this seems tedious. Happily, maths and R are to the rescue. Here is the line that minimizes those distances (all with the addition of one extra line of code).\n\nggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = F) + \n  theme_minimal() + \n  labs(x = \"Logged GDP per capita\",\n       y = \"Average life expectancy (years)\") + \n  scale_x_continuous(labels = label_dollar())\n\n\n\n\n\n\n\nHow did R do this? To answer this, we will first do some review.\nRemember the general equation for a line:\n\\[\ny = a + mx\n\\]\nRead this as: the value of \\(y\\) is the sum of some constant, \\(a\\), and some \\(x\\) variable that has been transformed by some slope value \\(m\\).\n\n\n\n\n\n\nNote\n\n\n\nRemember that the slope constant, \\(m\\), tells you how much \\(y\\) changes for every one unit increase in \\(x\\).\n\n\nSo, if:\n\\[\ny = 10 + 2x\n\\]\nThen, when \\(x = 20\\):\n\\[\ny = 10 + 2*20 = 50\n\\]\nFor many values of \\(x\\):\n\nggplot(tibble(x = 0:50, y = 10 + 2*x), aes(x = x, y = y)) + \n  geom_line(colour = \"lightgrey\", linewidth = 3) + \n  geom_point() + \n  theme_minimal()\n\n\n\n\nWell, let’s substitute in our variables of interest. Our \\(y\\) variable is a country’s average life expectancy and our \\(x\\) variable is that country’s logged GDP per capita.\n\\[\nlife Exp_x = \\beta_0 + \\beta_1 logGdpPerCap_x + \\epsilon\n\\]\nRead this as: a country’s average life expectancy is a function of some constant (\\(\\beta_0\\)) and its logged GDP per capita transformed by some value \\(\\beta_1\\) with some random error (\\(\\epsilon\\)), on average.\nLet’s imagine that this relationship is accurately described by the following formula:\n\\[\nlife Exp_x = 30 + 4 * logGdpPerCap_x\n\\]\n\n\n\n\n\n\nNote\n\n\n\nWe will get to that pesky error term in just a minute.\n\n\nThen, our model would predict the following average life expectancy for countries with log GDPs per capita between 0 and 20:\n\nggplot(\n  tibble(log_gdp_per_cap = 0:20, life_exp = 30 + 4*log_gdp_per_cap), \n  aes(x = log_gdp_per_cap, y = life_exp)\n) + \n  geom_line(colour = \"lightgrey\", linewidth = 3) + \n  geom_point() + \n  theme_minimal() + \n  labs(x = \"Logged GDP per capita\",\n       y = \"Average life expectancy (years)\") + \n  scale_x_continuous(labels = label_dollar())\n\n\n\n\nA country with a logged GDP per capita of 5 (the equivalent of a GDP per capita of $148.41) has a predicted average life expectancy of 50 years, or \\(30 + 4*5\\).\nA country with a logged GDP per capita of 10 (the equivalent of a GDP per capita of $22,026.47) has a predicted average life expectancy of 70 years, or \\(30 + 4*10\\).\nDoes this accurately describe what we see in our data? What is the average life expectancy for countries with roughly $22,000 GDP per capita?\n\ncountries_10 &lt;- filter(gapminder_df, gdp_per_cap &gt; 21000 & gdp_per_cap &lt; 23000)\n\ncountries_10\n\n# A tibble: 3 × 8\n  iso2c iso3c country          region  date gdp_per_cap life_exp log_gdp_per_cap\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;            &lt;chr&gt;  &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;           &lt;dbl&gt;\n1 BH    BHR   Bahrain          Middl…  2016      22878.     79.6           10.0 \n2 KN    KNA   St. Kitts and N… Latin…  2016      21068.     71.7            9.96\n3 SI    SVN   Slovenia         Europ…  2016      21678.     81.2            9.98\n\n\nWe predicted 70 years, but our data suggest that these countries have closer to an average of 77 years. Why do we have this difference?\nWell, we probably haven’t produced the best model we can (this isn’t the best line!). We just picked those numbers out of thin air. Let’s fit a linear OLS regression and see if we improve our ability to predict what we have seen in the wild.\n\n\nRemember, OLS regression simply finds the line that minimizes the distance between itself and all the data points. The constant that minimizes this distance is the mean of \\(Y\\) minus \\(\\beta_1\\) times the mean of \\(X\\).\nSo, the constant that best predicts a country’s average life expectancy based on its logged GDP per capita is equal to the average life expectancy across our sample (72.3 years) minus the average logged GDP per capita ($8.80, or $6,639.48 GDP per capita) transformed by \\(\\beta_1\\).\nSo…\n\n\n\nThe regression slope is the correlation coefficient between \\(X\\) and \\(Y\\) multiplied by the standard deviation of \\(Y\\) divided by the standard deviation of \\(X\\).\nEw… Let’s step through that.\nRemember, the correlation coefficient simply measures how \\(X\\) and \\(Y\\) change together. Does \\(Y\\) increase when \\(X\\) increases? How strong is this relationship?\nThe standard deviations of \\(X\\) and \\(Y\\) just measure how spread out they are.\nBringing these together, we are interested in how much \\(X\\) and \\(Y\\) change together moderated by how much they change independently of each other.\nFormally:\n\\[\n\\beta_1 = (\\frac{\\Sigma(\\frac{x_i - \\bar{x}}{s_X})(\\frac{y_i - \\bar{y}}{s_Y})}{n - 1})(\\frac{s_Y}{s_X}) = \\frac{\\Sigma(x_i - \\bar{x})(y_i - \\bar{y})}{\\Sigma(x_i - \\bar{x})^2}\n\\]\nHappily R does all of this for us.\n\n\n\n\nm &lt;- lm(life_exp ~ log_gdp_per_cap, data = gapminder_df)\n\nm\n\n\nCall:\nlm(formula = life_exp ~ log_gdp_per_cap, data = gapminder_df)\n\nCoefficients:\n    (Intercept)  log_gdp_per_cap  \n         32.833            4.517  \n\n\nOkay, so the line of best fit describing the relationship between life expectancy and logged GDP per capita is:\n\\[\nlife Exp_x = 32.9 + 4.5 * logGdpPerCap_x + \\epsilon\n\\]\nThat’s it! We now have a generalized model of the relationship between a country’s average life expectancy and its logged GDP per capita. This model is informed by what we actually observed in the world. It carefully balances our need to accurately describe what we have observed and to develop something that is generalizable.\nThe above model output is difficult to read. It will not be accepted by any journal or professor. Luckily, we can use modelsummary::modelsummary() to easily generate a professionally formatted table.\n\nmodelsummary(\n  m, \n  statistic = NULL,\n  coef_rename = c(\"log_gdp_per_cap\" = \"GDP per capita (logged)\"),\n  gof_map = \"nobs\"\n)\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n32.833\n\n\nGDP per capita (logged)\n4.517\n\n\nNum.Obs.\n203\n\n\n\n\n\n\n\nNote that OLS regression, particularly linear regression, requires that you make a lot of important assumptions about the relationship between your two variables. These were discussed in detail in the lecture. For example, we assume that the best line to fit is straight. We also assume that the best way to generate and describe the relationship across all observations is to fit the line that minimizes the distance between itself and the observed values or dots.\n\n\n\n\n\n\nNote\n\n\n\n\n\nThere are other approaches to determining the “best” line. These include maximum likelihood estimation (discussed in detail in GVPT729) and Bayesian statistics. We won’t discuss these approaches in this class or in GVPT722. It’s worth noting here; however, that OLS regression requires a whole bunch of assumptions that may or may not be appropriate to your research question or theory. This class prepares you to grapple with those questions and appropriately use these tools in your own research.\n\n\n\n\n\n\n\nOkay, so we now have a model that describes the relationship between our outcome of interest (health) and our independent variable of interest (wealth). We can use this to predict our outcome of interest for different values of our independent variable. For example, what do we predict to be the average life expectancy of a country with a GDP per capita of $20,000?\nbroom::tidy(m) makes this model object a lot easier (tidier) to work with.\n\ntidy(m)\n\n# A tibble: 2 × 5\n  term            estimate std.error statistic  p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)        32.8      1.76       18.7 7.08e-46\n2 log_gdp_per_cap     4.52     0.198      22.8 1.21e-57\n\n\nFirst, let’s pull out the estimated constant (or intercept or \\(\\beta_0\\)) for our calculations.\n\nm_res &lt;- tidy(m)\n\nbeta_0 &lt;- m_res |&gt; \n  filter(term == \"(Intercept)\") |&gt; \n  pull(estimate)\n\nbeta_0\n\n[1] 32.83331\n\n\nNext, let’s pull out the estimated coefficient for (logged) GDP per capita:\n\nbeta_1 &lt;- m_res |&gt; \n  filter(term == \"log_gdp_per_cap\") |&gt; \n  pull(estimate)\n\nbeta_1\n\n[1] 4.517274\n\n\nFinally, we can plug this in to our model:\n\\[\nlife Exp_x = \\beta_0 + \\beta_1 logGdpPerCap_x\n\\]\n\nlife_exp_20000 &lt;- beta_0 + beta_1 * log(20000)\nlife_exp_20000\n\n[1] 77.57007\n\n\nA country with a GDP per capita of $20,000 is predicted to have an average life expectancy of 78 years. Let’s take a look back at our data. Remember, these data describe what the World Bank actually observed for each country in 2016. How close is our predicted value to our observed values?\n\ncountries_10\n\n# A tibble: 3 × 8\n  iso2c iso3c country          region  date gdp_per_cap life_exp log_gdp_per_cap\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;            &lt;chr&gt;  &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;           &lt;dbl&gt;\n1 BH    BHR   Bahrain          Middl…  2016      22878.     79.6           10.0 \n2 KN    KNA   St. Kitts and N… Latin…  2016      21068.     71.7            9.96\n3 SI    SVN   Slovenia         Europ…  2016      21678.     81.2            9.98\n\n\nAs above, our data suggest that these countries have closer to an average of 77 years. Although our model predicted an average life expectancy closer to this than our guess above (which predicted 70 years), we still have a gap. Why?\nOur model is an attempt to formalize our understanding of the general relationship between a country’s wealth and health. Mapping our model against the observed values we used to generate it illustrates this point well.\n\nggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  geom_vline(xintercept = log(20000)) + \n  geom_hline(yintercept = life_exp_20000) + \n  geom_smooth(method = \"lm\", se = F) + \n  theme_minimal() + \n  labs(x = \"Logged GDP per capita\",\n       y = \"Average life expectancy (years)\") + \n  scale_x_continuous(labels = label_dollar())\n\n\n\n\nThe world is a complicated and messy place. There are many countries that have a GDP per capita of around $20,000 (those dots sitting around the vertical black line). They have a wide range of average life expectancy: look at their various placement along that vertical line. Some are higher than others.\nAlso, there are several countries with a wide range of logged GDP per capita that have an average life expectancy of 78 years (those sitting at or around the horizontal black line). These have a wide range of logged GDP per capita: some are further to the left than others.\nOur model is our best attempt at accounting for that diversity whilst still producing a useful summary of the relationship between health and wealth for those countries and all other countries with all observed values of GDP per capita.\nA bit of noise (error) is expected. How much error is okay? This is a complicated question that has contested answers. Let’s start with actually measuring that error. Then we can chat about whether or not it’s small enough to allow us to be confident in our model.\n\n\n\nReturning to our question above, how close are our predicted values to our observed values? For example, how far from the observed average life expectancy of countries with a GDP per capita of or close to $20,000 is 78 years?\nStart by working out the average life expectancy predicted by our model for the logged GDP per capita of all of our countries. We can then compare this to the average life expectancy actually observed in all these countries. We can predict values from a model using broom::augment():\n\naugment(m)\n\n# A tibble: 203 × 9\n   .rownames life_exp log_gdp_per_cap .fitted .resid    .hat .sigma   .cooksd\n   &lt;chr&gt;        &lt;dbl&gt;           &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n 1 1             75.6           10.3     79.2 -3.54  0.0103    4.11 0.00390  \n 2 2             63.1            6.26    61.1  2.03  0.0192    4.12 0.00243  \n 3 3             61.1            7.50    66.7 -5.63  0.00849   4.10 0.00809  \n 4 4             78.9            8.32    70.4  8.42  0.00533   4.08 0.0113   \n 5 6             79.3           10.6     80.8 -1.48  0.0132    4.12 0.000880 \n 6 7             76.3            9.46    75.6  0.757 0.00612   4.12 0.000105 \n 7 8             74.7            8.21    69.9  4.74  0.00558   4.11 0.00375  \n 8 10            78.2            9.71    76.7  1.46  0.00710   4.12 0.000457 \n 9 11            82.4           10.8     81.7  0.747 0.0150    4.12 0.000254 \n10 12            81.6           10.7     81.3  0.377 0.0140    4.12 0.0000609\n# ℹ 193 more rows\n# ℹ 1 more variable: .std.resid &lt;dbl&gt;\n\n\nThis function is simply fitting our model (\\(life Exp_x = 32.9 + 4.5 * logGdpPerCap_x\\)) to each country’s logged GDP per capita. You can confirm this by running the model yourself:\n\ngapminder_df |&gt; \n  transmute(\n    country,\n    log_gdp_per_cap,\n    .fitted = beta_0 + beta_1*log_gdp_per_cap\n  )\n\n# A tibble: 217 × 3\n   country              log_gdp_per_cap .fitted\n   &lt;chr&gt;                          &lt;dbl&gt;   &lt;dbl&gt;\n 1 Aruba                          10.3     79.2\n 2 Afghanistan                     6.26    61.1\n 3 Angola                          7.50    66.7\n 4 Albania                         8.32    70.4\n 5 Andorra                        10.6     80.7\n 6 United Arab Emirates           10.6     80.8\n 7 Argentina                       9.46    75.6\n 8 Armenia                         8.21    69.9\n 9 American Samoa                  9.50    75.7\n10 Antigua and Barbuda             9.71    76.7\n# ℹ 207 more rows\n\n\nHow did the model do? What is the difference between what it predicted and the country’s observed average life expectancy? Compare .fitted (the predicted average life expectancy) to life_exp (the actual observed average life expectancy).\n\nm_eval &lt;- augment(m) |&gt; \n  transmute(\n    life_exp, \n    .fitted,\n    diff = life_exp - .fitted\n  )\n\nm_eval\n\n# A tibble: 203 × 3\n   life_exp .fitted   diff\n      &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1     75.6    79.2 -3.54 \n 2     63.1    61.1  2.03 \n 3     61.1    66.7 -5.63 \n 4     78.9    70.4  8.42 \n 5     79.3    80.8 -1.48 \n 6     76.3    75.6  0.757\n 7     74.7    69.9  4.74 \n 8     78.2    76.7  1.46 \n 9     82.4    81.7  0.747\n10     81.6    81.3  0.377\n# ℹ 193 more rows\n\n\nNote that broom::augment() already did this calculation and stored it in the .resid variable. The formal term for the difference between the predicted and observed values is the residual.\n\naugment(m) |&gt; \n  select(life_exp, .fitted, .resid)\n\n# A tibble: 203 × 3\n   life_exp .fitted .resid\n      &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1     75.6    79.2 -3.54 \n 2     63.1    61.1  2.03 \n 3     61.1    66.7 -5.63 \n 4     78.9    70.4  8.42 \n 5     79.3    80.8 -1.48 \n 6     76.3    75.6  0.757\n 7     74.7    69.9  4.74 \n 8     78.2    76.7  1.46 \n 9     82.4    81.7  0.747\n10     81.6    81.3  0.377\n# ℹ 193 more rows\n\n\nOkay, so there are some differences. Let’s look at those differences a bit more closely:\n\nggplot(augment(m), aes(x = .resid)) + \n  geom_density() + \n  geom_vline(xintercept = 0) + \n  theme_minimal()\n\n\n\n\nIf our model perfectly predicted each country’s life expectancy, we would see no difference between the predicted and observed values. There would just be a very tall straight line at 0 on the graph above.\nOur model hasn’t predicted life expectancy perfectly. Whilst most predictions are within a couple of years of the country’s true life expectancy, there are some that are very different (up to 10 or 15 years!). Where the model has got it wrong, it has tended to overestimate life expectancy (note that the peak of the density curve sits above 0).\nCan you see for which points these large differences exist?\n\nggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = F) + \n  theme_minimal() + \n  labs(x = \"Logged GDP per capita\",\n       y = \"Average life expectancy (years)\") + \n  scale_x_continuous(labels = label_dollar())\n\n\n\n\nWhat is causing these differences? A lot of your work as a political scientist is trying to answer this very question!\n\n\nThe world is a messy and complicated place. Things often vary in random ways. That’s okay! It means that your observational data are going to move in funny and random ways. That’s okay too! As long as your model includes all of the systematic drivers of the thing you are interested in measuring (such as average life expectancy), we can accept a bit of random error.\nIn fact, we have already accounted for this. Remember that error term:\n\\[\nlife Exp_x = \\beta_0 + \\beta_1 logGdpPerCap_x + \\epsilon\n\\]\nWe run into issues when there are non-random things bundled up into the difference between what our model predicts and what we actually observe. We will discuss this more in later classes.\n\n\n\n\nWe often want to understand how the model has performed as a whole, rather than how well it predicts each individual observed data point. There are many different ways we can do this.\n\n\nThe sum of squared residuals measures the total error in our model. Formally:\n\\[\n\\Sigma(y_i - \\hat{y_i})^2\n\\]\nWhere \\(y_i\\) is each observed value (the country’s actual average life expectancy) and \\(\\hat{y_i}\\) is each predicted value (the model’s estimate of country’s average life expectancy).\nWe just add those all up to get a single measure of the model’s overall performance.\n\n\n\n\n\n\nNote\n\n\n\nRemember that we tend to square things when we don’t care about the direction. We don’t care that the predicted value is less or more than the observed value, just about how far they are from each other.\n\n\nWe can do this ourselves:\n\naugment(m) |&gt; \n  summarise(sum(.resid^2))\n\n# A tibble: 1 × 1\n  `sum(.resid^2)`\n            &lt;dbl&gt;\n1           3397.\n\n\nOr we can use broom::glance():\n\nglance(m)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.721         0.720  4.11      520. 1.21e-57     1  -574. 1154. 1164.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\nglance(m) |&gt; \n  select(deviance)\n\n# A tibble: 1 × 1\n  deviance\n     &lt;dbl&gt;\n1    3397.\n\n\n\n\n\n\n\n\nTip\n\n\n\nWhere broom::tidy() gives us information about the coefficients of our model, broom::glance() gives us information on the overall model performance.\n\n\nThis is useful, but it is influenced by the units by which we measure our variables. If one model includes something like GDP which is measured in terms of billions of dollars, we will get a very large sum of squared residuals. If another model includes something like percentage of as state’s citizens who will vote for Donald Trump, we will get a relatively small sum of squared residuals. What if we want to compare model performance in a meaningful way?\n\n\n\nThe \\(R^2\\) value measures the amount of variation in the dependent variable that is explained by the independent variable. In our example, it measures how much the changes in countries’ average life expectancy is explained by the changes in their (logged) GDP per capita.\n\\[\nR^2 = 1 - \\frac{Unexplained\\ variation}{Total\\ variation}\n\\]\nThe \\(R^2\\) value is useful because it does not reflect the units of measurement used in our variables. Therefore, we can compare how well different models perform.\nThe \\(R^2\\) value has three component parts.\n\n\nTSS measures the squared sum of the differences between all predicted values of the dependent variable and the mean of the dependent variable.\n\n\n\nESS measures the sum of the squares of the deviations of the predicted values from the mean value of the dependent variable.\n\n\n\nRSS measures the difference between the TSS and ESS. In other words, the error not explained by the model.\nFormally, the \\(R^2\\) value is:\n\\[\nR^2 = 1 - \\frac{RSS}{TSS} = 1 - \\frac{\\Sigma(y_i - \\hat{y_i})^2}{\\Sigma(y_i - \\hat{y})^2}\n\\]\nOr:\n\\[\nR^2 = \\frac{ESS}{TSS} = \\frac{\\Sigma(\\hat{y}_i - \\bar{y})^2}{\\Sigma(y_i - \\bar{y})^2}\n\\]\nOur model’s \\(R^2\\) can be accessed using broom::glance():\n\nglance(m) |&gt; \n  select(r.squared)\n\n# A tibble: 1 × 1\n  r.squared\n      &lt;dbl&gt;\n1     0.721\n\n\nAn \\(R^2\\) of 1 means that all of the change in the dependent variable are completely explained by changes in the independent variable. Here, it would mean that all changes to a country’s average life expectancy are explained through changes to the country’s logged GDP per capita.\nAccording to our model, 72.1% of changes in a country’s average life expectancy are explained through changes to the country’s logged GDP per capita.\n\n\n\n\n\nSometimes we want to know whether our outcome of interest changes based on the category in which it sits. For example, do levels of support for abortion access differ between Democrats, Republicans, and Independents? Do the number of women elected to parliament change based on whether or not the country has a formal quota? Do the number of civilians targeted in war change based on whether the war is intra- or inter-state?\nLet’s return to the American National Election Survey we first explored last week. We will focus on that first question: do levels of support for abortion access differ between Democrats, Republicans, and Independents?\nWe can access the 2012 survey through R using the poliscidata package:\npoliscidata::nes\n\n\nA simple cross tab can provide a nice summary of differences in your outcome of interest across your categories.\nFor example, let’s look at differences in the number of individuals who identified as Democrat, Republican, or Independent who do not support access to abortions, support access with some conditions, with more conditions, or always.\nWe can use modelsummary::datasummary_crosstab() to produce a nicely formatted cross tab of our variables:\n\ndatasummary_crosstab(abort4 ~ pid_3, data = nes)\n\n\n\n\n abort4\n\nDem\n Ind\nRep\nAll\n\n\n\n\nNever\nN\n187\n229\n252\n672\n\n\n\n% row\n27.8\n34.1\n37.5\n100.0\n\n\nSome conds\nN\n499\n583\n519\n1607\n\n\n\n% row\n31.1\n36.3\n32.3\n100.0\n\n\nMore conds\nN\n332\n337\n227\n898\n\n\n\n% row\n37.0\n37.5\n25.3\n100.0\n\n\nAlways\nN\n1325\n964\n381\n2680\n\n\n\n% row\n49.4\n36.0\n14.2\n100.0\n\n\nAll\nN\n2358\n2149\n1385\n5916\n\n\n\n% row\n39.9\n36.3\n23.4\n100.0\n\n\n\n\n\n\n\nWe can also visualise this:\n\nnes |&gt; \n  count(pid_3, abort4) |&gt; \n  drop_na(pid_3) |&gt; \n  ggplot(aes(x = n, y = pid_3, fill = abort4)) + \n  geom_bar(position = \"dodge\", stat = \"identity\") + \n  theme_minimal() + \n  labs(x = \"N\",\n       y = NULL,\n       fill = \"Level of support\") + \n  scale_fill_manual(values = c(\"#EDE5CF\",\"#E0C2A2\",\"#D39C83\",\"#C1766F\"))\n\n\n\n\nOr this:\n\nnes |&gt; \n  count(pid_3, abort4) |&gt; \n  drop_na(pid_3) |&gt; \n  ggplot(aes(x = n, y = abort4, fill = pid_3)) + \n  geom_bar(position = \"dodge\", stat = \"identity\") + \n  theme_minimal() + \n  labs(x = \"N\",\n       y = NULL,\n       fill = \"Party\") + \n  scale_fill_manual(values = c(\"#1375B7\",\"lightgrey\",\"#C93135\"))\n\n\n\n\n\n\n\nWe can use mean comparison tables to, well, compare means (in a table). Let’s compare the average response to the feeling thermometer (scale from 0 to 100) for the Republican party across parties:\n\nnes |&gt;\n  group_by(pid_3) |&gt; \n  summarise(\n    mean = mean(ft_rep, na.rm = T),\n    sd = sd(ft_rep, na.rm = T),\n    freq = n()\n  )\n\n# A tibble: 4 × 4\n  pid_3  mean    sd  freq\n  &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 Dem    24.5  21.8  2358\n2 Ind    43.0  23.1  2149\n3 Rep    70.3  18.3  1385\n4 &lt;NA&gt;   50    14.7    24\n\n\nAre these counts or averages meaningfully different from one another? We need some additional tools to answer that question. We will discuss those in the coming weeks.\n\n\n\nWe can also visualize the whole distribution of a continuous variable of interest within our categories.\n\nnes |&gt;\n  drop_na(pid_3) |&gt; \n  ggplot(aes(x = ft_rep, fill = pid_3)) + \n  geom_density(alpha = 0.5) + \n  theme_minimal() + \n  scale_fill_manual(values = c(\"#1375B7\",\"lightgrey\",\"#C93135\")) + \n  labs(x = \"Feeling thermometer\",\n       y = \"Density\", \n       fill = \"Party\")\n\n\n\n\nAs expected, Democrats appear to respond least favourably to the Republican Party, followed by Independents, and Republicans. This is demonstrated by both the mean comparison table and density plots."
  },
  {
    "objectID": "content/03-bivariate_relationships.html#set-up",
    "href": "content/03-bivariate_relationships.html#set-up",
    "title": "Relationships Between Two Variables",
    "section": "",
    "text": "Installing packages\n\n\n\n\n\nIf you have not already done so, please install or update these packages by running the following in your console:\n\ninstall.packages(c(\"broom\", \"ggridges\", \"modelsummary\"))\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(wbstats)\nlibrary(poliscidata)\nlibrary(countrycode)\nlibrary(broom)\nlibrary(janitor)\nlibrary(ggridges)\nlibrary(modelsummary)\n\n\n\nAs usual, we start with an interesting question. You have some outcome of interest and you think that there is an important determinant of that outcome that no one has yet identified. Or perhaps the relationship between some heavily chewed over determinant and the outcome of interest is misunderstood. We are steadily building up your ability to determine empirically the relationship between that determinant and the outcome of interest.\nSimply put (and there really is no need to over-complicate this), we have two or more variables: an outcome of interest (the dependent variable) and a set of independent variables that we theorize are important determinants of that outcome. We can use empirical analysis to understand 1) how the dependent and independent variables change in relation to one another, and 2) whether this relationship is strong enough that we should declare (though 12,000-word journal articles or by shouting from rooftops) that whenever we want to change or understand that outcome, we must consider these important independent variables.\nPreviously, we discussed various tools that you can use to explore your data. This is very important for building your intuition and, by extension, your expertise in the question at hand. These tools also allow you to identify unusual data points (or outliers).\nThis week, we will make the next step. We will explore how two variables relate to each other. How do they move with each other: when one goes up, does the other go down, up, or not really move? How strong is this association?\nThis exploration is particularly important for you to do with regard to the independent variable(s) that are the focal point of your theory and, therefore, your contribution to our understanding of the messy spaghetti bowl of things that determine your outcome of interest. Just as it is important for you to spend some time understanding the shape of your variables (using the tools we discussed last week), you must also start to understand the shape of the relationship between the outcome you are trying to understand or predict and the factors you think are important determinants of that outcome.\nLet’s begin!\n\n\n\nHow do two variables move with one another? When when goes up, does the other go down, up, or not really move at all? How dramatic is this shift?\nThe type of variables we have determines how we can answer this question. To begin, we will explore the relationship between two continuous variables. Later in the class, we will look at how to explore the relationship between a continuous and categorical variable.\nTo start, we will explore the relationship between wealth and health. This question was made popular by Hans Rosling’s Gapminder project.\n\n\n\n\nFirst, we need to collect our data. Following Rosling, we will use each country’s average life expectancy to measure its health and the country’s GDP per capita to measure its wealth. We will use wbstats::wb_data() to pull these data directly from the World Bank.\n\ngapminder_df &lt;- wb_data(\n  indicator = c(\"SP.DYN.LE00.IN\", \"NY.GDP.PCAP.CD\"),\n  start_date = 2016,\n  end_date = 2016\n) |&gt; \n  rename(\n    life_exp = SP.DYN.LE00.IN,\n    gdp_per_cap = NY.GDP.PCAP.CD\n  ) |&gt; \n  mutate(\n    log_gdp_per_cap = log(gdp_per_cap),\n    region = countrycode(country, \"country.name\", \"region\", custom_match = c(\"Turkiye\" = \"Europe & Central Asia\"))\n  ) |&gt; \n  relocate(region, .after = country)\n\ngapminder_df\n\n# A tibble: 217 × 8\n   iso2c iso3c country         region  date gdp_per_cap life_exp log_gdp_per_cap\n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;           &lt;chr&gt;  &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;           &lt;dbl&gt;\n 1 AW    ABW   Aruba           Latin…  2016      28450.     75.6           10.3 \n 2 AF    AFG   Afghanistan     South…  2016        523.     63.1            6.26\n 3 AO    AGO   Angola          Sub-S…  2016       1810.     61.1            7.50\n 4 AL    ALB   Albania         Europ…  2016       4124.     78.9            8.32\n 5 AD    AND   Andorra         Europ…  2016      39931.     NA             10.6 \n 6 AE    ARE   United Arab Em… Middl…  2016      41055.     79.3           10.6 \n 7 AR    ARG   Argentina       Latin…  2016      12790.     76.3            9.46\n 8 AM    ARM   Armenia         Europ…  2016       3680.     74.7            8.21\n 9 AS    ASM   American Samoa  East …  2016      13301.     NA              9.50\n10 AG    ATG   Antigua and Ba… Latin…  2016      16449.     78.2            9.71\n# ℹ 207 more rows\n\n\n\n\n\nWhat is the relationship between a country’s average life expectancy and its GDP per capita? The easiest way to determine this is to visualize these two variables.\n\nggplot(gapminder_df, aes(x = gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  theme_minimal() + \n  labs(x = \"GDP per capita (USD current)\",\n       y = \"Average life expectancy (years)\") + \n  scale_x_continuous(labels = label_dollar())\n\n\n\n\nThere seems to be a good case that there is a strong relationship between a country’s GDP per capita (wealth) and its average life expectancy (health). It appears that we expect citizens that live in countries that have larger GPD per capita to live longer, on average. But this relationship is not linear (a straight line drawn through them will not summarise this very well).\nBecause we want to explore linear relationships at this stage of the course, we will look at the logged GDP per capita variable:\n\nggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  theme_minimal() + \n  labs(x = \"Logged GDP per capita\",\n       y = \"Average life expectancy (years)\") + \n  scale_x_continuous(labels = label_dollar())\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can transform your data to make it easier to work with. Just remember that you now need to talk in terms of logged GDP per capita instead of GDP per capita.\n\n\nI can imagine drawing a straight line among these points that summarises how they vary with each other. It appears that as a country’s logged GDP per capita increases, so too does the average life expectancy of its population. As wealth increases, so too does health.\nWell, that was easy! What is the relationship between health and wealth? They increase with each other.\n\n\n\nNow we need some way of measuring the strength of the relationship. In other words, what amount of the variation in countries’ average life expectancy is associated with variation in their GDP per capita? We can measure the strength of this association using correlations. The correlation coefficient tells us how closely variables relate to one another. It tells us both the strength and direction of the association.\n\nStrength: how closely are these values tied to one another. Measured from 0 to |1|, with values closer to 0 indicating a very weak relationship and values closer to |1| indicating a very strong relationship.\n\n\n\n\n\n\n\nTip\n\n\n\nWhat are those funny looking |s? They represent the absolute value, which is shorthand for the number regardless of its sign. To demonstrate, |1| is the absolute value of 1 and -1.\n\n\n\nDirection: do both \\(X\\) and \\(Y\\) change in the same direction? Positive correlations show that when \\(X\\) increases (decreases), so does \\(Y\\). Negative correlations show that when \\(X\\) increases (decreases), \\(Y\\) decreases (increases). In other words, they move in different directions.\n\nWhat is the correlation between logged GDP per capita and life expectancy?\n\ncor(gapminder_df$log_gdp_per_cap, gapminder_df$life_exp, use = \"complete.obs\")\n\n[1] 0.8492004\n\n\nAs expected, the relationship is positive and strong.\n\n\n\nWe have very quickly gained the skills to determine whether the relationship between two variables is positive, negative, or non-existent. We have also learnt how to describe the strength of that relationship. To that end, we are now able to describe the bivariate relationship between health and wealth as a positive and strong one.\nThis is useful, but we tend to need a more concrete way of describing the relationship between two variables. For example, what if a policy-maker comes up to you and asks what you think the effect of a $1,000 increase in a country’s GDP per capita will do to its average life expectancy? We can build simple models of this relationship to provide that policy-maker with a prediction of what we might expect to happen on average. Further, we can use the model to describe the relationship between these two variables in a generalized way. If a new country were to spring into existence, we can use our knowledge of its GDP per capita to determine how long we might expect its citizens to live.\n\n\n\nLooking back at our data, we can image a straight line running between each country’s plotted average life expectancy and GDP per capita. Let’s draw that line.\n\nggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  theme_minimal() + \n  labs(x = \"Logged GDP per capita\",\n       y = \"Average life expectancy (years)\") + \n  scale_x_continuous(labels = label_dollar())\n\n\n\n\nWe can, of course, draw many different lines through these points. Each of us has probably drawn a slightly different line in our heads. Which is the best line? Ordinary least squares (OLS) regression provides an answer. Simply put, OLS regression draws the line that minimizes the distance between itself and all of the data points. That line can take many shapes, including a straight line, an S, a frowney face, and smiley face, etc.\nLooking at our data above, it appears that a straight line is the best line to draw.\n\n\n\n\n\n\nNote\n\n\n\nOverfitting involves fitting a model (or drawing a line through our data) that misses the forest for the trees. You can draw all kinds of shapes through those data that perhaps result in a smaller distance between itself and each dot. In fact, if you draw a line that connects all of those dots there will be no difference between your line and the data points. However, this model will be too focused on the data we have at hand. Our model will have no idea what to do with any new data points we introduce. This is bad! Your aim here is to produce a generalizable model of the relationship between these two variables, not to draw a line that connects this particular constellation of dots.\n\n\nOkay, so a straight line is the best type of line to draw. But there are still many, many different straight lines that we can draw. Which straight line is best? Remember, OLS regression finds the line that minimizes the distance between itself and all of the data points. Let’s step through this. Look at the graph above.\n\nDraw a line through those dots. Pick a line, any line!\nCalculate the distance between each dot and the line.\nSum up the absolute values of those distances. Remember, we just care about the distance, so we don’t need to worry about whether or not the dots are above or below the line.\nRepeat steps 1 - 3 many, many, many times.\nPick the line with the smallest sum of distances (or results from step 3).\n\nPhew, this seems tedious. Happily, maths and R are to the rescue. Here is the line that minimizes those distances (all with the addition of one extra line of code).\n\nggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = F) + \n  theme_minimal() + \n  labs(x = \"Logged GDP per capita\",\n       y = \"Average life expectancy (years)\") + \n  scale_x_continuous(labels = label_dollar())\n\n\n\n\n\n\n\nHow did R do this? To answer this, we will first do some review.\nRemember the general equation for a line:\n\\[\ny = a + mx\n\\]\nRead this as: the value of \\(y\\) is the sum of some constant, \\(a\\), and some \\(x\\) variable that has been transformed by some slope value \\(m\\).\n\n\n\n\n\n\nNote\n\n\n\nRemember that the slope constant, \\(m\\), tells you how much \\(y\\) changes for every one unit increase in \\(x\\).\n\n\nSo, if:\n\\[\ny = 10 + 2x\n\\]\nThen, when \\(x = 20\\):\n\\[\ny = 10 + 2*20 = 50\n\\]\nFor many values of \\(x\\):\n\nggplot(tibble(x = 0:50, y = 10 + 2*x), aes(x = x, y = y)) + \n  geom_line(colour = \"lightgrey\", linewidth = 3) + \n  geom_point() + \n  theme_minimal()\n\n\n\n\nWell, let’s substitute in our variables of interest. Our \\(y\\) variable is a country’s average life expectancy and our \\(x\\) variable is that country’s logged GDP per capita.\n\\[\nlife Exp_x = \\beta_0 + \\beta_1 logGdpPerCap_x + \\epsilon\n\\]\nRead this as: a country’s average life expectancy is a function of some constant (\\(\\beta_0\\)) and its logged GDP per capita transformed by some value \\(\\beta_1\\) with some random error (\\(\\epsilon\\)), on average.\nLet’s imagine that this relationship is accurately described by the following formula:\n\\[\nlife Exp_x = 30 + 4 * logGdpPerCap_x\n\\]\n\n\n\n\n\n\nNote\n\n\n\nWe will get to that pesky error term in just a minute.\n\n\nThen, our model would predict the following average life expectancy for countries with log GDPs per capita between 0 and 20:\n\nggplot(\n  tibble(log_gdp_per_cap = 0:20, life_exp = 30 + 4*log_gdp_per_cap), \n  aes(x = log_gdp_per_cap, y = life_exp)\n) + \n  geom_line(colour = \"lightgrey\", linewidth = 3) + \n  geom_point() + \n  theme_minimal() + \n  labs(x = \"Logged GDP per capita\",\n       y = \"Average life expectancy (years)\") + \n  scale_x_continuous(labels = label_dollar())\n\n\n\n\nA country with a logged GDP per capita of 5 (the equivalent of a GDP per capita of $148.41) has a predicted average life expectancy of 50 years, or \\(30 + 4*5\\).\nA country with a logged GDP per capita of 10 (the equivalent of a GDP per capita of $22,026.47) has a predicted average life expectancy of 70 years, or \\(30 + 4*10\\).\nDoes this accurately describe what we see in our data? What is the average life expectancy for countries with roughly $22,000 GDP per capita?\n\ncountries_10 &lt;- filter(gapminder_df, gdp_per_cap &gt; 21000 & gdp_per_cap &lt; 23000)\n\ncountries_10\n\n# A tibble: 3 × 8\n  iso2c iso3c country          region  date gdp_per_cap life_exp log_gdp_per_cap\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;            &lt;chr&gt;  &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;           &lt;dbl&gt;\n1 BH    BHR   Bahrain          Middl…  2016      22878.     79.6           10.0 \n2 KN    KNA   St. Kitts and N… Latin…  2016      21068.     71.7            9.96\n3 SI    SVN   Slovenia         Europ…  2016      21678.     81.2            9.98\n\n\nWe predicted 70 years, but our data suggest that these countries have closer to an average of 77 years. Why do we have this difference?\nWell, we probably haven’t produced the best model we can (this isn’t the best line!). We just picked those numbers out of thin air. Let’s fit a linear OLS regression and see if we improve our ability to predict what we have seen in the wild.\n\n\nRemember, OLS regression simply finds the line that minimizes the distance between itself and all the data points. The constant that minimizes this distance is the mean of \\(Y\\) minus \\(\\beta_1\\) times the mean of \\(X\\).\nSo, the constant that best predicts a country’s average life expectancy based on its logged GDP per capita is equal to the average life expectancy across our sample (72.3 years) minus the average logged GDP per capita ($8.80, or $6,639.48 GDP per capita) transformed by \\(\\beta_1\\).\nSo…\n\n\n\nThe regression slope is the correlation coefficient between \\(X\\) and \\(Y\\) multiplied by the standard deviation of \\(Y\\) divided by the standard deviation of \\(X\\).\nEw… Let’s step through that.\nRemember, the correlation coefficient simply measures how \\(X\\) and \\(Y\\) change together. Does \\(Y\\) increase when \\(X\\) increases? How strong is this relationship?\nThe standard deviations of \\(X\\) and \\(Y\\) just measure how spread out they are.\nBringing these together, we are interested in how much \\(X\\) and \\(Y\\) change together moderated by how much they change independently of each other.\nFormally:\n\\[\n\\beta_1 = (\\frac{\\Sigma(\\frac{x_i - \\bar{x}}{s_X})(\\frac{y_i - \\bar{y}}{s_Y})}{n - 1})(\\frac{s_Y}{s_X}) = \\frac{\\Sigma(x_i - \\bar{x})(y_i - \\bar{y})}{\\Sigma(x_i - \\bar{x})^2}\n\\]\nHappily R does all of this for us.\n\n\n\n\nm &lt;- lm(life_exp ~ log_gdp_per_cap, data = gapminder_df)\n\nm\n\n\nCall:\nlm(formula = life_exp ~ log_gdp_per_cap, data = gapminder_df)\n\nCoefficients:\n    (Intercept)  log_gdp_per_cap  \n         32.833            4.517  \n\n\nOkay, so the line of best fit describing the relationship between life expectancy and logged GDP per capita is:\n\\[\nlife Exp_x = 32.9 + 4.5 * logGdpPerCap_x + \\epsilon\n\\]\nThat’s it! We now have a generalized model of the relationship between a country’s average life expectancy and its logged GDP per capita. This model is informed by what we actually observed in the world. It carefully balances our need to accurately describe what we have observed and to develop something that is generalizable.\nThe above model output is difficult to read. It will not be accepted by any journal or professor. Luckily, we can use modelsummary::modelsummary() to easily generate a professionally formatted table.\n\nmodelsummary(\n  m, \n  statistic = NULL,\n  coef_rename = c(\"log_gdp_per_cap\" = \"GDP per capita (logged)\"),\n  gof_map = \"nobs\"\n)\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n32.833\n\n\nGDP per capita (logged)\n4.517\n\n\nNum.Obs.\n203\n\n\n\n\n\n\n\nNote that OLS regression, particularly linear regression, requires that you make a lot of important assumptions about the relationship between your two variables. These were discussed in detail in the lecture. For example, we assume that the best line to fit is straight. We also assume that the best way to generate and describe the relationship across all observations is to fit the line that minimizes the distance between itself and the observed values or dots.\n\n\n\n\n\n\nNote\n\n\n\n\n\nThere are other approaches to determining the “best” line. These include maximum likelihood estimation (discussed in detail in GVPT729) and Bayesian statistics. We won’t discuss these approaches in this class or in GVPT722. It’s worth noting here; however, that OLS regression requires a whole bunch of assumptions that may or may not be appropriate to your research question or theory. This class prepares you to grapple with those questions and appropriately use these tools in your own research.\n\n\n\n\n\n\n\nOkay, so we now have a model that describes the relationship between our outcome of interest (health) and our independent variable of interest (wealth). We can use this to predict our outcome of interest for different values of our independent variable. For example, what do we predict to be the average life expectancy of a country with a GDP per capita of $20,000?\nbroom::tidy(m) makes this model object a lot easier (tidier) to work with.\n\ntidy(m)\n\n# A tibble: 2 × 5\n  term            estimate std.error statistic  p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)        32.8      1.76       18.7 7.08e-46\n2 log_gdp_per_cap     4.52     0.198      22.8 1.21e-57\n\n\nFirst, let’s pull out the estimated constant (or intercept or \\(\\beta_0\\)) for our calculations.\n\nm_res &lt;- tidy(m)\n\nbeta_0 &lt;- m_res |&gt; \n  filter(term == \"(Intercept)\") |&gt; \n  pull(estimate)\n\nbeta_0\n\n[1] 32.83331\n\n\nNext, let’s pull out the estimated coefficient for (logged) GDP per capita:\n\nbeta_1 &lt;- m_res |&gt; \n  filter(term == \"log_gdp_per_cap\") |&gt; \n  pull(estimate)\n\nbeta_1\n\n[1] 4.517274\n\n\nFinally, we can plug this in to our model:\n\\[\nlife Exp_x = \\beta_0 + \\beta_1 logGdpPerCap_x\n\\]\n\nlife_exp_20000 &lt;- beta_0 + beta_1 * log(20000)\nlife_exp_20000\n\n[1] 77.57007\n\n\nA country with a GDP per capita of $20,000 is predicted to have an average life expectancy of 78 years. Let’s take a look back at our data. Remember, these data describe what the World Bank actually observed for each country in 2016. How close is our predicted value to our observed values?\n\ncountries_10\n\n# A tibble: 3 × 8\n  iso2c iso3c country          region  date gdp_per_cap life_exp log_gdp_per_cap\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;            &lt;chr&gt;  &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;           &lt;dbl&gt;\n1 BH    BHR   Bahrain          Middl…  2016      22878.     79.6           10.0 \n2 KN    KNA   St. Kitts and N… Latin…  2016      21068.     71.7            9.96\n3 SI    SVN   Slovenia         Europ…  2016      21678.     81.2            9.98\n\n\nAs above, our data suggest that these countries have closer to an average of 77 years. Although our model predicted an average life expectancy closer to this than our guess above (which predicted 70 years), we still have a gap. Why?\nOur model is an attempt to formalize our understanding of the general relationship between a country’s wealth and health. Mapping our model against the observed values we used to generate it illustrates this point well.\n\nggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  geom_vline(xintercept = log(20000)) + \n  geom_hline(yintercept = life_exp_20000) + \n  geom_smooth(method = \"lm\", se = F) + \n  theme_minimal() + \n  labs(x = \"Logged GDP per capita\",\n       y = \"Average life expectancy (years)\") + \n  scale_x_continuous(labels = label_dollar())\n\n\n\n\nThe world is a complicated and messy place. There are many countries that have a GDP per capita of around $20,000 (those dots sitting around the vertical black line). They have a wide range of average life expectancy: look at their various placement along that vertical line. Some are higher than others.\nAlso, there are several countries with a wide range of logged GDP per capita that have an average life expectancy of 78 years (those sitting at or around the horizontal black line). These have a wide range of logged GDP per capita: some are further to the left than others.\nOur model is our best attempt at accounting for that diversity whilst still producing a useful summary of the relationship between health and wealth for those countries and all other countries with all observed values of GDP per capita.\nA bit of noise (error) is expected. How much error is okay? This is a complicated question that has contested answers. Let’s start with actually measuring that error. Then we can chat about whether or not it’s small enough to allow us to be confident in our model.\n\n\n\nReturning to our question above, how close are our predicted values to our observed values? For example, how far from the observed average life expectancy of countries with a GDP per capita of or close to $20,000 is 78 years?\nStart by working out the average life expectancy predicted by our model for the logged GDP per capita of all of our countries. We can then compare this to the average life expectancy actually observed in all these countries. We can predict values from a model using broom::augment():\n\naugment(m)\n\n# A tibble: 203 × 9\n   .rownames life_exp log_gdp_per_cap .fitted .resid    .hat .sigma   .cooksd\n   &lt;chr&gt;        &lt;dbl&gt;           &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n 1 1             75.6           10.3     79.2 -3.54  0.0103    4.11 0.00390  \n 2 2             63.1            6.26    61.1  2.03  0.0192    4.12 0.00243  \n 3 3             61.1            7.50    66.7 -5.63  0.00849   4.10 0.00809  \n 4 4             78.9            8.32    70.4  8.42  0.00533   4.08 0.0113   \n 5 6             79.3           10.6     80.8 -1.48  0.0132    4.12 0.000880 \n 6 7             76.3            9.46    75.6  0.757 0.00612   4.12 0.000105 \n 7 8             74.7            8.21    69.9  4.74  0.00558   4.11 0.00375  \n 8 10            78.2            9.71    76.7  1.46  0.00710   4.12 0.000457 \n 9 11            82.4           10.8     81.7  0.747 0.0150    4.12 0.000254 \n10 12            81.6           10.7     81.3  0.377 0.0140    4.12 0.0000609\n# ℹ 193 more rows\n# ℹ 1 more variable: .std.resid &lt;dbl&gt;\n\n\nThis function is simply fitting our model (\\(life Exp_x = 32.9 + 4.5 * logGdpPerCap_x\\)) to each country’s logged GDP per capita. You can confirm this by running the model yourself:\n\ngapminder_df |&gt; \n  transmute(\n    country,\n    log_gdp_per_cap,\n    .fitted = beta_0 + beta_1*log_gdp_per_cap\n  )\n\n# A tibble: 217 × 3\n   country              log_gdp_per_cap .fitted\n   &lt;chr&gt;                          &lt;dbl&gt;   &lt;dbl&gt;\n 1 Aruba                          10.3     79.2\n 2 Afghanistan                     6.26    61.1\n 3 Angola                          7.50    66.7\n 4 Albania                         8.32    70.4\n 5 Andorra                        10.6     80.7\n 6 United Arab Emirates           10.6     80.8\n 7 Argentina                       9.46    75.6\n 8 Armenia                         8.21    69.9\n 9 American Samoa                  9.50    75.7\n10 Antigua and Barbuda             9.71    76.7\n# ℹ 207 more rows\n\n\nHow did the model do? What is the difference between what it predicted and the country’s observed average life expectancy? Compare .fitted (the predicted average life expectancy) to life_exp (the actual observed average life expectancy).\n\nm_eval &lt;- augment(m) |&gt; \n  transmute(\n    life_exp, \n    .fitted,\n    diff = life_exp - .fitted\n  )\n\nm_eval\n\n# A tibble: 203 × 3\n   life_exp .fitted   diff\n      &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1     75.6    79.2 -3.54 \n 2     63.1    61.1  2.03 \n 3     61.1    66.7 -5.63 \n 4     78.9    70.4  8.42 \n 5     79.3    80.8 -1.48 \n 6     76.3    75.6  0.757\n 7     74.7    69.9  4.74 \n 8     78.2    76.7  1.46 \n 9     82.4    81.7  0.747\n10     81.6    81.3  0.377\n# ℹ 193 more rows\n\n\nNote that broom::augment() already did this calculation and stored it in the .resid variable. The formal term for the difference between the predicted and observed values is the residual.\n\naugment(m) |&gt; \n  select(life_exp, .fitted, .resid)\n\n# A tibble: 203 × 3\n   life_exp .fitted .resid\n      &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1     75.6    79.2 -3.54 \n 2     63.1    61.1  2.03 \n 3     61.1    66.7 -5.63 \n 4     78.9    70.4  8.42 \n 5     79.3    80.8 -1.48 \n 6     76.3    75.6  0.757\n 7     74.7    69.9  4.74 \n 8     78.2    76.7  1.46 \n 9     82.4    81.7  0.747\n10     81.6    81.3  0.377\n# ℹ 193 more rows\n\n\nOkay, so there are some differences. Let’s look at those differences a bit more closely:\n\nggplot(augment(m), aes(x = .resid)) + \n  geom_density() + \n  geom_vline(xintercept = 0) + \n  theme_minimal()\n\n\n\n\nIf our model perfectly predicted each country’s life expectancy, we would see no difference between the predicted and observed values. There would just be a very tall straight line at 0 on the graph above.\nOur model hasn’t predicted life expectancy perfectly. Whilst most predictions are within a couple of years of the country’s true life expectancy, there are some that are very different (up to 10 or 15 years!). Where the model has got it wrong, it has tended to overestimate life expectancy (note that the peak of the density curve sits above 0).\nCan you see for which points these large differences exist?\n\nggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = F) + \n  theme_minimal() + \n  labs(x = \"Logged GDP per capita\",\n       y = \"Average life expectancy (years)\") + \n  scale_x_continuous(labels = label_dollar())\n\n\n\n\nWhat is causing these differences? A lot of your work as a political scientist is trying to answer this very question!\n\n\nThe world is a messy and complicated place. Things often vary in random ways. That’s okay! It means that your observational data are going to move in funny and random ways. That’s okay too! As long as your model includes all of the systematic drivers of the thing you are interested in measuring (such as average life expectancy), we can accept a bit of random error.\nIn fact, we have already accounted for this. Remember that error term:\n\\[\nlife Exp_x = \\beta_0 + \\beta_1 logGdpPerCap_x + \\epsilon\n\\]\nWe run into issues when there are non-random things bundled up into the difference between what our model predicts and what we actually observe. We will discuss this more in later classes.\n\n\n\n\nWe often want to understand how the model has performed as a whole, rather than how well it predicts each individual observed data point. There are many different ways we can do this.\n\n\nThe sum of squared residuals measures the total error in our model. Formally:\n\\[\n\\Sigma(y_i - \\hat{y_i})^2\n\\]\nWhere \\(y_i\\) is each observed value (the country’s actual average life expectancy) and \\(\\hat{y_i}\\) is each predicted value (the model’s estimate of country’s average life expectancy).\nWe just add those all up to get a single measure of the model’s overall performance.\n\n\n\n\n\n\nNote\n\n\n\nRemember that we tend to square things when we don’t care about the direction. We don’t care that the predicted value is less or more than the observed value, just about how far they are from each other.\n\n\nWe can do this ourselves:\n\naugment(m) |&gt; \n  summarise(sum(.resid^2))\n\n# A tibble: 1 × 1\n  `sum(.resid^2)`\n            &lt;dbl&gt;\n1           3397.\n\n\nOr we can use broom::glance():\n\nglance(m)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.721         0.720  4.11      520. 1.21e-57     1  -574. 1154. 1164.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\nglance(m) |&gt; \n  select(deviance)\n\n# A tibble: 1 × 1\n  deviance\n     &lt;dbl&gt;\n1    3397.\n\n\n\n\n\n\n\n\nTip\n\n\n\nWhere broom::tidy() gives us information about the coefficients of our model, broom::glance() gives us information on the overall model performance.\n\n\nThis is useful, but it is influenced by the units by which we measure our variables. If one model includes something like GDP which is measured in terms of billions of dollars, we will get a very large sum of squared residuals. If another model includes something like percentage of as state’s citizens who will vote for Donald Trump, we will get a relatively small sum of squared residuals. What if we want to compare model performance in a meaningful way?\n\n\n\nThe \\(R^2\\) value measures the amount of variation in the dependent variable that is explained by the independent variable. In our example, it measures how much the changes in countries’ average life expectancy is explained by the changes in their (logged) GDP per capita.\n\\[\nR^2 = 1 - \\frac{Unexplained\\ variation}{Total\\ variation}\n\\]\nThe \\(R^2\\) value is useful because it does not reflect the units of measurement used in our variables. Therefore, we can compare how well different models perform.\nThe \\(R^2\\) value has three component parts.\n\n\nTSS measures the squared sum of the differences between all predicted values of the dependent variable and the mean of the dependent variable.\n\n\n\nESS measures the sum of the squares of the deviations of the predicted values from the mean value of the dependent variable.\n\n\n\nRSS measures the difference between the TSS and ESS. In other words, the error not explained by the model.\nFormally, the \\(R^2\\) value is:\n\\[\nR^2 = 1 - \\frac{RSS}{TSS} = 1 - \\frac{\\Sigma(y_i - \\hat{y_i})^2}{\\Sigma(y_i - \\hat{y})^2}\n\\]\nOr:\n\\[\nR^2 = \\frac{ESS}{TSS} = \\frac{\\Sigma(\\hat{y}_i - \\bar{y})^2}{\\Sigma(y_i - \\bar{y})^2}\n\\]\nOur model’s \\(R^2\\) can be accessed using broom::glance():\n\nglance(m) |&gt; \n  select(r.squared)\n\n# A tibble: 1 × 1\n  r.squared\n      &lt;dbl&gt;\n1     0.721\n\n\nAn \\(R^2\\) of 1 means that all of the change in the dependent variable are completely explained by changes in the independent variable. Here, it would mean that all changes to a country’s average life expectancy are explained through changes to the country’s logged GDP per capita.\nAccording to our model, 72.1% of changes in a country’s average life expectancy are explained through changes to the country’s logged GDP per capita.\n\n\n\n\n\nSometimes we want to know whether our outcome of interest changes based on the category in which it sits. For example, do levels of support for abortion access differ between Democrats, Republicans, and Independents? Do the number of women elected to parliament change based on whether or not the country has a formal quota? Do the number of civilians targeted in war change based on whether the war is intra- or inter-state?\nLet’s return to the American National Election Survey we first explored last week. We will focus on that first question: do levels of support for abortion access differ between Democrats, Republicans, and Independents?\nWe can access the 2012 survey through R using the poliscidata package:\npoliscidata::nes\n\n\nA simple cross tab can provide a nice summary of differences in your outcome of interest across your categories.\nFor example, let’s look at differences in the number of individuals who identified as Democrat, Republican, or Independent who do not support access to abortions, support access with some conditions, with more conditions, or always.\nWe can use modelsummary::datasummary_crosstab() to produce a nicely formatted cross tab of our variables:\n\ndatasummary_crosstab(abort4 ~ pid_3, data = nes)\n\n\n\n\n abort4\n\nDem\n Ind\nRep\nAll\n\n\n\n\nNever\nN\n187\n229\n252\n672\n\n\n\n% row\n27.8\n34.1\n37.5\n100.0\n\n\nSome conds\nN\n499\n583\n519\n1607\n\n\n\n% row\n31.1\n36.3\n32.3\n100.0\n\n\nMore conds\nN\n332\n337\n227\n898\n\n\n\n% row\n37.0\n37.5\n25.3\n100.0\n\n\nAlways\nN\n1325\n964\n381\n2680\n\n\n\n% row\n49.4\n36.0\n14.2\n100.0\n\n\nAll\nN\n2358\n2149\n1385\n5916\n\n\n\n% row\n39.9\n36.3\n23.4\n100.0\n\n\n\n\n\n\n\nWe can also visualise this:\n\nnes |&gt; \n  count(pid_3, abort4) |&gt; \n  drop_na(pid_3) |&gt; \n  ggplot(aes(x = n, y = pid_3, fill = abort4)) + \n  geom_bar(position = \"dodge\", stat = \"identity\") + \n  theme_minimal() + \n  labs(x = \"N\",\n       y = NULL,\n       fill = \"Level of support\") + \n  scale_fill_manual(values = c(\"#EDE5CF\",\"#E0C2A2\",\"#D39C83\",\"#C1766F\"))\n\n\n\n\nOr this:\n\nnes |&gt; \n  count(pid_3, abort4) |&gt; \n  drop_na(pid_3) |&gt; \n  ggplot(aes(x = n, y = abort4, fill = pid_3)) + \n  geom_bar(position = \"dodge\", stat = \"identity\") + \n  theme_minimal() + \n  labs(x = \"N\",\n       y = NULL,\n       fill = \"Party\") + \n  scale_fill_manual(values = c(\"#1375B7\",\"lightgrey\",\"#C93135\"))\n\n\n\n\n\n\n\nWe can use mean comparison tables to, well, compare means (in a table). Let’s compare the average response to the feeling thermometer (scale from 0 to 100) for the Republican party across parties:\n\nnes |&gt;\n  group_by(pid_3) |&gt; \n  summarise(\n    mean = mean(ft_rep, na.rm = T),\n    sd = sd(ft_rep, na.rm = T),\n    freq = n()\n  )\n\n# A tibble: 4 × 4\n  pid_3  mean    sd  freq\n  &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 Dem    24.5  21.8  2358\n2 Ind    43.0  23.1  2149\n3 Rep    70.3  18.3  1385\n4 &lt;NA&gt;   50    14.7    24\n\n\nAre these counts or averages meaningfully different from one another? We need some additional tools to answer that question. We will discuss those in the coming weeks.\n\n\n\nWe can also visualize the whole distribution of a continuous variable of interest within our categories.\n\nnes |&gt;\n  drop_na(pid_3) |&gt; \n  ggplot(aes(x = ft_rep, fill = pid_3)) + \n  geom_density(alpha = 0.5) + \n  theme_minimal() + \n  scale_fill_manual(values = c(\"#1375B7\",\"lightgrey\",\"#C93135\")) + \n  labs(x = \"Feeling thermometer\",\n       y = \"Density\", \n       fill = \"Party\")\n\n\n\n\nAs expected, Democrats appear to respond least favourably to the Republican Party, followed by Independents, and Republicans. This is demonstrated by both the mean comparison table and density plots."
  },
  {
    "objectID": "content/04-research_design.html",
    "href": "content/04-research_design.html",
    "title": "Research Design",
    "section": "",
    "text": "Pollock & Edwards, Chapter 4\n\n\n\n Pollock & Edwards R Companion, Chapter 5"
  },
  {
    "objectID": "content/04-research_design.html#readings",
    "href": "content/04-research_design.html#readings",
    "title": "Research Design",
    "section": "",
    "text": "Pollock & Edwards, Chapter 4\n\n\n\n Pollock & Edwards R Companion, Chapter 5"
  },
  {
    "objectID": "content/04-research_design.html#section",
    "href": "content/04-research_design.html#section",
    "title": "Research Design",
    "section": "Section",
    "text": "Section\n\nPrerequisites\n\ninstall.packages(c(\"rio\", \"httr2\", \"rvest\"))\n\n\n# Data cleaning and handling\nlibrary(tidyverse)\n\n# Reading data files\nlibrary(rio)\n\n# Web-scraping\nlibrary(httr2)\nlibrary(rvest)\n\n\n\nHow will what you learn this week help your research?\nCritical to our ability to understand the relationship between our outcome of interest and all of the variables that we think determine (or are, at least, associated) with that outcome is our ability to observe those variables moving with one another. To do this, we need quality data that accurately measure those variables. This week, we discuss how you can collect that quality data. The method by which you do this is very important to the quality of that data (i.e. how accurately it reflects the phenomena in which you are interested).\nWe have two broad methods at our disposal: observational research and experimental studies. Experimental studies are considered to be the gold standard for causal inference (your ability to confidently say that changes to your independent variable cause changes to your dependent variable).\nExperiments are not always feasible or appropriate. In fact, a lot of political science research (particularly international relations research) relies on observational studies. Unlike experiments, in which the researcher intervenes, observational research involves recording changes in your variables without influencing those variables.\nWhen you conduct your own research, you will need to make important decisions about what data you will use to uncover the relationships described by your theory. This week introduces you to some of the factors you will need to consider when making those decisions.\n\n\nData collection\nA lot of political science research uses observational data to make inferences about the relationship between some outcome of interest and various factors that are associated with that outcome of interest. For example, we collect data on the incidence of civil war. We also collect data on the regime type of the governments that have experienced civil wars and those that have not. We can then look at whether more civil wars occur in countries with particular regime types. This week, I will introduce some common techniques for collecting observational data used by political scientists.\n\nCommon data sets\nThere are many large-scale data sets that are commonly used in political science research. These include: the American National Election Survey, with which you are becoming very familiar; the Varieties of Democracy data set; and various Correlates of War data sets, include the Militarized Interstate Disputes data set.\nThe easiest way to access these data sets is via their associated websites. You can download the data file, store it in your R project, and use it in your analysis.\nHowever, these data sets are updated regularly. It can be tedious to download new data files every time they are updated.\n\n\nProgrammatically downloading data sets from the internet\nIt is very easy to download a file programmatically. The URL used to access the file can be thought of as a file path. Therefore, all you need to do is provide that URL (file path) to the appropriate R command and you will read in that data set directly from the internet.\nFor example, let’s read the UCDP/PRIO Armed Conflict Data Set into our current R session.\n\n\n\n\n\n\nNote\n\n\n\nFrom the UCDP website, the UCDP/PRIO Armed Conflict Data Set is:\nA conflict-year dataset with information on armed conflict where at least one party is the government of a state in the time period 1946-2022.\n\n\nWe can use the very flexible rio::import() function to do this:\n\nucdp_actor_df &lt;- import(\"https://ucdp.uu.se/downloads/ucdpprio/ucdp-prio-acd-231-csv.zip\") |&gt; \n  as_tibble()\n\nhead(ucdp_actor_df)\n\n# A tibble: 6 × 28\n  conflict_id location   side_a side_a_id side_a_2nd side_b side_b_id side_b_2nd\n        &lt;int&gt; &lt;chr&gt;      &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;     \n1       11342 India      Gover… 141       \"\"         GNLA   1163      \"\"        \n2       11342 India      Gover… 141       \"\"         GNLA   1163      \"\"        \n3       11343 Egypt, Is… Gover… 117       \"\"         Gover… 121       \"\"        \n4       11343 Egypt, Is… Gover… 117       \"\"         Gover… 121       \"\"        \n5       11343 Egypt, Is… Gover… 117       \"\"         Gover… 121       \"\"        \n6       11343 Egypt, Is… Gover… 117       \"\"         Gover… 121       \"\"        \n# ℹ 20 more variables: incompatibility &lt;int&gt;, territory_name &lt;chr&gt;, year &lt;int&gt;,\n#   intensity_level &lt;int&gt;, cumulative_intensity &lt;int&gt;, type_of_conflict &lt;int&gt;,\n#   start_date &lt;IDate&gt;, start_prec &lt;int&gt;, start_date2 &lt;IDate&gt;,\n#   start_prec2 &lt;int&gt;, ep_end &lt;int&gt;, ep_end_date &lt;IDate&gt;, ep_end_prec &lt;lgl&gt;,\n#   gwno_a &lt;chr&gt;, gwno_a_2nd &lt;chr&gt;, gwno_b &lt;chr&gt;, gwno_b_2nd &lt;chr&gt;,\n#   gwno_loc &lt;chr&gt;, region &lt;chr&gt;, version &lt;dbl&gt;\n\n\nInsofar as that URL points to the most up-to-date data file, you now have programmatic access to that data set.\n\n\nAPIs\nThis process is great because it is very straightforward and easy to implement. However, it can often not be very durable. Sometimes links break, or the authors of the data set change the structure of the data set stored in that data file, or they make a new link for more up-to-date data.\nPopular data sets often come with APIs, or Application Programming Interfaces. These can help us maintain durable, programmatic access to our data.\nAPIs can get a bit complicated. Today, I am going to briefly introduce you to them and some packages that can help you work with them from R. This is, by no means, a comprehensive tutorial on working with APIs, but it should give you the tools to get started with this very useful data collection process.\nThe best package in R for working with APIs is httr2. The documentation for this package is great and I would encourage you to have a look at it if you get stuck.\nUCDP provides an API to access their data sets. You can see the documentation for this API here: https://ucdp.uu.se/apidocs/.\nTo start, we need to load the httr2 package into our R session.\n\nlibrary(httr2)\n\nNext, we need to create our API request. For the UCDP API (and many other APIs), this essentially involves constructing a URL from which you will download the data.\nThe UCDP API request takes four different arguments:\n\nThe &lt;resource&gt;, or the data set you want to access. We will stick with the UCDP/PRIO Armed Conflict Data Set.\nThe &lt;version&gt;, or the version of the data set you want to access. As I said, this data set is regularly updated. At the time of writing, the latest version of the data set is 23.1.\nThe &lt;pagesize&gt; and page. This data set is (kind of) large: it has 2,626 rows. The API will only provide between 1 and 1,000 rows at a time. Therefore, if you want to return more than 1,000 rows (as you would need to if you wanted to access the whole data set), you need to set the page size to 1,000 and iterate over multiple pages (in this case, we need to go through 3 pages to get the full 2,626 rows.\n\nLet’s start simply by accessing the first 10 rows of the data set.\nFirst, we need to get the base URL. This is provided in the UCDP API documentation. You will see that they provide the following base URL:\n\nucdp_url &lt;- \"https://ucdpapi.pcr.uu.se/api/&lt;resource&gt;/&lt;version&gt;?&lt;pagesize=x&gt;&&lt;page=x&gt;\"\n\nYou can see where we need to insert our four arguments: &lt;resource&gt;, &lt;version&gt;, &lt;pagesize=x&gt;, and &lt;page=x&gt;. We can get the relevant values from the UCDP API documentation. These are:\n\n&lt;resource&gt;: ucdpprioconflict for the UCDP/PRIO Armed Conflict Data Set;\n&lt;version&gt;: 23.1.\n\nWe will start slowly by retrieving the first 10 rows of this data set. Therefore:\n\n&lt;pagesize=x&gt;: &lt;pagesize=10&gt;;\npage=x: page=1.\n\nAdding these into our base URL creates the following request URL:\n\nucdp_url &lt;- \"https://ucdpapi.pcr.uu.se/api/ucdpprioconflict/23.1?pagesize=10&page=1\"\n\nNow that we have our URL, we can make our request to the API using httr2::request():\n\nreq &lt;- request(ucdp_url)\nreq\n\nNext we need to perform the request using httr2::req_perform():\n\nresp &lt;- req |&gt; \n  req_perform()\n\nWe can check the status of our request:\n\nresp |&gt; \n  resp_status()\n\n[1] 200\n\n\n200 is good! It means that we have successfully performed our request. For a full list of HTTP status codes, look here.\nNow, let’s get our data! You can look at it using the following command:\n\nresp |&gt; \n  resp_body_json()\n\nThe response is an array of JSON objects. JSON is just a very light-weight way of sharing data. Lots of APIs will respond with this structure.\nThe API gave us lots of information. We want the Result object. This is where the data set is stored. First, we need to access the data from the response. We do this using the httr2::resp_body_json() function.\n\nresults_raw &lt;- resp |&gt; \n  resp_body_json()\n\nThe data set is stored in the Results object. You can have a look at it by running this:\n\nresults_raw$Result\n\nThis is a JSON object, which can be a bit difficult to work with. It is much easier to work with a tibble or data frame. To convert this JSON object to a tibble, we can use tibble::enframe() and tidyr::unnest_wider() (both of which are loaded with tidyverse.\n\nresults_df &lt;- enframe(results_raw$Result) |&gt; \n  unnest_wider(value)\n\nresults_df\n\n# A tibble: 10 × 29\n    name conflict_id location    side_a    side_a_Id side_a_2nd side_b side_b_Id\n   &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;  &lt;chr&gt;    \n 1     1 11345       South Sudan Governme… 113       \"Governme… SPLM/… 4226     \n 2     2 11345       South Sudan Governme… 113       \"Governme… SPLM/… 4226     \n 3     3 11345       South Sudan Governme… 113       \"\"         SPLM/… 4226     \n 4     4 11345       South Sudan Governme… 113       \"\"         SPLM/… 4226     \n 5     5 11345       South Sudan Governme… 113       \"\"         SPLM/… 4226     \n 6     6 11345       South Sudan Governme… 113       \"\"         NAS    6794     \n 7     7 11345       South Sudan Governme… 113       \"\"         NAS    6794     \n 8     8 11345       South Sudan Governme… 113       \"\"         NAS, … 6794, 82…\n 9     9 11346       Libya       Governme… 111       \" Governm… Force… 1126, 11…\n10    10 11346       Libya       Governme… 111       \"\"         ASL    7046     \n# ℹ 21 more variables: side_b_2nd &lt;chr&gt;, incompatibility &lt;chr&gt;,\n#   territory_name &lt;chr&gt;, year &lt;chr&gt;, intensity_level &lt;chr&gt;,\n#   cumulative_intensity &lt;chr&gt;, type_of_conflict &lt;chr&gt;, start_date &lt;chr&gt;,\n#   start_prec &lt;chr&gt;, start_date2 &lt;chr&gt;, start_prec2 &lt;chr&gt;, ep_end &lt;chr&gt;,\n#   ep_end_date &lt;chr&gt;, ep_end_prec &lt;chr&gt;, gwno_a &lt;chr&gt;, gwno_a_2nd &lt;chr&gt;,\n#   gwno_b &lt;chr&gt;, gwno_b_2nd &lt;chr&gt;, gwno_loc &lt;chr&gt;, region &lt;chr&gt;, version &lt;chr&gt;\n\n\nCool! We now have the first 10 rows of the UCDP/PRIO Armed Conflict Data Set.\nYou can do more precise requests with APIs. For example, you can request data for specific countries or time frames. This can be very useful when you are working with big data because it can allow you to work in memory by segmenting your analysis.\n\n\nWeb scraping\nAnother useful method for collecting data from the internet is web scraping. Web scraping allows you to extract data from a website. I am going to very quickly introduce you to this process and some useful R packages for performing web scraping. Again, this will be a very cursory introduction that aims to provide you with the tools to build up this skill if you need it.\nThe best package for web scraping in R is rvest. We can use this package to harvest data from the web.\nFirst, let’s load rvest into our current R session:\n\nlibrary(rvest)\n\nLet’s practice by creating a data set of all of the UNSC resolutions passed this year. The UNSC provides a table of these resolutions on its website:\n\nThis table includes some very useful information for anyone interested in looking at UNSC behaviour over time. It provides the unique resolution ID, the data of adoption, and a brief title or description of the resolution. We are going to use rvest and friends to read that information into R so that we can analyze it.\nThe first step you need to do is to read in the web page:\n\nunsc_res &lt;- read_html(\"https://www.un.org/securitycouncil/content/resolutions-adopted-security-council-2023\")\n\nWeb pages are (generally) HTML files. HTML is a markup language that is the standard format for documents displayed in web browsers.\nWe can have a look at this HTML file:\n\nunsc_res\n\n{html_document}\n&lt;html lang=\"en\" dir=\"ltr\"&gt;\n[1] &lt;head&gt;\\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] &lt;body class=\"layout-no-sidebars has-featured-top page-node-691811 path-no ...\n\n\nI am not about to teach you yet another scripting language. All you need to know is that HTML documents are highly structured. We can use this structure to point R to the specific part of the document that we want to scrape.\nHere is a basic example of HTML (from the rvest documentation):\n&lt;html&gt;\n&lt;head&gt;\n  &lt;title&gt;Page title&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n  &lt;h1 id='first'&gt;A heading&lt;/h1&gt;\n  &lt;p&gt;Some text &amp; &lt;b&gt;some bold text.&lt;/b&gt;&lt;/p&gt;\n  &lt;img src='myimg.png' width='100' height='100'&gt;\n&lt;/body&gt;\nHTML is hierarchical: all elements are nested within one another, providing them with a place within the documentation.\nAll HTML elements are contained within two tags: a start tag (&lt;tag&gt;) and corresponding end tag (&lt;/tag&gt;). These tags can have attributes that can group them together.\nFor example, the above HTML code includes a heading (&lt;h1&gt;) that includes an id attribute 'first'. The text of the heading - A heading - is contained within the heading tag. We know where the heading ends because it closes out with a &lt;/h1&gt; tag.\nHere, we want to scrape the table that contains all the useful information about the resolutions. We need to find the part of the HTML document that contains that table.\nHead back to the web page. We are looking at the rendered HTML. We need to find where in this document the table is located. To do this, right click anywhere in the table and select Inspect.\n\nA new window will pop up on the side of your web page that shows you the raw HTML language that is generating this web page.\n\nThis can look a bit weird if you have not come across HTML before, but it can be easy to work with. Scroll your mouse across different lines in that raw HTML window. You will see the corresponding parts of the rendered page light up.\n\nScroll until you have highlighted the whole table. You will be on a line that reads: &lt;table class=\"table table-striped table-sm\"&gt;.\n\nWithout getting too into the HTML weeds, this is the code used to generate the table. It includes a couple of unique identifiers that are useful for web scraping. Here, we are going to use the unique class of this table.\nThe class is table table-striped table-sm. When providing that to httr2::html_element() (which is function we use to select that part of the HTML document), we need to replace those spaces with full stops:\n\nunsc_res |&gt; \n  html_element(\"table.table-striped.table-sm\")\n\n{html_node}\n&lt;table class=\"table table-striped table-sm\"&gt;\n[1] &lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;td&gt;&lt;a href=\"http://undocs.org/en/S/RES/2721(2023)\"&gt;S/RES/ ...\n\n\nWe have filtered the whole HTML document to only the table we want to select. We don’t want to work with this table in HTML. Let’s convert it into a data frame. We can use the very helpful httr2::html_table() function to extract this tabular data and convert it:\n\nunsc_res |&gt; \n  html_element(\"table.table-striped.table-sm\") |&gt; \n  html_table()\n\n# A tibble: 50 × 3\n   X1                X2               X3                                        \n   &lt;chr&gt;             &lt;chr&gt;            &lt;chr&gt;                                     \n 1 S/RES/2721 (2023) 29 December 2023 The situation in Afghanistan              \n 2 S/RES/2720 (2023) 22 December 2023 The situation in the Middle East, includi…\n 3 S/RES/2719 (2023) 21 December 2023 Cooperation between the United Nations an…\n 4 S/RES/2718 (2023) 21 December 2023 The situation in the Middle East (UNDOF)  \n 5 S/RES/2717 (2023) 19 December 2023 The situation concerning the Democratic R…\n 6 S/RES/2716 (2023) 14 December 2023 Threats to to international peace and sec…\n 7 S/RES/2715 (2023) 1 December 2023  Reports of the Secretary-General on the S…\n 8 S/RES/2714 (2023) 1 December 2023  The situation in Somalia                  \n 9 S/RES/2713 (2023) 1 December 2023  Peace and Security in Africa              \n10 S/RES/2712 (2023) 15 November 2023 The situation in the Middle East, includi…\n# ℹ 40 more rows\n\n\nAwesome! We now have a table of all of the resolutions passed by the UNSC this year. Let’s clean it up a bit:\n\nunsc_res_df &lt;- unsc_res |&gt; \n  html_element(\"table.table-striped.table-sm\") |&gt; \n  html_table() |&gt; \n  rename(id = X1,\n         date_adoped = X2,\n         title = X3)\n\nunsc_res_df\n\n# A tibble: 50 × 3\n   id                date_adoped      title                                     \n   &lt;chr&gt;             &lt;chr&gt;            &lt;chr&gt;                                     \n 1 S/RES/2721 (2023) 29 December 2023 The situation in Afghanistan              \n 2 S/RES/2720 (2023) 22 December 2023 The situation in the Middle East, includi…\n 3 S/RES/2719 (2023) 21 December 2023 Cooperation between the United Nations an…\n 4 S/RES/2718 (2023) 21 December 2023 The situation in the Middle East (UNDOF)  \n 5 S/RES/2717 (2023) 19 December 2023 The situation concerning the Democratic R…\n 6 S/RES/2716 (2023) 14 December 2023 Threats to to international peace and sec…\n 7 S/RES/2715 (2023) 1 December 2023  Reports of the Secretary-General on the S…\n 8 S/RES/2714 (2023) 1 December 2023  The situation in Somalia                  \n 9 S/RES/2713 (2023) 1 December 2023  Peace and Security in Africa              \n10 S/RES/2712 (2023) 15 November 2023 The situation in the Middle East, includi…\n# ℹ 40 more rows\n\n\nFor more information on web-scraping, I recommend reading the Web scraping 101 article in the rvest documentation."
  },
  {
    "objectID": "content/09-hypothesis_testing.html",
    "href": "content/09-hypothesis_testing.html",
    "title": "Hypothesis Testing III",
    "section": "",
    "text": "Pollock & Edwards R Companion, Chapter 7"
  },
  {
    "objectID": "content/09-hypothesis_testing.html#readings",
    "href": "content/09-hypothesis_testing.html#readings",
    "title": "Hypothesis Testing III",
    "section": "",
    "text": "Pollock & Edwards R Companion, Chapter 7"
  },
  {
    "objectID": "content/09-hypothesis_testing.html#section",
    "href": "content/09-hypothesis_testing.html#section",
    "title": "Hypothesis Testing III",
    "section": "Section",
    "text": "Section\n\nlibrary(tidyverse)\nlibrary(modelsummary)\nlibrary(infer)\nlibrary(poliscidata)\n\nHow can we test hypotheses that ask questions of categorical data? For example, is an individual’s level of attendance at religious ceremonies associated with their party identification? Do democracies join more international organizations than non-democracies?\nTo answer these questions, we need to compare behavior across categories. Are there meaningful differences between categories?\n\nFederal spending on parks and recreation\nWe will explore hypothesis testing across categorical variables by answering the question: is an individual’s party identification associated with their support for current levels of federal spending on parks and recreation. We will use data from the GSS, obtained using poliscidata::gss.\n\ngss &lt;- poliscidata::gss |&gt; \n  # Select only the relevant columns\n  select(id, partyid_3, natpark) |&gt; \n  # Remove non-complete responses\n  drop_na()\n\n\n\nCalculating our observed counts\nFirst, we need to look at our observed data. We will make a cross tab of the data using modelsummary::datasummary_crosstab().\n\ndatasummary_crosstab(natpark ~ partyid_3, data = gss)\n\n\n\n\nnatpark\n\nDem\n Ind\nRep\nAll\n\n\n\n\nToo little\nN\n239\n251\n104\n594\n\n\n\n% row\n40.2\n42.3\n17.5\n100.0\n\n\nAbout right\nN\n413\n450\n290\n1153\n\n\n\n% row\n35.8\n39.0\n25.2\n100.0\n\n\nToo much\nN\n28\n46\n35\n109\n\n\n\n% row\n25.7\n42.2\n32.1\n100.0\n\n\nAll\nN\n680\n747\n429\n1856\n\n\n\n% row\n36.6\n40.2\n23.1\n100.0\n\n\n\n\n\n\n\nThe GSS surveyed 1,856 individuals in 2012, asking them of their party identification and level of support for current federal spending on parks and recreation.\n\n  gss |&gt; \n    # Get the number of respondents in each party who indicated each level of \n    # support\n    count(partyid_3, natpark) |&gt; \n  # Convert these counts to proportions by party\n  group_by(partyid_3) |&gt; \n  mutate(prop = n / sum(n)) |&gt; \n  # Plot these proportions\n  ggplot(aes(x = natpark, y = prop)) + \n  geom_col() + \n  facet_wrap(~ partyid_3) + \n  labs(x = \"Support for spending on parks and recreation\",\n       y = \"Percentage of respondents\") + \n  theme_minimal() + \n  scale_y_continuous(labels = scales::label_percent())\n\n\n\n\nConsistent across parties, the majority of people think that the federal government is spending about the right amount on parks and recreation. However, a greater proportion of Democrats think that the government is spending too little on parks than do Republicans. Is this difference significant?\n\n\nSetting up our null world\nWhat would the distribution of respondents across parties and levels of support look like if there was no difference? What are our expected counts for each category in this null world? If we work this out then we can compare these expected values to our observed values. How likely is it that we would observe those values in a world in which there was no significant association between party identification and level of support for current funding of parks and recreation?\nTo work this out, we need to consider what number of respondents would fall into each category if the only difference between those counts was due to differences in the size of the groups: the number of people who support each party and the number of people who support each level of funding.\n\\[\nExpected\\ count = \\frac{Row\\ total * Column\\ total}{N}\n\\]\nFor our data:\n\n# Calculate the observed count for each category\nobs_values &lt;- count(gss, natpark, partyid_3, name = \"obs_n\")\n  \n# Calculate the total number of respondents for each party ID\npartyid_3_totals &lt;- count(gss, partyid_3, name = \"partyid_total\")\n# Calculate the total number of respondents for each support level\nnatpark_totals &lt;- count(gss, natpark, name = \"natpark_total\")\n\nobs_exp_counts &lt;- natpark_totals |&gt; \n  expand_grid(partyid_3_totals) |&gt; \n  relocate(partyid_3) |&gt; \n  # Calculated the expected values\n  mutate(exp_n = (natpark_total * partyid_total) / nrow(gss)) |&gt;\n  # Add the observed values for comparison\n  left_join(obs_values, by = c(\"partyid_3\", \"natpark\"))\n\nobs_exp_counts\n\n# A tibble: 9 × 6\n  partyid_3 natpark     natpark_total partyid_total exp_n obs_n\n  &lt;fct&gt;     &lt;fct&gt;               &lt;int&gt;         &lt;int&gt; &lt;dbl&gt; &lt;int&gt;\n1 Dem       Too little            594           680 218.    239\n2 Ind       Too little            594           747 239.    251\n3 Rep       Too little            594           429 137.    104\n4 Dem       About right          1153           680 422.    413\n5 Ind       About right          1153           747 464.    450\n6 Rep       About right          1153           429 267.    290\n7 Dem       Too much              109           680  39.9    28\n8 Ind       Too much              109           747  43.9    46\n9 Rep       Too much              109           429  25.2    35\n\n\n\n\nComparing that null world to our observed counts\nWe have now worked out the number of respondents who would fall into each category if their party identification had no effect on their level of support for current federal spending on parks and recreation. We can now compare that expected value to the counts we actually observed in our survey:\n\nggplot(obs_exp_counts, aes(y = partyid_3, colour = natpark)) + \n  geom_segment(aes(x = exp_n, xend = obs_n, yend = partyid_3)) + \n  geom_point(aes(x = exp_n), shape = 1) + \n  geom_point(aes(x = obs_n)) + \n  labs(x = \"Number of respondents\",\n       y = NULL,\n       colour = \"Support for funding levels\",\n       caption = \"Hollow points represent the expected values, solid points represent the observed values.\") + \n  theme_minimal()\n\n\n\n\nWe are very interested in these differences. The greater the difference, the less likely we would be able to conduct a well-run survey and get the proportion of respondents we observed if, in fact, we lived in the null world. How big does this difference need to be before we can confidently reject the null hypothesis of no association?\n\n\nIs this difference significant?\nTo answer this question, we first need a single number that accounts for the differences we have observed across our different categories. For this, we can use the chi-square statistic.\n\\[\n\\chi_i^A = \\frac{sd}{\\sqrt{n}} = \\hat{p}(1 - \\hat{p}) = \\bar{x} - \\mu_0 = \\beta_0 + \\beta_1x\n\\]\nThe number is \\(\\Sigma\\).\n\\[\n\\chi^2 = \\Sigma{\\frac{(Observed\\ count - Expected\\ count)^2}{Expected\\ count}}\n\\]\nThis statistic provides us with a summary of the total difference between our observed values and values we would expect if there was no association between party identification and support for current funding levels.\nLet’s calculate that for our data:\n\nchi_sq &lt;- obs_exp_counts |&gt; \n  mutate(diff = obs_n - exp_n,\n         diff_2 = diff^2,\n         diff_2_standard = diff_2 / exp_n) |&gt; \n  summarise(chi_sq = sum(diff_2_standard)) |&gt; \n  pull()\n\nchi_sq\n\n[1] 20.96365\n\n\nNext, we need to see how likely we would be to observe this difference between our observed and expected values (represented by that chi-squared value) if the null hypothesis were true.\nTo do this, we need to calculate our degrees of freedom:\n\\[\ndf = (Number\\ of\\ rows - 1)(Number\\ of\\ columns - 1)\n\\]\nThen we can use pchisq() to access the probability that we would observe this difference or a greater difference if the null hypothesis were true.\n\npchisq(chi_sq, df = 4, lower.tail = F)\n\n[1] 0.0003219683\n\n\nIt is super unlikely that I would observe these counts in a world in which party identification had no effect on an individual’s level of support for current federal spending on parks and recreation. In fact, we can reject the null hypothesis with over 99 percent confidence.\n\n\nA handy shortcut\nWe can do this whole process using one command in R:\n\nchisq.test(gss$natpark, gss$partyid_3)\n\n\n    Pearson's Chi-squared test\n\ndata:  gss$natpark and gss$partyid_3\nX-squared = 20.964, df = 4, p-value = 0.000322\n\n\n\n\nA simulation approach\nLike last week, it can sometimes be helpful to see these tests in action. Let’s simulate our null world and compare it to our observation.\n\nnatpark_null &lt;- gss |&gt; \n  specify(natpark ~ partyid_3) |&gt; \n  hypothesize(null = \"independence\") |&gt; \n  generate(reps = 5000, type = \"permute\")\n\nnatpark_null\n\nResponse: natpark (factor)\nExplanatory: partyid_3 (factor)\nNull Hypothesis: independence\n# A tibble: 9,280,000 × 3\n# Groups:   replicate [5,000]\n   natpark     partyid_3 replicate\n   &lt;fct&gt;       &lt;fct&gt;         &lt;int&gt;\n 1 About right Ind               1\n 2 About right Ind               1\n 3 Too little  Ind               1\n 4 About right Rep               1\n 5 Too little  Dem               1\n 6 Too little  Dem               1\n 7 About right Ind               1\n 8 About right Ind               1\n 9 About right Ind               1\n10 About right Dem               1\n# ℹ 9,279,990 more rows\n\n\nWe have simulated drawing 5,000 different samples from our null world in which there is no meaningful difference between parties. We created this null world by randomly assigning each respondent with a party identification. If we live in a world in which there is no meaningful effect of party identification on support for current funding levels, respondents’ party identification can be shuffled randomly!\nThese simulated counts represent our expected counts. Therefore, we can calculate the chi-squared statistic of the difference between what we actually observed and these expected counts as we did above. This time, we can use infer::calculate().\n\nchi_sq_sim &lt;- calculate(natpark_null, stat = \"Chisq\")\nchi_sq_sim\n\nResponse: natpark (factor)\nExplanatory: partyid_3 (factor)\nNull Hypothesis: independence\n# A tibble: 5,000 × 2\n   replicate  stat\n       &lt;int&gt; &lt;dbl&gt;\n 1         1  4.07\n 2         2  2.77\n 3         3  2.66\n 4         4  2.55\n 5         5  6.52\n 6         6  3.59\n 7         7  4.93\n 8         8  3.05\n 9         9  5.71\n10        10  4.93\n# ℹ 4,990 more rows\n\n\nNow we have the chi-squared statistic for our 5,000 different samples. Let’s visualize them:\n\nvisualize(chi_sq_sim)\n\n\n\n\nThis follows the chi-squared distribution! When we calculated the p-value of our chi-squared statistic above, we were assuming that this was the case. Here, we are explicitly creating the distribution. Therefore, we can calculate the proportion of these simulated chi-squared statistics from our 5,000 samples from the null world that are equal to or greater than the difference between the expected and observed counts for our data.\n\nvisualize(chi_sq_sim) + \n  geom_vline(aes(xintercept = chi_sq), colour = \"red\")\n\n\n\n\nThe vertical red line on the graph above shows the chi-squared statistic we calculated earlier.\nWe can use infer::get_p_value() to calculate the proportion of simulated chi-squared statistics that are greater than or equal to our observed chi-squared statistic:\n\nchi_sq_sim |&gt; \n  get_p_value(obs_stat = chi_sq, direction = \"greater\")|&gt; \n  mutate(p_value_clean = scales::pvalue(p_value))\n\n# A tibble: 1 × 2\n  p_value p_value_clean\n    &lt;dbl&gt; &lt;chr&gt;        \n1  0.0006 &lt;0.001"
  },
  {
    "objectID": "content/07-statistical_inference.html",
    "href": "content/07-statistical_inference.html",
    "title": "Hypothesis Testing I",
    "section": "",
    "text": "Pollock & Edwards, Chapter 6\n\n\n\n Pollock & Edwards R Companion, Chapter 6\n\n\n\n Gelman, Hill, and Vehtari Regression and Other Stories, Chapter 4\nHadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund, R for Data Science (2e), Chapter 26: Functions"
  },
  {
    "objectID": "content/07-statistical_inference.html#readings",
    "href": "content/07-statistical_inference.html#readings",
    "title": "Hypothesis Testing I",
    "section": "",
    "text": "Pollock & Edwards, Chapter 6\n\n\n\n Pollock & Edwards R Companion, Chapter 6\n\n\n\n Gelman, Hill, and Vehtari Regression and Other Stories, Chapter 4\nHadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund, R for Data Science (2e), Chapter 26: Functions"
  },
  {
    "objectID": "content/07-statistical_inference.html#section",
    "href": "content/07-statistical_inference.html#section",
    "title": "Hypothesis Testing I",
    "section": "Section",
    "text": "Section\n\nPrerequisites\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(poliscidata)\nlibrary(ggdist)\nlibrary(MetBrewer)\nlibrary(modelsummary)\nlibrary(DescTools)\n\nset.seed(1234)\n\n\n\nPopulation and sample\nSay we are interested in the proportion of US voters who will vote for Joe Biden in the 2024 general election. We cannot ask all US voters of their intentions. Instead, we ask a sample of the US voting population and infer from that sample the population’s intentions. When we generalize from the sample statistic to the parameter we are engaging in statistical inference.\n\n\n\n\n\n\nNote\n\n\n\nThe data point of interest among the population is referred to as the parameter. Here, it is the proportion of US voters who intend to vote for Joe Biden in the 2024 general election. We do not know this.\nThe data point of interest among the sample is referred to as the statistic. Here, it is the proportion of survey respondents who intend to vote for Joe Biden in the 2024 general election. We do know this.\nWe aim to have a statistic that accurately represents the parameter.\n\n\nWe want to know how many people intend to vote for Joe Biden. In other words, we want to know whether our survey results reflect the true proportion of the population who intend to vote for him. How can we be confident that our statistic represents the parameter?\nGenerally speaking, the more our sample “looks like” our population, the more confident we can be that we have a good statistic. Drawing on probability theory, our sample is increasingly likely to resemble our population with its randomness and size.\nYou should strive for a large pure random sample. In a pure random sample, every individual within your population is equally likely to be drawn. This is really hard to achieve! Think about normal election surveys. Many are conducted over the phone. There are plenty of people who do not have a landline phone, or do not pick up calls from unknown numbers, or who keep their phones on do not disturb during the day. These people will be harder to contact than those who are sitting by the phone waiting eagerly for a call. Even if you have access to all US voters’ phone numbers (never mind that some voters do not have phone numbers) and you take a random sample of those phone numbers and start calling, you still will not get a hold of them all with equal probability.\nYou should also strive for as large a sample as you can possibly get. More is always better in terms of statistical inference (if not your research budget or time). Remember back to our coin flips last week. The more coin flips we did, the closer we got to the true probability distribution between heads and tails. This principle also holds here.\n\n\nSampling error\nImagine you have a large and representative sample. You are still going to have some error. This is because your sample varies in all the normal ways events with uncertainty vary. To illustrate, let’s return to our coin flips.\nWe state our possible outcomes:\n\npossible_outcomes &lt;- c(\"HEADS\", \"TAILS\")\npossible_outcomes\n\n[1] \"HEADS\" \"TAILS\"\n\n\nWe flip our coin 100 times:\n\ncoin_flip_100 &lt;- sample(possible_outcomes, 100, replace = T, prob = c(0.5, 0.5))\ncoin_flip_100\n\n  [1] \"TAILS\" \"HEADS\" \"HEADS\" \"HEADS\" \"HEADS\" \"HEADS\" \"TAILS\" \"TAILS\" \"HEADS\"\n [10] \"HEADS\" \"HEADS\" \"HEADS\" \"TAILS\" \"HEADS\" \"TAILS\" \"HEADS\" \"TAILS\" \"TAILS\"\n [19] \"TAILS\" \"TAILS\" \"TAILS\" \"TAILS\" \"TAILS\" \"TAILS\" \"TAILS\" \"HEADS\" \"HEADS\"\n [28] \"HEADS\" \"HEADS\" \"TAILS\" \"TAILS\" \"TAILS\" \"TAILS\" \"HEADS\" \"TAILS\" \"HEADS\"\n [37] \"TAILS\" \"TAILS\" \"HEADS\" \"HEADS\" \"HEADS\" \"HEADS\" \"TAILS\" \"HEADS\" \"TAILS\"\n [46] \"HEADS\" \"HEADS\" \"TAILS\" \"TAILS\" \"HEADS\" \"TAILS\" \"TAILS\" \"HEADS\" \"HEADS\"\n [55] \"TAILS\" \"HEADS\" \"TAILS\" \"HEADS\" \"TAILS\" \"HEADS\" \"HEADS\" \"TAILS\" \"TAILS\"\n [64] \"TAILS\" \"TAILS\" \"HEADS\" \"TAILS\" \"HEADS\" \"TAILS\" \"HEADS\" \"TAILS\" \"HEADS\"\n [73] \"TAILS\" \"HEADS\" \"TAILS\" \"HEADS\" \"TAILS\" \"TAILS\" \"TAILS\" \"HEADS\" \"HEADS\"\n [82] \"TAILS\" \"TAILS\" \"HEADS\" \"TAILS\" \"HEADS\" \"TAILS\" \"TAILS\" \"TAILS\" \"HEADS\"\n [91] \"TAILS\" \"HEADS\" \"TAILS\" \"TAILS\" \"TAILS\" \"HEADS\" \"TAILS\" \"TAILS\" \"TAILS\"\n[100] \"HEADS\"\n\n\nWe know that the true probability of the fair coin landing on heads is 0.5. Therefore, if we flip a fair coin 100 times, we should get 50 heads. However, we also know from last week that these random draws are a bit noisy: we can get proportions that do not reflect the underlying probability of 0.5. What did we get in this draw?\n\ntable(coin_flip_100)\n\ncoin_flip_100\nHEADS TAILS \n   45    55 \n\n\nThe more flips we do, the closer we will get to that true probability distribution. To demonstrate, let’s do 1,000,000 100-coin flip trials and record the number of heads we get each time:\n\ncoin_flip &lt;- function(possible_outcomes, n) {\n  \n  outcomes &lt;- sample(possible_outcomes, size = n, replace = T, prob = c(0.5, 0.5))\n  \n  return(table(outcomes)[\"HEADS\"])\n  \n}\n\nresults &lt;- tibble(trial = 1:1e6) |&gt; \n  rowwise() |&gt; \n  mutate(n_heads = coin_flip(possible_outcomes, 100))\n\nresults\n\n# A tibble: 1,000,000 × 2\n# Rowwise: \n   trial n_heads\n   &lt;int&gt;   &lt;int&gt;\n 1     1      52\n 2     2      52\n 3     3      48\n 4     4      60\n 5     5      61\n 6     6      49\n 7     7      52\n 8     8      50\n 9     9      49\n10    10      45\n# ℹ 999,990 more rows\n\n\nWhat are the results of these repeated trials?\n\nggplot(results, aes(x = n_heads)) + \n  geom_histogram() + \n  geom_vline(xintercept = 50) + \n  theme_minimal() + \n  labs(x = \"Percentage of heads drawn in 100 coin-flip trial\",\n       y = \"Count of trials\")\n\n\n\n\nThe most common outcome of these 1,000,000 100-coin-flip trials is our true underlying proportion: \\(Pr(H, T) = \\{0.5, 0.5\\}\\). The next most common outcomes are very close to that true underlying proportion. If I were to select a trial at random from those 1,000,000 we just conducted, I am most likely to get one that resulted in 50% heads or very close to 50% heads. This is because there are many more trials that resulted in and around 50% heads than did not.\nWe do get some trials in which we draw many more or far fewer than our expected 50 heads. We have some as low as 26 heads and some as large as 74 heads. But again, the number of heads recorded in most of our trials are clustered around our expected 50. The average number of heads drawn in all of our trials is 49.991061 heads which is really, really close to our known parameter of 0.5 or 50%. Yay!\nSo, even with representative and large samples you will get some error. That’s okay. As long as the mean of the sampling distribution of an infinite number of identical trials would equal the true population parameter, we have an unbiased statistic that we can use to infer things about our population of interest. Here, each of the outcomes of our trials - including those ones that resulted in 26 heads and 74 heads - are unbiased statistics because the mean of infinite identical trials would be 50 heads.\n\n\nSampling distributions\nLet’s move on from coin flips. Suppose that we want to know how many Americans identify as Democrats. We will return to the American National Election Survey to answer this question.\nThis survey asks respondents whether they identify as a Democrat (this binary variable takes on 0 if not and 1 if they do). Let’s look at these data for our first five survey respondents:\n\nnes |&gt; \n  select(caseid, dem) |&gt; \n  head(5)\n\n  caseid dem\n1    408   0\n2   3282   1\n3   1942   0\n4    118   1\n5   5533   0\n\n\nLet’s very cheekily pretend that this is a complete survey of the entire voting population of America. That way, we can pretend that we know the true proportion of US voters (our population of interest) who identify as Democrats (our parameter).\nWhat is that proportion?\n\ntabyl(nes, dem)\n\n dem    n     percent valid_percent\n   0 3534 0.597363083     0.5997963\n   1 2358 0.398580122     0.4002037\n  NA   24 0.004056795            NA\n\n\nOkay, so let’s pretend that 40% of all US voters identify as Democrats.\nWe can’t survey all voters, so instead we take a representative and large simple random sample from this population:\n\nnes_sample &lt;- nes |&gt; \n  select(caseid, dem) |&gt; \n  slice_sample(n = 3000)\n\nWe have taken a random sample of 3,000 individuals (or 51% of our population of 5,916 voters). Every voter (member of our population) had an equal probability of being picked for this sample.\nWhat proportion of this sample identify as Democrats?\n\ntabyl(nes_sample, dem)\n\n dem    n     percent valid_percent\n   0 1793 0.597666667     0.6002678\n   1 1194 0.398000000     0.3997322\n  NA   13 0.004333333            NA\n\n\n39.80%. Nice! But what if we took a different sample of 3,000?\n\nnes_sample_2 &lt;- nes |&gt; \n  select(caseid, dem) |&gt; \n  slice_sample(n = 3000)\n\ntabyl(nes_sample_2, dem)\n\n dem    n percent valid_percent\n   0 1764   0.588     0.5903614\n   1 1224   0.408     0.4096386\n  NA   12   0.004            NA\n\n\nWe get a different answer: 40.80%. Of course! This is just like our different coin flip trials from last week and above. Each resulted in a different number of heads. The more flips we did, the closer we got to the true underlying probability distribution.\nLet’s take 1,000 different samples of 3,000 US voters and see what we get:\n\ndem_survey &lt;- function(df, n) {\n  \n  slice_sample(df, n = n) |&gt; \n    tabyl(dem) |&gt; \n    filter(dem == 1) |&gt; \n    pull(percent)\n  \n}\n\nnes_samples_1000 &lt;- tibble(survey = 1:1000) |&gt; \n  rowwise() |&gt; \n  mutate(prop_dem = dem_survey(select(nes, caseid, dem), 3000)) |&gt; \n  ungroup()\n\nnes_samples_1000\n\n# A tibble: 1,000 × 2\n   survey prop_dem\n    &lt;int&gt;    &lt;dbl&gt;\n 1      1    0.399\n 2      2    0.399\n 3      3    0.397\n 4      4    0.396\n 5      5    0.395\n 6      6    0.39 \n 7      7    0.403\n 8      8    0.396\n 9      9    0.401\n10     10    0.397\n# ℹ 990 more rows\n\n\n\nggplot(nes_samples_1000, aes(x = prop_dem)) + \n  geom_histogram() + \n  geom_vline(xintercept = tabyl(nes, dem) |&gt; filter(dem == 1) |&gt; pull(percent), \n             colour = \"lightblue\", \n             linewidth = 2) + \n  geom_vline(xintercept = mean(nes_samples_1000$prop_dem), \n             colour = \"pink\",\n             linewidth = 2) + \n  theme_minimal() + \n  labs(x = \"Proportion of respondents who identify as Democrats\",\n       y = \"Count of trials\")\n\n\n\n\nOn average, 39.85% of US voters in our 1,000 samples of 3,000 US voters identified as Democrats. This is shown by the pink line in the graph above. Our (cheeky) population proportion is 39.86% (shown by the blue line in the graph above). Yay! These are very, very close to each other and we only did 1,000 samples. If took more and more samples, the average proportion of respondents who identify as Democrats in those trials would get increasingly close to our population’s proportion of Democrats. In fact, theoretically, if we took an infinite number of samples from these 5,916 observations, the average of the proportion of individuals who identify as Democrats in each of those infinite samples would equal exactly the population’s proportion: 39.86%. Further, we know that the proportions found in each of those trials would be normally distributed around that mean, with the most common outcomes sitting at or very close to the mean.\nAs long as our sample of our population is large and randomly drawn, we know enough about its shape and central point to use it to infer what is going on in the population. When your sample is large and representative (see randomly drawn), your sampling distribution will be near normally distributed. The center will be at (or very, very close to) the population mean. This is called the Central Limit Theorem. This theorem suggests that statistics (including means, proportions, counts) from large and randomly drawn samples are very good approximations of the underlying (and often unobservable) population parameter.\n\n\nInferring from a single “trial”\nIn a lot of (social) science research it is not practical or, in some cases, possible to do many trials. For example, a lot of us study the onset, conduct, and termination of wars. Unlike a game of chess, you cannot reset and run a war many times in order to get your sampling distribution of your variable of interest.\nFurther, we often do not know the shape or size of our population. For example, the best guess we have of the demographics of the US population comes from the census. But this misses a lot of people. For example, if you want to study houselessness, you might need to rely on surveys of samples of people that may or may not be representative of this difficult to reach population of people.\nA lot of the time; therefore, you will have one data point: one mean, one proportion, one count. To use this one data point to infer something about our population of interest, we need to use some of lessons that we learned from above and make some pretty important assumptions.\nLet’s return to our survey work above. We took 1,000 different samples of 3,000 US voters. We asked each of those US voters whether they identified as Democrats. We then found the proportion of the 3,000 respondents who identified as Democrats in each of our 1,000 different samples. We then took the average of those 1,000 different proportions and compared it to our population average. In line with the Central Limit Theorem, we found that the average of our sample statistics was very, very close to our population parameter.\nOkay, now imagine that you could only run one of those trials. Let’s select one at random:\n\nnes_single &lt;- slice_sample(nes_samples_1000)\nnes_single\n\n# A tibble: 1 × 2\n  survey prop_dem\n   &lt;int&gt;    &lt;dbl&gt;\n1    703    0.384\n\n\nHow close is this single sample statistic to the population parameter of 39.86%? Pretty close! In fact, as discussed above, you are more likely to get a sample statistic close to the population parameter than not.\nRemember, when we ran multiple trials we got many sample statistics that were clustered around the population mean.\n\nggplot(nes_samples_1000, aes(x = prop_dem)) + \n  geom_histogram() + \n  geom_vline(xintercept = tabyl(nes, dem) |&gt; filter(dem == 1) |&gt; pull(percent)) + \n  theme_minimal() + \n  labs(x = \"Proportion of respondents who identify as Democrat\",\n       y = \"Count of trials\")\n\n\n\n\nSo, if you were to pick one of these trials at random, you are more likely to pick one with a sample statistic that is close to the population parameter than not. Convenient!\n\n\nHow confident can we be in our statistic?\nThat being said, we could get unlucky and have drawn a large and representative sample that sits at one of those extreme values. How confident can we be that our single sample statistic is close to the population parameter?\nRemember back to our week on descriptive statistics. There are some super handy properties of normal distributions on which we will draw.\n\nnorm_5_2 &lt;- tibble(x = rnorm(n = 1e6, mean = 5, sd = 2))\n\nggplot(norm_5_2, aes(x = x)) + \n  stat_slab(\n    aes(fill_ramp = after_stat(cut_cdf_qi(cdf))), fill = met.brewer(\"Egypt\")[2]\n  ) + \n  scale_fill_ramp_discrete(range = c(1, 0.2), guide = \"none\") + \n  theme_minimal()\n\n\n\n\nFor normally distributed data:\n\nApproximately 68% of the data fall within one standard deviation of the mean (the dark blue).\nApproximately 95% of the data fall within two standard deviations of the mean (the medium blue).\nApproximately 99.7% of the data fall within three standard deviations of the mean (the light blue).\n\nSo, if we assume that the statistic we get from our large and representative sample is our “best guess” at the population parameter, we can center our theoretical sampling distribution around this point. We know that these data are normally distributed. We can use what we know about normal distributions to identify the boundaries around which we are confident some proportion of all statistics from an infinite number of identically drawn samples would fall.\nLet’s make this more concrete by going back to a single random sample of 3,000 respondents from the NES survey. Let’s draw that sample:\n\nnes_sample &lt;- sample_n(nes, 3000) |&gt; \n  select(caseid, dem)\n\nhead(nes_sample)\n\n  caseid dem\n1   5144   0\n2   3483   1\n3   4773   0\n4   4340   0\n5   6385   0\n6    288   0\n\n\nIn this sample, 39.77% of respondents identify as Democrats. This is our best guess at our parameter of interest: the proportion of US voters who identify as Democrats.\nUsing what we know from above, what would our distribution of proportions look like if this was not the (average) proportion drawn from one trial but instead it was the average proportion drawn from an infinite number of identical trials? Well, those proportions would be normally distributed around that center point. To fill in that blank, we need one additional piece of information: the standard deviation (or spread) of those points around that center point.\n\n\n\n\n\n\nNote\n\n\n\nWhen we are looking at the standard deviation of the sampling distribution, we refer to it as the standard error.\n\n\nThe formula for working out the standard error of a proportion (such as the proportion of a population who identify as Democrats) is:\n\\[\nse(\\hat{p}) = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n\\]\nWhere \\(\\hat{p}\\) is the sample proportion (here: 0.398).\n\np_hat &lt;- tabyl(nes_sample, dem) |&gt; \n  filter(dem == 1) |&gt; \n  pull(percent)\nn &lt;- 3000\n\nse &lt;- sqrt((p_hat * (1 - p_hat)) / n)\nse\n\n[1] 0.00893547\n\n\nNow, let’s use this information to draw many different hypothetical proportions of respondents who identify as Democrats in many different hypothetical samples.\n\nggplot(tibble(x = rnorm(1e6, mean = p_hat, sd = se)), aes(x = x)) + \n  stat_halfeye(.width = c(0.95, 0.68)) + \n  theme_minimal() + \n  labs(x = \"Proportion of respondents who identify as Democrats\",\n       caption = \"Median and mean shown with point. One and two standard deviations are shown by the black bars.\")\n\n\n\n\nSo, if our sample is large and random and we were to draw it many different times (here, 1,000,000 times), we would get a sampling distribution of proportions that resembles the one presented above. This variation is due to random error. There is nothing wrong with our research design or sampling method.\nWe can use what we know about normal distributions to say that approximately 68% of these hypothetical proportions of people who identify as Democrats fall within one standard deviation of the mean, or:\n\nlower_68 &lt;- p_hat - 1 * se\nlower_68\n\n[1] 0.3887312\n\nupper_68 &lt;- p_hat + 1 * se\nupper_68\n\n[1] 0.4066021\n\n\nThese boundaries are marked by the thick black line on the graph above.\nAlso, approximately 95% of these hypothetical proportions of people who identify as Democrats fall within 1.96 standard deviations of the mean, or:\n\nlower_95 &lt;- p_hat - 1.96 * se\nlower_95\n\n[1] 0.3801531\n\nupper_95 &lt;- p_hat + 1.96 * se\nupper_95\n\n[1] 0.4151802\n\n\nThese boundaries are marked by the thinner black line on the graph above.\nWe are putting a lot of stead in our single sample. That’s okay as long as your sample is large and representative. Over these past few weeks we have discussed in sometimes painful detail why we can make some of the assumptions on which we rely. But, at the end of the day, you are the expert. You have explored your data with a critical eye. You have read everything you possibly can about this topic. You might have even gone out in the field and gotten your hands dirty. The more you know about your subject matter, the better you will be able to detect whether something strange is going on with your sample and your findings. This is so important. The strength of your empirical analysis is built on these foundations.\n\n\nChoosing your sample size\nWe know that our confidence around our point estimate increases with the number of observations. Formally:\n\\[\nse(\\hat{p}) = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n\\]\nSo, as you increase that \\(n\\), you decrease your standard error and you narrow the interval over which you have a given level of confidence.\nI have said before that more is always better. Technically, this is very, very true. But those marginal returns diminish. And those survey costs stack up.\nTo illustrate, let’s look at how much your standard error decreases as you increase your sample size.\n\nse_from_sample &lt;- function(n_samples) {\n  \n  nes |&gt; \n    slice_sample(n = n_samples) |&gt; \n    pull(dem) |&gt; \n    MeanSE(na.rm = T)\n  \n}\n\ntibble(sample_size = seq(from = 100, to = 3000, by = 100)) |&gt; \n  rowwise() |&gt; \n  mutate(se = se_from_sample(sample_size)) |&gt; \n  ggplot(aes(x = sample_size, y = se)) + \n  geom_line() + \n  theme_minimal() + \n  labs(x = \"Number of observations in the sample\",\n       y = \"Standard error\")\n\n\n\n\nMoving from 100 to 1,000 observations in your sample dramatically decreases your standard error. Moving the same distance from 1,000 to 1,900 makes a far smaller difference to your error.\n\n\nHypthesis testing\nGenerally speaking, we want to make some statement about what is going on in the world. For example, does trade dependence between states reduce the likelihood that those states will go to war with one another? Does election day voter registration increase the number of people who vote in elections in the US? Does party identification shape your views on abortion?\nWe can answer these questions by proposing and testing hypotheses, or measurable and observable statements about a population. For example, state dyads with high levels of trade dependence go to war with each other less frequently than state dyads with low levels of trade dependence. Election day voter registration increases the number of people who vote in an election.\nWe can use statistical tests of our data to test these hypotheses. Do we see support for our hypothesis in our data? How confident can we be that the differences identified in the data are meaningful and not just the product of random error?\nFrequentist statistics provides us with a means of answering that latter question. It asks you to imagine a world in which your hypothesis is wrong: there is no effect of trade dependence on the prevalence of war, or no effect of election-day registration on the number of people who vote.\nTo build our intuition for this approach, let’s consider a simple question: do more than zero Democrats support access to abortions with no conditions.\n\n\n\n\n\n\nNote\n\n\n\nMore interesting hypotheses are often framed as difference-of-means or difference-of-proportions tests. We will get to those next week.\n\n\nTo test this hypothesis, you you go out and you get a large, random sample from your population. Happily, we have access to the NES, which asked asked 5,916 US adults with which party they identified and the conditions under which they think abortions should be allowed. Here are the first five respondents’ answers:\n\nnes |&gt; \n  select(caseid, pid_3, abort4) |&gt; \n  head(5)\n\n  caseid pid_3     abort4\n1    408   Ind     Always\n2   3282   Dem More conds\n3   1942   Ind More conds\n4    118   Dem     Always\n5   5533   Ind     Always\n\n\nWe are going to test the hypothesis that more than zero Democrats said that they support access to abortions with no conditions. Let’s start by working out what proportion of Democrats in our survey support that statement:\n\nnes_sample &lt;- nes |&gt; \n  mutate(abortion_always = if_else(abort4 == \"Always\", 1, 0)) |&gt; \n  filter(dem == 1)\n\ntabyl(nes_sample, abortion_always)\n\n abortion_always    n     percent valid_percent\n               0 1018 0.431721798     0.4344857\n               1 1325 0.561916879     0.5655143\n              NA   15 0.006361323            NA\n\n\nOur sample found that 56% of Democrats think abortions should always be allowed. It looks like we have found support for our hypothesis. But what if this is just random noise? How can we be confident that this represents the true number of Democrats in the population who support unconditional access to abortions?\nAssuming that our sample is large and representative of the population, we can build out the hypothetical distribution of the proportion of Democrats who state their support for access to abortions without conditions in many hypothetical identically drawn samples of our population:\n\nse &lt;- sqrt((prop_dems_always * (1 - prop_dems_always)) / nrow(nes_sample))\n\nggplot(tibble(x = rnorm(1e6, mean = prop_dems_always, sd = se)), aes(x = x)) + \n  stat_halfeye(.width = c(0.95, 0.68)) + \n  theme_minimal() + \n  labs(x = \"Proportion of Democrats who always support access to abortion\",\n       caption = \"Median and mean shown with point. One and two standard deviations are shown by the black bars.\")\n\n\n\n\nBut how can we be sure that this difference is not just random error or noise? Let’s return to our statistical test. Imagine a world in which you are wrong: zero percent of Democrats support unconditional access to abortions. If this null hypothesis is true, how likely would we be take a large and random sample of our population and find that 56% of democrats within that population support unconditional access to abortions?\nLet’s add this null world to our findings:\n\nggplot(tibble(x = rnorm(1e6, mean = prop_dems_always, sd = se)), aes(x = x)) + \n  stat_halfeye(.width = c(0.95, 0.68)) + \n  geom_vline(xintercept = 0) + \n  theme_minimal() + \n  labs(x = \"Proportion of Democrats who always support access to abortion\",\n       caption = \"Median and mean shown with point. One and two standard deviations are shown by the black bars.\")\n\n\n\n\nThe null hypothesis of no effect (or zero Democrats) is shown by the vertical line at 0. It is highly unlikely that would we take a large and random sample of our population and find that 56% of democrats within that population support unconditional access to abortions if our null hypothesis was, in fact, true."
  },
  {
    "objectID": "content/05-applications.html",
    "href": "content/05-applications.html",
    "title": "Applications & Midterm Exam Review",
    "section": "",
    "text": "Segal, Jeffrey A. & Albert D. Cover. 1989. “Ideological Values and the Votes of U.S. Supreme Court Justices.” American Political Science Review 83(2): 557-565.\n Sondheimer, Rachel Milstein & Donald P. Green. 2010. “Using Experiments to Estimate the Effects of Education on Voter Turnout.” American Journal of Political Science 54(1): 174-189."
  },
  {
    "objectID": "content/05-applications.html#readings",
    "href": "content/05-applications.html#readings",
    "title": "Applications & Midterm Exam Review",
    "section": "",
    "text": "Segal, Jeffrey A. & Albert D. Cover. 1989. “Ideological Values and the Votes of U.S. Supreme Court Justices.” American Political Science Review 83(2): 557-565.\n Sondheimer, Rachel Milstein & Donald P. Green. 2010. “Using Experiments to Estimate the Effects of Education on Voter Turnout.” American Journal of Political Science 54(1): 174-189."
  },
  {
    "objectID": "content/02-descriptive_statistics.html",
    "href": "content/02-descriptive_statistics.html",
    "title": "Descriptive Statistics",
    "section": "",
    "text": "Throughout this course, you will need a series of data sets I have collected, cleaned, and stored in the polisciols R package. These data sets were collected and published by political scientists (including some incredible GVPT alumni).\nThis package is not published on CRAN1, so you will need to install it using the following code:\n\ninstall.packages(\"devtools\")\n\ndevtools::install_github(\"hgoers/polisciols\")\n\nRemember, you only need to do this once on your computer. Run this in the console.\nYou will also need access to the following R packages to complete this week’s activities:\n\n\n\n\n\n\nInstalling packages\n\n\n\n\n\nIf you have not already done so, please install or update these packages by running the following in your console:\n\ninstall.packages(c(\"tidyverse\",\n                   \"wbstats\",\n                   \"janitor\",\n                   \"skimr\",\n                   \"countrycode\",\n                   \"scales\"))\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(polisciols)\nlibrary(wbstats)\nlibrary(janitor)\nlibrary(skimr)\nlibrary(countrycode)\nlibrary(scales)"
  },
  {
    "objectID": "content/02-descriptive_statistics.html#set-up",
    "href": "content/02-descriptive_statistics.html#set-up",
    "title": "Descriptive Statistics",
    "section": "",
    "text": "Throughout this course, you will need a series of data sets I have collected, cleaned, and stored in the polisciols R package. These data sets were collected and published by political scientists (including some incredible GVPT alumni).\nThis package is not published on CRAN1, so you will need to install it using the following code:\n\ninstall.packages(\"devtools\")\n\ndevtools::install_github(\"hgoers/polisciols\")\n\nRemember, you only need to do this once on your computer. Run this in the console.\nYou will also need access to the following R packages to complete this week’s activities:\n\n\n\n\n\n\nInstalling packages\n\n\n\n\n\nIf you have not already done so, please install or update these packages by running the following in your console:\n\ninstall.packages(c(\"tidyverse\",\n                   \"wbstats\",\n                   \"janitor\",\n                   \"skimr\",\n                   \"countrycode\",\n                   \"scales\"))\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(polisciols)\nlibrary(wbstats)\nlibrary(janitor)\nlibrary(skimr)\nlibrary(countrycode)\nlibrary(scales)"
  },
  {
    "objectID": "content/02-descriptive_statistics.html#how-will-what-you-learn-this-week-help-your-research",
    "href": "content/02-descriptive_statistics.html#how-will-what-you-learn-this-week-help-your-research",
    "title": "Descriptive Statistics",
    "section": "How will what you learn this week help your research?",
    "text": "How will what you learn this week help your research?\nYou have an interesting question that you want to explore. You have some data that relate to that question. Included in these data are information on your outcome of interest and information on the things that you think determine or shape that outcome. You think that one (or more) of the drivers is particularly important, but no one has yet written about it or proven its importance. Brilliant! What do you do now?\nThe first step in any empirical analysis is getting to know your data. I mean, really getting to know your data. You want to dig into it with a critical eye. You want to understand any patterns lurking beneath the surface.\nUltimately, you want to get a really good understanding of the data generation process. This process can be thought of in two different and important ways. First, you want to understand how, out there in the real world, your outcome and drivers come to be. For example, if you are interested in voting patterns, you want to know the nitty gritty process of how people actually vote. Do they have to travel long distances, stand in long queues, fill out a lot of paperwork? Are there age restrictions on their ability to vote? Are there more insidious restrictions that might suppress voting for one particular group in the electorate?\nYou can use the skills we will discuss in this section to help you answer these questions. For example, you can determine whether there are relatively few young voters compared to older voters. If so, why? In turn, your growing expertise in and understanding of the data generation process should inform your exploration of the data. You might note that people have to wait in long queues on a Tuesday to vote. Does this impact the number of workers vs. retirees who vote?\nNow, this is made slightly more tricky by the second part of this process. You need to understand how your variables are actually measured. How do we know who turns out to vote? Did you get access to the voter file, which records each individual who voted and some interesting and potentially relevant demographic information about them? Or are you relying on exit polls, that only include a portion of those who voted? Were the people included in the polls reflective of the total voting population? What or whom is missing from this survey? Of course, if your sample is not representative, you might find some patterns that appear to be very important to your outcome of interest but are, in fact, just an artifact of a poorly drawn sample. If your survey failed to get responses from young people, you may be led to falsely believe that young people don’t vote.\nThis week you will be introduced to the first part of the data analysis process: data exploration. We use descriptive statistics to describe patterns in our data. These are incredibly powerful tools that will arm you with an intimate knowledge of the shape of your variables of interest. With this knowledge, you will be able to start to answer your important question and potentially identify new ones. You will also be able to sense-check your more complex models and pick up on odd or incorrect relationships that they may find.\nAs you make your frequency tables and histograms and very elaborate dot plots and box charts, keep in mind that these tools are useful for your interrogation of the data generation process. Be critical. Continue to ask whether your data allow you to detect true relationships between your variables of interest. Build your intuition for what is really going on and what factors are really driving your outcome of interest.\nLet’s get started."
  },
  {
    "objectID": "content/02-descriptive_statistics.html#describing-your-data",
    "href": "content/02-descriptive_statistics.html#describing-your-data",
    "title": "Descriptive Statistics",
    "section": "Describing your data",
    "text": "Describing your data\nBroadly, there are two types of variables: categorical and continuous variables.\nCategorical variables are discrete. They can be unordered (nominal) - for example, the different colours of cars - or ordered (ordinal) - for example, whether you strongly dislike, dislike, are neutral about, like, or strongly like Taylor Swift.\n\n\n\n\n\n\nNote\n\n\n\nDichotomous (or binary) variables are a special type of categorical variable. They take on one of two values. For example: yes or no; at war or not at war; is a Swifty, or is not a Swifty.\n\n\nContinuous variables are, well, continuous. For example, your height or weight, a country’s GDP or population, or the number of fatalities in a battle.\n\n\n\n\n\n\nNote\n\n\n\nContinuous variables can be made into (usually ordered) categorical variables. This process is called binning. For example, you can take individuals’ ages and reduce them to 0 - 18 years old, 18 - 45 years old, 45 - 65 years old, and 65+ years old.\nYou lose information in this process: you cannot go from 45 - 65 years old back to the individuals’ precise age. In other words, you cannot go from a categorical to continuous variable.\n\n\nLet’s take a look at how you can describe these different types of variables using real-world political science examples."
  },
  {
    "objectID": "content/02-descriptive_statistics.html#describing-categorical-variables",
    "href": "content/02-descriptive_statistics.html#describing-categorical-variables",
    "title": "Descriptive Statistics",
    "section": "Describing categorical variables",
    "text": "Describing categorical variables\nGenerally, we can get a good sense of a categorical variable by looking at counts or proportions. For example, which category contains the most number of observations? Which contains the least?\n\n\n\n\n\n\nNote\n\n\n\nLater, we will ask interesting questions using these summaries. These include whether differences between the counts and/or percentages of cases that fall into each category are meaningfully (and/or statistically significantly) different from one another. This deceptively simple question serves as the foundation for a lot of political science research.\n\n\nLet’s use the American National Election Survey to explore how to produce useful descriptive statistics for categorical variables using R. The ANES surveys individual US voters prior to and just following US Presidential Elections. It surveys them about their political beliefs and behavior.\nWe can access the latest survey (from the 2020 Presidential Election) using the polisciols package:\n\npolisciols::nes\n\n\n\n\n\n\n\nExercise\n\n\n\nTake a look at the different pieces of information collected about each respondent by running ?nes in your console.\n\n\nLet’s look at US voters’ views on income inequality in the US. Specifically, we will look at whether individuals think the difference in incomes between rich people and poor people in the United States today is larger, smaller, or about the same as it was 20 years ago.\nRespondents could provide one of four answers (or refuse to answer the question):\n\ndistinct(nes, income_gap)\n\n# A tibble: 5 × 1\n  income_gap    \n  &lt;ord&gt;         \n1 About the same\n2 Larger        \n3 Smaller       \n4 &lt;NA&gt;          \n5 Don't know    \n\n\nThis is an ordinal categorical variable. It is discrete and ordered. We can take a look at the variable itself using the helpful skimr::skim() function:\n\nskim(nes$income_gap)\n\n\nData summary\n\n\nName\nnes$income_gap\n\n\nNumber of rows\n8280\n\n\nNumber of columns\n1\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\ndata\n54\n0.99\nTRUE\n4\nLar: 6117, Abo: 1683, Sma: 416, Don: 10\n\n\n\n\nFrom this, we learn that:\n\nThe variable type is a factor (see the R tip below)\nWe are missing 54 observations (in other words, 54 people did not answer the question)\nThis means that we have information on 99% of our observations (from complete_rate).\n\n\n\n\n\n\n\nTip\n\n\n\nRemember, there are many different types of data that R recognizes. These include characters (\"A\", \"B\", \"C\"), integers (1, 2, 3), and logical values (TRUE or FALSE). R treats categorical variables as factors.\n\n\nFrequency distribution\nWe can take advantage of janitor::tabyl() to quickly calculate the number and proportion of respondents in each age bracket.\n\ntabyl(nes, income_gap)\n\n     income_gap    n     percent valid_percent\n     Don't know   10 0.001207729   0.001215658\n        Smaller  416 0.050241546   0.050571359\n About the same 1683 0.203260870   0.204595186\n         Larger 6117 0.738768116   0.743617797\n           &lt;NA&gt;   54 0.006521739            NA\n\n\n\n\n\n\n\n\nTip\n\n\n\nvalid_percent provides the proportion of respondents who provided each answer with missing values removed from the denominator. For example, the ANES surveyed 8,280 respondents in 2020, but only 8,226 of them answered this question.\n6,117 responded that they believe the income gap is larger today than it was 20 years ago. Therefore, the Larger proportion (which is bounded by 0 and 1, whereas percentages are bounded by 0 and 100) is 6,117 / 8,280 and its valid proportion is 6117 / 8,226.\n\n\nVisualizing this frequency\nIt is a bit difficult to quickly determine relative counts. Which was the most popular answer? Which was the least? Are these counts very different from each other?\nVisualizing your data will give you a much better sense of it. I recommend using a bar chart to show clearly relative counts.\n\nggplot(nes, aes(y = income_gap)) + \n  geom_bar() +\n  theme_minimal() + \n  theme(plot.title = element_text(face = \"bold\"),\n        plot.title.position = \"plot\") + \n  labs(\n    title = \"Do you think the difference in incomes between rich people and poor people in the United States today is larger, \\nsmaller, or about the same as it was 20 years ago?\", \n    x = \"Count of respondents\",\n    y = NULL,\n    caption = \"Source: ANES 2020 Survey\"\n  ) + \n  scale_x_continuous(labels = scales::label_comma())\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\ngeom_bar() automatically counts the number of observations in each category.\n\n\nFrom this plot we quickly learn that a large majority of respondents believe that the income gap has grown over the last 20 years. Very few people believe it has shrunk."
  },
  {
    "objectID": "content/02-descriptive_statistics.html#describing-continuous-variables",
    "href": "content/02-descriptive_statistics.html#describing-continuous-variables",
    "title": "Descriptive Statistics",
    "section": "Describing continuous variables",
    "text": "Describing continuous variables\nWe need to treat continuous variables differently from categorical ones. Continuous variables cannot meaningfully be bound together and compared. For example, imagine making a frequency table or bar chart that counts the number of countries with each observed GDP. You would have 193 different counts of one. Not very helpful!\nWe can get a much better sense of our continuous variables by looking at how they are distributed across the range of all possible values they could take on. Phew! Let’s make sense of this using some real-world data.\nFor this section, we will look at how much each country spends on education as a proportion of its gross domestic product (GDP). We will use wbstats::wb_data() to collect these data.\n\nperc_edu &lt;- wb_data(\n  \"SE.XPD.TOTL.GD.ZS\", start_date = 2020, end_date = 2020, return_wide = F\n) |&gt; \n  transmute(\n    country, \n    region = countrycode(country, \"country.name\", \"region\"),\n    year = date,\n    value\n  )\n\nperc_edu\n\n# A tibble: 217 × 4\n   country             region                      year value\n   &lt;chr&gt;               &lt;chr&gt;                      &lt;dbl&gt; &lt;dbl&gt;\n 1 Afghanistan         South Asia                  2020 NA   \n 2 Albania             Europe & Central Asia       2020  3.34\n 3 Algeria             Middle East & North Africa  2020  7.04\n 4 American Samoa      East Asia & Pacific         2020 NA   \n 5 Andorra             Europe & Central Asia       2020  2.86\n 6 Angola              Sub-Saharan Africa          2020  2.74\n 7 Antigua and Barbuda Latin America & Caribbean   2020  2.99\n 8 Argentina           Latin America & Caribbean   2020  5.28\n 9 Armenia             Europe & Central Asia       2020  2.71\n10 Aruba               Latin America & Caribbean   2020 NA   \n# ℹ 207 more rows\n\n\n\n\n\n\n\n\nNote\n\n\n\nI have added each country’s region (using countrycode::countrycode()) so that we can explore regional trends in our data.\n\n\nWe can get a good sense of how expenditure varied by country by looking at the center, spread, and shape of the distribution.\nVisualizing continuous distributions\nFirst, let’s plot each country’s spending to see how they relate to one another. There are two plot types commonly used for this: histograms and density curves.\nHistograms\nA histogram creates buckets along the range of values our variable can take (i.e. buckets of 10 between 1 and 100 would include 1 - 10, 11 - 20, 21 - 30). It then counts the number of observations that fall into each of those buckets and plots that count.\nLet’s plot our data as a histogram with a bin width of 1 percentage point:\n\nggplot(perc_edu, aes(x = value)) + \n  geom_histogram(binwidth = 1) + \n  theme_minimal() + \n  labs(\n    x = \"Expenditure on education as a proportion of GDP\",\n    y = \"Number of countries\"\n  )\n\n\n\n\nFrom this we learn that most countries spend between three to five percent of their GDP on education. There appears to be an outlier: a country that spends around 10 percent of its GDP on education.\nIf we pick a narrower bin width, we will see more fine-grained detail about the distribution of our data:\n\nggplot(perc_edu, aes(x = value)) + \n  geom_histogram(binwidth = 0.25) + \n  theme_minimal() + \n  labs(\n    x = \"Expenditure on education as a proportion of GDP\",\n    y = \"Number of countries\"\n  )\n\n\n\n\nFrom this we learn that there most countries spend around four percent of their GDP on education. There is a small cluster of countries that spend between around 7.5 to nine percent on these services.\nDensity curves\nDensity curves also communicate the distribution of continuous variables. They plot the density of the data that fall at a given value on the x-axis.\nLet’s plot our data using a density plot:\n\nggplot(perc_edu, aes(x = value)) + \n  geom_density() + \n  theme_minimal() + \n  labs(\n    x = \"Expenditure on education as a proportion of GDP\",\n    y = \"Density\"\n  )\n\n\n\n\nThis provides us with the same information above, but highlights the broader shape of our distribution. We again learn that most countries spend around four percent of their GDP on education. There are some that spend above 7.5 percent.\nUnderstanding distributions\nWe can use the shape of a variable’s distribution to compare our variables of interest to other variables. Is the distribution symmetric or skewed? Where are the majority of observations clustered? Are there multiple distinct clusters, or high points, in the distribution?\nThere are three broad distributions that you should know: normal, right-skewed, and left-skewed. People use these terms to summarize the shape of their continuous data.\nNormal distribution\nA normally distributed variable includes values that fall symmetrically away from their center point, which is the peak (or most common value).\nExamples of normally distributed data include the height or weight of all individuals in a large population.\n\n\n\n\n\n\nNote\n\n\n\nThis distribution is also referred to as a bell-curve.\n\n\n\n\n\n\n\nRight-skewed distribution\nWith right-skewed data, the majority of data have small values with a small number of larger values.\nExamples of right-skewed data include countries’ GDP.\n\n\n\n\n\nLeft-skewed distribution\nWith left-skewed data, the majority of data have large values with a small number of small values.\nExamples of left-skewed data include democracies’ election turn-out rates.\n\n\n\n\n\nMeasures of central tendency: mean, median, and mode\nWe can also use measures of central tendency to quickly describe and compare our variables.\nMean\nThe mean is the average of all values. Formally:\n\\[\n\\bar{x} = \\frac{\\Sigma x_i}{n}\n\\]\nIn other words, add all of your values together and then divide that total by the number of values you have.\nIn R:\n\nmean(perc_edu$value, na.rm = T)\n\n[1] 4.571329\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you do not use the argument na.rm (read “NA remove!”), you will get an NA if any exist in your vector of values. This is a good default! You should be very aware of missing data points.\n\n\nOn average, countries spent 4.57% of their GDP on education in 2020.\nMedian\nThe median is the mid-point of all values.\nTo calculate it, put all of your values in order from smallest to largest. Identify the value in the middle. That’s your median.\nIn R:\n\nmedian(perc_edu$value, na.rm = T)\n\n[1] 4.50287\n\n\nThe median country spent 4.50% of their GDP on education in 2020.\nMode\nThe mode is the most frequent of all values.\nTo calculate it, count how many times each value occurs in your data set. The one that occurs the most is your mode.\n\n\n\n\n\n\nNote\n\n\n\nThis is usually a more useful summary statistic for categorical variables than continuous ones. For example, which colour of car is most popular? Which political party has the most members?\n\n\nIn R:\n\nx &lt;- c(1, 1, 2, 4, 5, 32, 5, 1, 10, 3, 4, 6, 10)\n\ntable(x)\n\nx\n 1  2  3  4  5  6 10 32 \n 3  1  1  2  2  1  2  1 \n\n\nUsing central tendency to describe and understand distributions\nNormally distributed values have the same mean and median.\n\n\n\n\n\nFor right skewed data, the mean is greater than the median.\n\n\n\n\n\nFor left skewed data, the mean is smaller than the median.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhen do we care about the mean or the median? There is no simple answer to this question. Both of these values are useful summaries of our continuous data. By default, we use the average to describe our data in statistical analysis. As you will learn, most regression models are, fundamentally, just fancy averages of our data. However, this default is not always sensible.\nAs you may have noted above, the average value is more sensitive to extreme values. If you have one very large or very small number in your vector of numbers, your average will be pulled well away from your mid-point (or median). This can lead you astray. To illustrate, let’s look at the average and median of the numbers between one and 10:\n\nx &lt;- 1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\nmean(x)\n\n[1] 5.5\n\nmedian(x)\n\n[1] 5.5\n\n\nIf we add one very large number to our vector, our average will shoot up but our median will only move up one additional number in our collection:\n\nx &lt;- c(x, 1000)\nx\n\n [1]    1    2    3    4    5    6    7    8    9   10 1000\n\n\n\nmean(x)\n\n[1] 95.90909\n\nmedian(x)\n\n[1] 6\n\n\nWhich number better summarizes our data? Here, I would suggest that the average is misleading. That one 1,000 data point is doing a lot of the work. The median better describes the majority of my data.\nWe will talk more about this (and outliers more specifically) throughout the semester.\n\n\nFive number summary\nAs you can see, we are attempting to summarize our continuous data to give us a meaningful but manageable sense of it. Means and medians are useful for continuous data.\nWe can provide more context to our understanding using more summary statistics. A common approach is the five number summary. This includes:\n\nThe smallest value;\nThe 25th percentile value, or the median of the lower half of the data;\nThe median;\nThe 75th percentile value, or the median of the upper half of the data;\nThe largest value.\n\nWe can use skimr::skim() to quickly get useful information about our continuous variable.\n\nskim(perc_edu$value)\n\n\nData summary\n\n\nName\nperc_edu$value\n\n\nNumber of rows\n217\n\n\nNumber of columns\n1\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\ndata\n58\n0.73\n4.57\n1.66\n1.37\n3.34\n4.5\n5.54\n10.54\n▃▇▅▁▁\n\n\n\n\nWe have 217 rows (because our unit of observation is a country, we can read this as 217 countries2). We are missing education spending values for 58 of those countries (see n_missing), giving us a complete rate of 73% (see complete_rate).\nThe country that spent the least on education as a percent of its GDP in 2020 was Haiti, which spent 1.4% (see p0). The country that spent the most was the Micronesia, Fed. Sts., which spent 10.5% (see p100). The average percent of GDP spent on education in 2020 was 4.6% (see mean) and the median was 4.5% (see p50).\nThis description was a bit unwieldy. As usual, to get a better sense of our data we should visualize it.\nBox plots\nBox plots (sometimes referred to as box and whisker plots) visualize the five number summary (with bonus features) nicely.\n\nggplot(perc_edu, aes(x = value)) + \n  geom_boxplot() + \n  theme_minimal() + \n  theme(\n    axis.text.y = element_blank()\n  ) + \n  labs(\n    x = \"Expenditure on education as a proportion of GDP\",\n    y = NULL\n  )\n\n\n\n\nThe box in the graph above displays the 25th percentile, the median, and the 75th percentile values. The tails show you all the data up to a range 1.5 times the interquartile range (IQR), or the 75th percentile minus the 25th percentile (or the upper edge of the box minus the lower edge of the box). If the smallest or largest values fall below or above (respectively) 1.5 times the IQR, the tail ends at that value. The remaining data points (if they exist) are displayed as dots shooting away from the whiskers of our box and whisker plot.\nOutliers\nNote that some countries’ expenditure are displayed as dots. The box plot above is providing you with a bit more information than the five number summary alone. If the data include values that fall outside of the IQR, they are displayed as dots. These are (very rule of thumb, take with a grain of salt, please rely on your theory and data generation process instead!) candidates for outliers.\nOutliers fall so far away from the majority of the other values that they should be examined closely and perhaps excluded from your analysis. As discussed above, they can distort your mean. They do not, however, distort your median.\n\n\n\n\n\n\nNote\n\n\n\nWe will talk more about how to deal with outliers later in the course.\n\n\nMeasures of spread: range, variance, and standard deviation\nWe now have a good sense of some of the features of our data. Another useful thing to know is the shape of the distribution. Here, measures of spread are useful.\nRange\nThe range is the difference between the largest and smallest value.\n\\[\nrange = max - min\n\\]\n\nmax(perc_edu$value, na.rm = T) - min(perc_edu$value, na.rm = T)\n\n[1] 9.168\n\n\nThe difference between the country that spends the highest proportion of its GDP on education and that which spends the least is 9.17 percentage points.\nVariance\nThe variance measures how spread out your values are. On average, how far are your observations from the mean?\nThis measure can, at first, be a bit too abstract to get an immediate handle on. Let’s walk through it. Imagine we have two data sets, wide_dist and narrow_dist. Both are normally distributed, share the same mean (0), and the same number of observations (1,000,000).\n\nwide_dist\n\n# A tibble: 1,000,000 × 1\n        x\n    &lt;dbl&gt;\n 1  0.316\n 2 -0.505\n 3 -4.78 \n 4 -2.85 \n 5 -0.577\n 6 -2.83 \n 7 -1.83 \n 8  0.900\n 9 -1.63 \n10 -0.232\n# ℹ 999,990 more rows\n\n\n\nnarrow_dist\n\n# A tibble: 1,000,000 × 1\n        x\n    &lt;dbl&gt;\n 1  0.532\n 2 -0.942\n 3 -0.667\n 4 -0.731\n 5  0.513\n 6  1.03 \n 7 -1.22 \n 8  0.827\n 9  0.167\n10  0.114\n# ℹ 999,990 more rows\n\n\nLet’s plot them:\n\n\n\n\n\nDespite both having the same center point and number of observations, the data are much more spread out around that center point in the top graph (of wide_dist).\nThe data in the top graph have higher variance (are more spread out) than those in the bottom graph. We measure this by calculating the average of the squares of the deviations of the observations from their mean.\n\\[\ns^2 = \\frac{\\Sigma(x_i - \\bar{x})^2}{n - 1}\n\\]\nLet’s step through this. We will first calculate the variance of wide_dist. To do this:\n\nCalculate the mean of your values.\nCalculate the difference between each individual value and that mean (how far from the mean is every value?).\nSquare those differences.\n\n\n\n\n\n\n\nTip\n\n\n\nWe do not care whether the value is higher or lower than the mean. We only care how far from the mean it is. Squaring a value removes its sign (positive or negative). Remember, if you multiply a negative number by a negative number, you get a positive number. This allows us to concentrate on the difference between each individual data point and the mean.\n\n\n\nAdd all of those squared differences to get a single number.\nDivide that single number by the number of observations you have minus 1.\n\nYou now have your variance!\nIn R:\n\nwide_dist_mean &lt;- mean(wide_dist$x)\n\nwide_var_calc &lt;- wide_dist |&gt; \n  mutate(\n    mean = wide_dist_mean,\n    diff = x - mean,\n    diff_2 = diff^2\n  )\n\nwide_var_calc\n\n# A tibble: 1,000,000 × 4\n        x      mean   diff  diff_2\n    &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1  0.316 -0.000537  0.316  0.100 \n 2 -0.505 -0.000537 -0.505  0.255 \n 3 -4.78  -0.000537 -4.78  22.8   \n 4 -2.85  -0.000537 -2.85   8.10  \n 5 -0.577 -0.000537 -0.576  0.332 \n 6 -2.83  -0.000537 -2.83   8.00  \n 7 -1.83  -0.000537 -1.83   3.36  \n 8  0.900 -0.000537  0.900  0.811 \n 9 -1.63  -0.000537 -1.63   2.65  \n10 -0.232 -0.000537 -0.232  0.0536\n# ℹ 999,990 more rows\n\n\nWe the add those squared differences between each observation and the mean of our whole sample together. Finally, we divide that by one less than our number of observations.\n\nwide_var &lt;- sum(wide_var_calc$diff_2) / (nrow(wide_var_calc) - 1)\n\nwide_var\n\n[1] 4.007193\n\n\nWe can compare this to the variance for our narrower distribution.\n\nnarrow_var_calc &lt;- narrow_dist |&gt; \n  mutate(\n    mean = mean(narrow_dist$x),\n    diff = x - mean,\n    diff_2 = diff^2\n  )\n\nnarrow_var &lt;- sum(narrow_var_calc$diff_2) / (nrow(narrow_var_calc) - 1)\n\nnarrow_var\n\n[1] 1.000488\n\n\nIt is, in fact, smaller!\nThat was painful. Happily we can use var() to do this in one step:\n\nvar(wide_dist)\n\n         x\nx 4.007193\n\n\n\nvar(narrow_dist)\n\n         x\nx 1.000488\n\n\n\nvar(wide_dist) &gt; var(narrow_dist)\n\n     x\nx TRUE\n\n\nOn average, countries spent 2.75% more or less than the average of 4.57% of their GDP on education in 2020.\nStandard deviation\nA simpler measure of spread is the standard deviation. It is simply the square root of the variance.\n\nsqrt(wide_var)\n\n[1] 2.001797\n\n\n\nsqrt(narrow_var)\n\n[1] 1.000244\n\n\nYou can get this directly using sd():\n\nsd(wide_dist$x)\n\n[1] 2.001797\n\n\n\nsd(narrow_dist$x)\n\n[1] 1.000244\n\n\nThe standard deviation of all countries’ percentage of their GDP that they spent on education in 2020 was 1.66%. This horrible sentence demonstrates that standard deviations are most usefully employed in contexts other than attempts to better understand your variables of interest. They are very important for determining how certain we can be about the relationships between different variables we uncover using statistical models (which we will get to later in the semester)."
  },
  {
    "objectID": "content/02-descriptive_statistics.html#conclusion",
    "href": "content/02-descriptive_statistics.html#conclusion",
    "title": "Descriptive Statistics",
    "section": "Conclusion",
    "text": "Conclusion\nYour empirical analysis is only as strong as its foundation. You can use the tools you learnt this week to build a very strong foundation. Always start any analysis by getting a very strong sense of your data. Look at it with a critical eye. Does it match your intuition? Is something off? What can you learn about the peaks and troughs among your observations?"
  },
  {
    "objectID": "content/02-descriptive_statistics.html#footnotes",
    "href": "content/02-descriptive_statistics.html#footnotes",
    "title": "Descriptive Statistics",
    "section": "Footnotes",
    "text": "Footnotes\n\nThe Comprehensive R Archive Network (CRAN) hosts many R packages that can be installed easily using the familiar install.packages() function. These packages have gone through a comprehensive quality assurance process. I wrote polisciols for this class and will update it regularly. I, therefore, will not host it through CRAN: the quality assurance process takes too long to be practical for our weekly schedule. Instead, you are downloading it directly from its Github repository.↩︎\nYou are right: there were not nrow(perc_edu) countries in 2020. The World Bank collects data on some countries that are not members of the UN (and would not, traditionally, be considered to be countries).↩︎"
  },
  {
    "objectID": "content/08_hypothesis_testing.html",
    "href": "content/08_hypothesis_testing.html",
    "title": "Hypothesis Testing II",
    "section": "",
    "text": "Pollock & Edwards, Chapter 7\n\n\n\n Pollock & Edwards R Companion, Chapter 7"
  },
  {
    "objectID": "content/08_hypothesis_testing.html#readings",
    "href": "content/08_hypothesis_testing.html#readings",
    "title": "Hypothesis Testing II",
    "section": "",
    "text": "Pollock & Edwards, Chapter 7\n\n\n\n Pollock & Edwards R Companion, Chapter 7"
  },
  {
    "objectID": "content/08_hypothesis_testing.html#section",
    "href": "content/08_hypothesis_testing.html#section",
    "title": "Hypothesis Testing II",
    "section": "Section",
    "text": "Section\n\nPrerequisites\n\ninstall.packages(\"infer\")\n\n\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(wbstats)\nlibrary(countrycode)\nlibrary(janitor)\nlibrary(broom)\nlibrary(infer)\nlibrary(scales)\n\nset.seed(1234)\n\n\n\nDifference of means\nDo richer countries win more medals in the summer Olympics than poorer countries, on average? To answer this question, we need the following data:\n\nNumber of medals countries have won in the summer Olympics,\nCountries’ GDP for those corresponding years,\nSome threshold by which we will determine whether a country has a high income or not.\n\nLet’s collect those data. To start, we can get data on all athletes who have competed in the Olympics from 1896 to 2016.\n\nolympics_df &lt;- read_csv(\"https://raw.githubusercontent.com/cosmoduende/r-olympic-games/main/datasets/athleteEvents.csv\")\nolympics_df\n\n# A tibble: 271,116 × 15\n      ID Name     Sex     Age Height Weight Team  NOC   Games  Year Season City \n   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;\n 1     1 A Dijia… M        24    180     80 China CHN   1992…  1992 Summer Barc…\n 2     2 A Lamusi M        23    170     60 China CHN   2012…  2012 Summer Lond…\n 3     3 Gunnar … M        24     NA     NA Denm… DEN   1920…  1920 Summer Antw…\n 4     4 Edgar L… M        34     NA     NA Denm… DEN   1900…  1900 Summer Paris\n 5     5 Christi… F        21    185     82 Neth… NED   1988…  1988 Winter Calg…\n 6     5 Christi… F        21    185     82 Neth… NED   1988…  1988 Winter Calg…\n 7     5 Christi… F        25    185     82 Neth… NED   1992…  1992 Winter Albe…\n 8     5 Christi… F        25    185     82 Neth… NED   1992…  1992 Winter Albe…\n 9     5 Christi… F        27    185     82 Neth… NED   1994…  1994 Winter Lill…\n10     5 Christi… F        27    185     82 Neth… NED   1994…  1994 Winter Lill…\n# ℹ 271,106 more rows\n# ℹ 3 more variables: Sport &lt;chr&gt;, Event &lt;chr&gt;, Medal &lt;chr&gt;\n\n\nLet’s clean that up a bit:\n\nolympic_medals_df &lt;- olympics_df |&gt; \n  # Only look at the most recent year (2016)\n  slice_max(Year) |&gt; \n  # Include more general country code\n  mutate(iso3c = countrycode(Team, \"country.name\", \"iso3c\")) |&gt; \n  # Filter out individuals who didn't win a medal\n  drop_na(Medal) |&gt; \n  # Count the number of medals won by each country (team) that year\n  count(iso3c, name = \"medals_won\")\n\nolympic_medals_df\n\n# A tibble: 85 × 2\n   iso3c medals_won\n   &lt;chr&gt;      &lt;int&gt;\n 1 ARE            1\n 2 ARG           22\n 3 ARM            4\n 4 AUS           82\n 5 AUT            2\n 6 AZE           18\n 7 BDI            1\n 8 BEL           21\n 9 BGR            7\n10 BHR            2\n# ℹ 75 more rows\n\n\nWe now have a count of all medals won by each country that competed in the 2016 Summer Olympics. Let’s have a look at the shape of those data:\n\nskim(olympic_medals_df)\n\n\nData summary\n\n\nName\nolympic_medals_df\n\n\nNumber of rows\n85\n\n\nNumber of columns\n2\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\niso3c\n1\n0.99\n3\n3\n0\n84\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nmedals_won\n0\n1\n23.8\n41.61\n1\n2\n8\n22\n264\n▇▁▁▁▁\n\n\n\n\n\n85 countries competed in the 2016 Summer Olympics. On average, they won 23.8 medals. Every country won at least one medal. The US won the most medals, with an astonishing 264 medals. These data are heavily skewed to the right, with the majority of countries winning a small number of medals.\n\nggplot(olympic_medals_df, aes(x = medals_won)) +\n  geom_histogram() + \n  theme_minimal() + \n  labs(x = \"Number of medals won\",\n       y = \"Count of countries\")\n\n\n\n\nNow we need data on each country’s GDP that year:\n\ngdp_df &lt;- wb_data(\"NY.GDP.MKTP.CD\", start_date = 2016, end_date = 2016, return_wide = F) |&gt;\n  select(iso3c, gdp = value)\n\ngdp_df\n\n# A tibble: 217 × 2\n   iso3c           gdp\n   &lt;chr&gt;         &lt;dbl&gt;\n 1 AFG    18116572395.\n 2 ALB    11861199831.\n 3 DZA   180763839522.\n 4 ASM      671000000 \n 5 AND     2896610480.\n 6 AGO    52761617226.\n 7 ATG     1489692593.\n 8 ARG   557532320663.\n 9 ARM    10546136236.\n10 ABW     2983635196.\n# ℹ 207 more rows\n\n\nLet’s take a look at that:\n\nggplot(gdp_df, aes(x = gdp / 1e9)) + \n  geom_histogram() + \n  scale_x_continuous(label = label_dollar()) + \n  labs(x = \"Current GDP (billions of US dollars)\",\n       y = \"Count\") +\n  theme_minimal()\n\n\n\n\nWe need to define a threshold for national income beyond which we consider a country to be rich. Let’s use the 75th percentile:\n\ngdp_75th &lt;- skim(gdp_df, gdp) |&gt; \n  pull(numeric.p75)\n\ngdp_df &lt;- gdp_df |&gt; \n  mutate(income_level = if_else(gdp &lt; gdp_75th, \"Medium or low\", \"High\"),\n         income_level = factor(income_level, ordered = T, levels = c(\"Medium or low\", \"High\")))\n\nLet’s join these together to get our full data set:\n\nmedals_gdp_df &lt;- olympic_medals_df |&gt; \n  left_join(gdp_df, by = \"iso3c\") |&gt; \n  drop_na(income_level)\n\nmedals_gdp_df\n\n# A tibble: 81 × 4\n   iso3c medals_won     gdp income_level \n   &lt;chr&gt;      &lt;int&gt;   &lt;dbl&gt; &lt;ord&gt;        \n 1 ARE            1 3.69e11 High         \n 2 ARG           22 5.58e11 High         \n 3 ARM            4 1.05e10 Medium or low\n 4 AUS           82 1.21e12 High         \n 5 AUT            2 3.96e11 High         \n 6 AZE           18 3.79e10 Medium or low\n 7 BDI            1 2.64e 9 Medium or low\n 8 BEL           21 4.76e11 High         \n 9 BGR            7 5.40e10 Medium or low\n10 BHR            2 3.23e10 Medium or low\n# ℹ 71 more rows\n\n\nDid high-income countries win more medals at the 2016 Summer Olympics than medium- or low-income countries?\n\nggplot(medals_gdp_df, aes(x = gdp / 1e9, y = medals_won)) + \n  geom_point() + \n  theme_minimal() + \n  labs(x = \"Current GDP (billions of US dollars)\",\n       y = \"Number of medals won\")\n\n\n\n\n\nggplot(medals_gdp_df, aes(x = medals_won, y = income_level, fill = income_level)) + \n  geom_boxplot() + \n  theme_minimal() +\n  theme(legend.position = \"none\") + \n  scale_fill_discrete(type = c(\"#D88C9A\", \"#8bc5ed\"))\n\n\n\n\n\nmedals_avg &lt;- medals_gdp_df |&gt; \n  group_by(income_level) |&gt; \n  summarise(n_countries = n(), \n            avg_medals = mean(medals_won)) |&gt; \n  mutate(diff_means = avg_medals - lead(avg_medals))\n\nmedals_avg\n\n# A tibble: 2 × 4\n  income_level  n_countries avg_medals diff_means\n  &lt;ord&gt;               &lt;int&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 Medium or low          35       9.06      -27.6\n2 High                   46      36.7        NA  \n\n\nIt certainly looks like high-income countries won more medals on average rather than medium- and low-income countries. This difference looks pretty substantial, but we still need to confirm that it is not due to random error.\nYou have been introduced to a lot of different ways to answer this question. However, as Professor Allen Downey writes, there really is only one test:\n\nCompute the test statistic that you are interested in testing,\nDefine the null hypothesis, which is a model of the world in which there is no effect,\nUse that model of the null hypothesis to generate many simulated samples pulled from your null world,\nFor each simulated sample, compute the test statistic that you are interested in testing,\nWork out the proportion of times those test statistics calculated in step 4 exceed the test statistic from your sample (calculated at step 1). If this proportion is very small, it is very unlikely that you would manage to get a sample that produced the test statistic you found in step 1 if, in fact, there was no effect.\n\nThis process is the broad foundation of all frequentist null hypothesis significance tests that you will come across. Let’s use it to answer our question about income and Olympic medal tallies.\n\nStep 1: Compute the test statistic from your sample\nWe are interested in determining whether high-income countries won more models on average than medium- and low-income countries. Therefore, we are interested in the difference in the average number of medals won by high-income countries and medium- and low-income countries.\nHappily, we worked this out above:\n\ndiff_means &lt;- medals_avg |&gt; \n  drop_na(diff_means) |&gt; \n  pull(diff_means)\n\ndiff_means\n\n[1] -27.63851\n\n\nOn average, high-income countries won 27.6 more medals than medium- and low-income countries.\n\n\nStep 2: Define the null hypothesis\nOur null hypothesis is that countries with these two different income levels won the same number of medals, on average. More succinctly: there is no difference of means. There are a couple of ways that we can build a model that captures this null effect (pulled from another great post from Professor Allen Downey):\nParametric: Merge the groups. Work out the shape of the pooled data: the center and spread. Then generate random samples from that distribution. This will give you a modeled world in which there is no difference between these two groups.\nResampling: Similar to the parametric approach, merge the groups. Draw samples (with replacement) from this merged group.\nPermutation: Assign elements in the sample your different categories of interest at random. This is similar to resampling without replacement.\nWe are going to use permutation to generate our data sets of worlds in which there is no difference in the average number of medals won by high-income countries and by medium- and low-income countries.\n\n\n\n\n\n\nNote\n\n\n\nThe broad point to take from this is that there are many different ways to create and think about the null hypothesis. Many of the tests you will use to determine statistical significance bury the decision they make about how to model the null hypothesis. It can be useful to bring that decision to the surface. This will allow you to control what is going on. Simulation-based hypothesis testing requires that you do this explicitly. More traditional tests, including the t-test, are doing this in the background.\n\n\n\n\nStep 3: Generate many simulated data sets from the null hypothesis model\nWe are now going to draw 5,000 different random samples from our modeled null world. We are going to use various functions from the fantastic infer package to do this.\n\nincome_null &lt;- medals_gdp_df |&gt;  \n  # Set up our DV and IV\n  specify(medals_won ~ income_level) |&gt;  \n  # Define our null hypothesis\n  hypothesize(null = \"independence\") |&gt;\n  # Generate 5000 simulated samples from that null world using permutation\n  generate(reps = 5000, type = \"permute\")\n\nincome_null\n\nResponse: medals_won (numeric)\nExplanatory: income_level (factor)\nNull Hypothesis: independence\n# A tibble: 405,000 × 3\n# Groups:   replicate [5,000]\n   medals_won income_level  replicate\n        &lt;dbl&gt; &lt;fct&gt;             &lt;int&gt;\n 1          8 High                  1\n 2          2 High                  1\n 3         41 Medium or low         1\n 4          7 High                  1\n 5          2 High                  1\n 6          4 Medium or low         1\n 7        113 Medium or low         1\n 8         82 High                  1\n 9          4 Medium or low         1\n10         69 Medium or low         1\n# ℹ 404,990 more rows\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe specify our preferred method for modelling the null world (permutation) using the type = \"permute\" argument in the generate() function.\n\n\nLet’s take a look at some of those samples:\n\nrandom_5 &lt;- sample(1:5000, 5)\n\nincome_null |&gt; \n  # Select five samples at random\n  filter(replicate %in% random_5) |&gt; \n  # Plot the distribution of the # of medals won by these simulated countries\n  ggplot(aes(x = medals_won, y = factor(replicate), fill = income_level)) + \n  geom_boxplot() + \n  theme_minimal() + \n  labs(x = \"Number of medals won\",\n       y = \"Random sample #\",\n       fill = \"Income level\") + \n  scale_fill_discrete(type = c(\"#D88C9A\", \"#8bc5ed\"))\n\n\n\n\nThese are 5 randomly drawn samples from our null hypothesis world. Cool! You can see that the difference between the number of medals won by high- and low- and middle-income countries is generally small (there is a lot of overlap between the two boxplots drawn for each world). We can compare these distributions to those we observe in the actual number of medals countries won in the 2016 Olympics (from above):\n\nggplot(medals_gdp_df, aes(x = medals_won, y = income_level, fill = income_level)) + \n  geom_boxplot() + \n  theme_minimal() +\n  theme(legend.position = \"none\") + \n  scale_fill_discrete(type = c(\"#D88C9A\", \"#8bc5ed\"))\n\n\n\n\nOur observed data have a much more pronounced difference. However, you can also see that there are some differences in our samples from our null world. The only thing driving those differences is random chance. This is why we need to run these tests. Just like our coin flips or our random samples of US voters discussed in the previous week, random chance can create real differences between our groups. Even if national income has no effect on the number of medals your country is likely to win, there will still be years in which high-income countries win more medals than low- and medium-income countries and other years in which the inverse is the case.\n\n\nStep 4: Compute the test statistic for each of these null worlds\nRemember, we are interested in the difference in the average number of medals won by high-income countries and medium- and low-income countries. In our sample, we found a difference of 27.6 medals.\nWe now need to calculate that difference for each of our samples from our simulated null world. In this world, we would expect there to be no difference, but because of random sampling error we are likely to occasionally see some years in which high-income countries win some more medals than medium- and low-income countries, and some years where the inverse is the case. As you can see from the graph below, our simulated null world reflects this: in most of our random samples of the null world, there is no difference in average medal counts. Other samples have some differences, and very few have large differences.\nWe can calculate this difference using infer::calculate():\n\nincome_diffs_null &lt;- income_null |&gt; \n  calculate(\"diff in means\", order = c(\"Medium or low\", \"High\"))\n\n\nvisualize(income_diffs_null) + \n  labs(x = \"Simulated difference in average ratings (High income − Medium and low income)\", y = \"Count\",\n       title = \"Simulation-based null distribution of differences in means\")\n\n\n\n\nWe now have 5,000 simulated differences in the average number of medals won by high-income countries and medium- and low-income countries for our 5,000 samples from our simulated null world. The only thing creating these differences is random noise.\n\n\nStep 5: Work out whether our sample could plausibly exist in this null world\nWe now have a good idea of the distribution of medals between high-income countries and medium- and low-income countries in a null world in which there was no difference between these groups other than that created by random chance.\nHow likely would we be to see our sample difference of means of 27.6 medals in this null world? Well, first let’s grab our commonly used 95% confidence interval:\n\nboostrapped_confint &lt;- get_confidence_interval(income_diffs_null)\nboostrapped_confint\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1    -17.0     18.6\n\n\n\nvisualize(income_diffs_null) + \n  shade_confidence_interval(boostrapped_confint, color = \"#8bc5ed\", fill = \"#85d9d2\") + \n  labs(x = \"Simulated difference in average ratings (High income − Medium and low income)\", y = \"Count\",\n       title = \"Simulation-based null distribution of differences in means\")\n\n\n\n\nThese boundaries capture 95% of the differences between the number of medals won by high- and medium- and low-income countries in our random samples from our null world. Only 5% of the differences found in our 5,000 samples sit outside that shaded area. This equates to 250 statistics: 125 sitting below -16.97 medals and 125 sitting above 18.65. In other words, in only 2.5% of our samples, medium- and low-income countries won 16.97 or more medals more than high-income countries and in only 2.5% of our samples, high-income countries won 18.65 or more medals more than medium- and low-income countries.\nHow did we get that confidence interval? These are the 2.5th percentile and 97.5th percentile of the simulated statistics:\n\nquantile(income_diffs_null$stat, probs = c(0.025, 0.975))\n\n     2.5%     97.5% \n-16.97267  18.64720 \n\n\nThis makes sense! We want to work out the boundary within which 95% of our data fall. So, we just find that directly. I am treating this as a two tailed interval, so we need to account for the remaining 5% of data on both sides of the distribution so we look at the 2.5% and 97.5% boundaries.\nIf I wanted to apply a directional interval, I would just find the boundary beyond which 5% of the data fall according to my hypothesis:\n\nquantile(income_diffs_null$stat, probs = 0.05)\n\n       5% \n-14.75901 \n\n\n\nvisualize(income_diffs_null) + \n  shade_confidence_interval(quantile(income_diffs_null$stat, probs = c(0.05, 1)), color = \"#8bc5ed\", fill = \"#85d9d2\") + \n  labs(x = \"Simulated difference in average ratings (High income − Medium and low income)\", y = \"Count\",\n       title = \"Simulation-based null distribution of differences in means\")\n\n\n\n\nOur hypothesis is directional: we think that richer countries win more medals than poorer countries. If I observe in my sample that medium- and low-income countries have a much higher number of medals on average, I will need to reject my hypothesis that high-income countries win more medals on average. So the band of observed differences that would cause me to reject my hypothesis moves up.\n\n\n\n\n\n\nNote\n\n\n\nBecause we are working with actual data points, we don’t have to worry about finding a theoretically-derived confidence interval. We just need to find the boundaries within which 95% of those actual data fall. It’s always a good thing when you don’t have to learn a new formula!\n\n\nHow plausible that, given all of this, we would be able to draw a sample from this null world that would have a difference of means of -27.6 medals?\n\nvisualize(income_diffs_null) + \n  shade_confidence_interval(boostrapped_confint, color = \"#8bc5ed\", fill = \"#85d9d2\") + \n  geom_vline(xintercept = diff_means, size = 1, color = \"#77002c\") +\n  labs(x = \"Simulated difference in average ratings (High income − Medium and low income)\", y = \"Count\",\n       title = \"Simulation-based null distribution of differences in means\",\n       subtitle = \"Red line shows observed difference\")\n\n\n\n\nIt is highly unlikely that we would be able to draw a sample from our population and find a difference of 27.6 medals if there were, in fact, no differences between the average number of medals won by high-income countries and medium- and low-income countries. The graph above illustrates this well. Therefore, we can confidently state that we reject the null hypothesis of no difference. How confidently? To answer this we need to ask one more question of our data.\nOur hypothesis is directional: we think that richer countries win more medals than medium- and low-income countries on average. Therefore, we can use a one-tailed test.\n\n\n\n\n\n\nTip\n\n\n\nThe infer::get_p_value() function uses more human- (as opposed to statistician!) friendly language. You can use words like \"less\" or \"greater\" instead of one-, left-, or right-tailed.\n\n\n\nincome_diffs_null |&gt; \n  get_p_value(obs_stat = diff_means, direction = \"less\") |&gt; \n  mutate(p_value_clean = pvalue(p_value))\n\n# A tibble: 1 × 2\n  p_value p_value_clean\n    &lt;dbl&gt; &lt;chr&gt;        \n1  0.0002 &lt;0.001       \n\n\nThis function uses the randomization-based null distribution to calculate the proportion of those simulated statistics that were less than our observed statistic of 27.6 medals. It found none.\nWe can reject that null hypothesis with a very high level of confidence. If there were really no relationship between income levels and medal tallies, the probability that we would see a statistic as or more extreme than 27.6 medals is less than 0.001.\n\n\nTwo sample t-tests\nAbove, we calculated the p-value for our difference of means test by looking at the proportion of the simulated statistics that were more extreme than our observed statistic. A different (more theoretically driven) approach is a t-test.\nThe t-test asks whether the difference of means between two groups represents a true difference or merely a random difference. It produces two values as its output: the t-score and the degrees of freedom.\nThe t-score is the ratio of the difference between the two means and the variation that exists within the samples of the two groups. Higher values of the t-score indicate that a large difference exists between the two sampled groups. Formally:\n\\[\nt = \\frac{(\\bar{x_1} - \\bar{x_2})}{\\sqrt(\\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2})}\n\\]\nWhere \\(\\bar{x_i}\\) is the statistic (for example, mean) calculated from group \\(i\\), \\(s_i\\) is the standard deviation of group \\(i\\), and \\(n_i\\) is the size of group \\(i\\).\nWe can easily perform the t-test in R:\n\nt_test_medals &lt;- t.test(medals_won ~ income_level, data = medals_gdp_df)\nt_test_medals\n\n\n    Welch Two Sample t-test\n\ndata:  medals_won by income_level\nt = -3.4656, df = 50.007, p-value = 0.001095\nalternative hypothesis: true difference in means between group Medium or low and group High is not equal to 0\n95 percent confidence interval:\n -43.65682 -11.62020\nsample estimates:\nmean in group Medium or low          mean in group High \n                   9.057143                   36.695652 \n\n\nThis t-test provides the same answer as our simulation-based test: we are very, very unlikely to be able to pull out a sample from our population that had such a large difference of means if there was, in fact, no difference between these two groups. The p-value from this test - 0.0010953 - is very, very small."
  },
  {
    "objectID": "resources/quarto.html",
    "href": "resources/quarto.html",
    "title": "Introduction to Quarto and markdown",
    "section": "",
    "text": "Quarto is a tool that helps you to create fully reproducible research outputs. It allows you to combine your code, results, and prose in one document. For example, this website - with all of its R code, prose, and visualizations - was created using Quarto.\nYou can use Quarto from RStudio.1 Below is a screen shot of a Quarto document (file extension .qmd) and its HTML output. You can render a Quarto document to many different types of formats, including PDF and MS Word.\nLet’s make a new Quarto document, including some R code and prose."
  },
  {
    "objectID": "resources/quarto.html#a-new-quarto-document",
    "href": "resources/quarto.html#a-new-quarto-document",
    "title": "Introduction to Quarto and markdown",
    "section": "A new Quarto document",
    "text": "A new Quarto document\nOpen up a new Quarto document in RStudio:\n\nFill in the relevant fields:\n\nYour new document will have a .qmd file extension. It will also already contain some text and code. Most of this is demonstrative and can be deleted. However, the top section is very important and should be kept. This section (written in YAML) includes the metadata for your document. By default, it includes the title, author, format in which it will rendered, and the default RStudio editor.\n\n\n\n\n\n\nExercise\n\n\n\nSwitch your output (format) from HTML to PDF by changing html to pdf.\n\n\n\n\n\n\n\n\nTip\n\n\n\nA full list of the formats to which you can render your Quarto document is provided here.\n\n\n\nThere are two ways to work with and view Quarto documents. The default editor is visual, which follows a more “what-you-see-is-what-you-get” style. If you have worked a lot with MS Word documents or Google Docs, this interface will look familiar. Alternatively, you can edit in source, which looks more like a raw script. To switch between the two, you can use the Source and Visual icons in the top left hand side of the screen.\n\n\n\n\n\n\nNote\n\n\n\nI find myself switching between these two formats all the time. The visual editor is much easier to work in when writing, but it can be a bit buggy when it comes to writing code. I work in source when I am writing and running R code."
  },
  {
    "objectID": "resources/quarto.html#rendering-your-document",
    "href": "resources/quarto.html#rendering-your-document",
    "title": "Introduction to Quarto and markdown",
    "section": "Rendering your document",
    "text": "Rendering your document\nTo render your document into your chosen format (in this case: HTML), you need to hit the Render icon in the document’s top bar. This will produce an HTML version of your Quarto document in the same folder in which you saved your Quarto document.\n\n\n\n\n\n\nTip\n\n\n\nYou can preview your document in RStudio by changing your settings to Preview in Viewer Pane.\n\nNow, whenever you render your document a preview of it will show up in the Viewer pane (which is in the same place as your Files, Plots, and Help panes).\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you toggle on the Render on Save option, your Quarto document will render and update your viewer every time you hit save. This can be helpful when you are formatting your document."
  },
  {
    "objectID": "resources/quarto.html#writing-prose-in-quarto",
    "href": "resources/quarto.html#writing-prose-in-quarto",
    "title": "Introduction to Quarto and markdown",
    "section": "Writing prose in Quarto",
    "text": "Writing prose in Quarto\nYou can write prose as you would in any other text editor in Quarto. When you are in the Visual editor model, Quarto provides you with the shortcut keys for many of the formats you use in other text editors, including MS Word and Google Docs. You can also use your usual keyboard shortcuts.\nIn the Source editor mode, you will need to use markdown. Markdown is a lightweight markup language that allows you to format plain text. It gives you a lot of control over the format of your text documents (similar to Latex).\n\n\n\n\n\n\nExercise\n\n\n\nComplete this great Markdown tutorial."
  },
  {
    "objectID": "resources/quarto.html#running-code-in-quarto",
    "href": "resources/quarto.html#running-code-in-quarto",
    "title": "Introduction to Quarto and markdown",
    "section": "Running code in Quarto",
    "text": "Running code in Quarto\nYou can also run code from within your Quarto document. You can do this through a code chunk or in-line code. I will step through both options now.\n\nCode chunks\nA code chunk starts with ```{r} and ends with ```. You can then write whole “chunks” of code that will output in your rendered document.\nFor example, I will load the tidyverse R package into my current session:\n```{r}\n#| echo: true\n\nlibrary(tidyverse)\n```\nYou can specify your chunk options using #| at the start of the line. For example, above I specified that I wanted the code in the code chunk to be shown when I render my document. You can hide the code by changing the chunk option echo to false. There are many different chunk options that you can control. A full list can be found here.\nYou can set the chunk options in the individual chunks, as show above. Alternatively, you can set them universally in the YAML section at the top of your Quarto document using the execute command. For example:\n```{yaml}\nexecute:\n  echo: true\n  message: false\n  warning: false\n```\nThis will apply to all code chunks unless you overwrite it by including chunk-specific options in a code chunk.\nCode chunks are useful for running large amounts of code. Commonly, I use them to include a plot, a regression table, or to read in my data or model results. For example, you can write the code to create a ggplot directly in your document.\n\n\n\nSource: Quarto\n\n\n\n\nIn-line code\nYou will often want to reference numbers or results in your prose. For example, I may be writing up the data section of a paper and want to specify that my data set includes 100 observations. If I were to write this in normally and then go away and collect more data, I would need to come back and update this number manually to reflect my new number of observations. I may do this several times (very tedious) or I may miss a time (we are all human). In-line coding allows you to make these updates programmatically.\nYou include R code directly in your prose using the expression: r. For example:\n\nWill render as: There are r nrow(mpg) observations in our data. No need to go and update this reference if that number changes!\n\n\n\n\n\n\nTip\n\n\n\nscales is a great R package for formatting numbers.\nFor example, R will output raw numbers such as 1000000000 and 8932348920. scales allows you to format these numbers so they are easier to read: scales::comma(1000000000) gives you r scales::comma(1000000000) and scales::dollar(8932348920) gives you r scales::dollar(8932348920).\n\n\nYou can use Quarto to produce all kinds of fully reproducible documents, including journal articles and reports. You can also use it to produce very professional-looking presentations. Finally, you can even use it to produce websites. In fact, all of the resources provided to you here were produced in Quarto."
  },
  {
    "objectID": "resources/quarto.html#footnotes",
    "href": "resources/quarto.html#footnotes",
    "title": "Introduction to Quarto and markdown",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou can also use it from VS Code, Jupyter, Neovim, and Editor.↩︎"
  },
  {
    "objectID": "content/06-probability_theory.html",
    "href": "content/06-probability_theory.html",
    "title": "Probability Theory",
    "section": "",
    "text": "Pollock & Edwards, Chapter 5\n\n\n\n Pollock & Edwards R Companion, Chapter 6"
  },
  {
    "objectID": "content/06-probability_theory.html#readings",
    "href": "content/06-probability_theory.html#readings",
    "title": "Probability Theory",
    "section": "",
    "text": "Pollock & Edwards, Chapter 5\n\n\n\n Pollock & Edwards R Companion, Chapter 6"
  },
  {
    "objectID": "content/06-probability_theory.html#section",
    "href": "content/06-probability_theory.html#section",
    "title": "Probability Theory",
    "section": "Section",
    "text": "Section\n\nPrerequisites\n\nlibrary(tidyverse)\nlibrary(janitor)\n\nset.seed(1234)\n\n\n\n\n\n\n\nNote\n\n\n\nToday, we are working with randomness and chance. To make sure we are all working with the same randomness and chance, you need to set your seed! set.seed() sets the random number generator state in which R will operate for this session. The draws we make in the session are still random, but they will be the same random draw each time we take it. This is important for replication, so you will use it outside of class.\n\n\n\n\nRandomness and avoiding doing the dishes\nImagine you and a friend are trying to decide who will do the dishes after you have both cooked a very large and very messy meal. You agree to flip a coin. Should you pick heads or tails?\nYou really don’t want to do the dishes. Therefore, you want to maximize your chances of winning the coin flip. If you could predict the outcome of the coin flip with certainty, you would simply pick the winning side. Even if you don’t know for certain which side will land on top, you want to pick the side that has the highest chance of winning. How can you work this out?\nFirst, you need to work out all the possible outcomes. This task is simple for a coin flip: heads or tails.\n\npossible_outcomes &lt;- c(\"HEADS\", \"TAILS\")\npossible_outcomes\n\n[1] \"HEADS\" \"TAILS\"\n\n\nThen, you need to work out how likely each of those outcomes are to eventuate. How can we do this? One option available to us here is repeated trials. Flip your coin many times and record how many heads and tails you get. This provides you with a rough understanding of the chance that your coin will land on heads or tails for any given flip.\nFor example, you can flip the coin 10 times and record the results of each flip.\n\nrepeat_trials &lt;- sample(possible_outcomes, size = 10, replace = T, prob = c(0.5, 0.5))\nrepeat_trials\n\n [1] \"TAILS\" \"HEADS\" \"HEADS\" \"HEADS\" \"HEADS\" \"HEADS\" \"TAILS\" \"TAILS\" \"HEADS\"\n[10] \"HEADS\"\n\n\n\n\n\n\n\n\nTip\n\n\n\nWe want to sample with replacement, so we include the argument replace = T. This just means that we include all possible outcomes in every draw. If we sampled without replacement, we would remove each outcome from the sample after it has been selected in a previous draw.\nFor example, imagine you have 10 different colored marbles in a bag. You pull out a marble and record its color. If you want to sample with replacement, you put the marble you just pulled out back into the bag before you take your next draw. This means that you can draw that same marble out again in the subsequent draws.\n\n\nYou can then tally up those results to get your baseline understanding of the chances of heads vs tails.\n\ntable(repeat_trials)\n\nrepeat_trials\nHEADS TAILS \n    7     3 \n\n\nGiven the results of this trial, I would expect that for every 10 coin flips, I should get 7 heads and 3 tails. If I want to use this information to determine the chance that the coin will land on its head after one flip, I can convert this to percentages. There is a 70% chance that the coin will land on heads. Therefore, based solely on the 10 flip trial, you should pick heads.\nHmm… but aren’t fair coins meant to land on heads or tails with equal probability? In fact, if you look back to our sample (drawn using the sample() function) you will see that I explicitly set the probability of landing on heads and tails to be an even 0.5 and 0.5 each (using the prob argument). Why then are we getting 70% for heads and 30% for tails instead of 50% and 50%?\nTo answer this question, we need to build up some foundations in probability theory. Let’s start with independence.\n\n\nIndependence\nYou want to maximize your chances of not doing the dishes (i.e. of picking the winning side of the coin). To do this, you need to know all possible outcomes (heads and tails) and the probability that each of those outcomes will eventuate. To learn this, you ran a trial in which you flipped the coin 10 times and recorded the outcome of each flip. How can you trust that this trial is revealing the true underlying probabilities of heads vs. tails?\nEach time you flipped that coin, you undertook the very process you will eventually take to decide who has to do the dishes. You will only flip that deciding coin once, so you need to know what the chances are that the coin will land on heads or tails that one time. You can’t ever observe that. If you flip a coin once, you will either see heads or tails. But we know that if we flip it again we might get a different outcome. In other words, if you flip a coin once and it lands on heads, this does not necessarily mean that the probability of getting heads is 1 and the probability of getting tails is 0 (or that you will always get heads). We use trials to try to estimate the unobservable underlying probabilities of each possible outcome of a single coin flip.\nTo make sure that we can infer from our observed flips the underlying and unobservable probability of heads vs. tails of one coin flip, we need to make sure that our trials meet certain conditions. The first is independence: the outcome of any other flips cannot impact the outcome of the current flip.\nFor example, let’s go back to our bag of 10 different marbles. Say there are 2 red, 3 blue, and 5 green marbles in your bag. You want to know the probability of drawing out a green marble. You pull out a marble. We know that there is a 20% chance your marble will be red, a 30% chance it will be blue, and a 50% chance it will be green. It is green. You then do not replace the marble before your next draw. Now, there is a 22% chance that marble will be red, a 33% chance it is blue, and a 44% chance it will be green (there are now only nine marbles in your bag: 2 red, 3, blue, and 4 green). These draws are not independent of each other! Your first draw changed the underlying probability of drawing a green marble in your second draw.\nIf your draws are independent of one another, you can infer from the results of the trial the underlying probability of each outcome eventuating.\nBut hold on: we did that and we still got uneven results!\n\ntable(repeat_trials)\n\nrepeat_trials\nHEADS TAILS \n    7     3 \n\n\nWhy?\n\n\nThe law of large numbers\nIn short, our trial was too small.\nEven if our underlying probability is {0.5, 0.5} (which it is: remember that prob = c(0.5, 0.5) argument), we may observe a set of outcomes in our trial that do not reflect this true distribution.\nTo illustrate, think of the outcome you could observe from only one draw: heads or tails. If you draw heads and then use that trial to infer the underlying probability of drawing heads vs. tails, you will state that the underlying probability of drawing a head and tail is equal to {1, 0}. You will be very surprised if you subsequently flip a tail.\nNow, what if you run a trial of two flips?\n\nsample(possible_outcomes, 2, replace = T, prob = c(0.5, 0.5))\n\n[1] \"HEADS\" \"HEADS\"\n\n\nBoth heads!\nIn fact, if we flip a coin twice many times (say, 10 times), we will probably get a couple of trials in which we flip two heads or two tails:\n\ntrial_two_flips &lt;- tibble(trial = 1:10) |&gt; \n  rowwise() |&gt; \n  mutate(outcome = list(sample(possible_outcomes, 2, replace = T, prob = c(0.5, 0.5)))) |&gt; \n  unnest_wider(outcome, names_sep = \"_\") |&gt; \n  rename(\"flip_1\" = outcome_1, \"flip_2\" = outcome_2)\n\ntrial_two_flips\n\n# A tibble: 10 × 3\n   trial flip_1 flip_2\n   &lt;int&gt; &lt;chr&gt;  &lt;chr&gt; \n 1     1 TAILS  HEADS \n 2     2 TAILS  HEADS \n 3     3 TAILS  TAILS \n 4     4 TAILS  TAILS \n 5     5 TAILS  TAILS \n 6     6 TAILS  TAILS \n 7     7 TAILS  HEADS \n 8     8 HEADS  HEADS \n 9     9 HEADS  TAILS \n10    10 TAILS  TAILS \n\n\n6 or 60% of our 10 trials resulted in two of the same outcomes. How can we be confident that our trials are good reflections of the actual distribution of probabilities?\nThe law of large numbers suggests that when your population of independent observations has a finite mean, as the number of observations drawn increases, the mean of the observed values in the sample approaches the mean of the population.\nIn other words, the more flips you do, the closer you will get to the true underlying distribution of probabilities. Cool!\nLet’s try this out.\nFirst, let’s flip the coin 10 times:\n\n\n\n\n\n\nNote\n\n\n\nThe red dots are sitting at {0.5,0.5}. We are aiming for this true probability distribution.\n\n\n\ntibble(outcome = sample(possible_outcomes, 10, replace = T, prob = c(0.5, 0.5))) |&gt; \n  tabyl(outcome) |&gt; \n  ggplot(aes(y = outcome, x = percent)) + \n  geom_col() + \n  geom_point(aes(x = c(0.5, 0.5)), size = 3, colour = \"red\") + \n  theme_minimal() + \n  scale_x_continuous(limits = c(0, 1))\n\n\n\n\nNow, let’s flip it 100 times:\n\ntibble(outcome = sample(possible_outcomes, 100, replace = T, prob = c(0.5, 0.5))) |&gt; \n  tabyl(outcome) |&gt; \n  ggplot(aes(y = outcome, x = percent)) + \n  geom_col() + \n  geom_point(aes(x = c(0.5, 0.5)), size = 3, colour = \"red\") + \n  theme_minimal() + \n  scale_x_continuous(limits = c(0, 1))\n\n\n\n\nNow, let’s flip it 1,000 times:\n\ntibble(outcome = sample(possible_outcomes, 1000, replace = T, prob = c(0.5, 0.5))) |&gt; \n  tabyl(outcome) |&gt; \n  ggplot(aes(y = outcome, x = percent)) + \n  geom_col() + \n  geom_point(aes(x = c(0.5, 0.5)), size = 3, colour = \"red\") + \n  theme_minimal() + \n  scale_x_continuous(limits = c(0, 1))\n\n\n\n\nNow, let’s flip it 10,000 times:\n\ntibble(outcome = sample(possible_outcomes, 10000, replace = T, prob = c(0.5, 0.5))) |&gt; \n  tabyl(outcome) |&gt; \n  ggplot(aes(y = outcome, x = percent)) + \n  geom_col() + \n  geom_point(aes(x = c(0.5, 0.5)), size = 3, colour = \"red\") + \n  theme_minimal() + \n  scale_x_continuous(limits = c(0, 1))\n\n\n\n\nFinally, let’s flip it 100,000 times:\n\ntibble(outcome = sample(possible_outcomes, 100000, replace = T, prob = c(0.5, 0.5))) |&gt; \n  tabyl(outcome) |&gt; \n  ggplot(aes(y = outcome, x = percent)) + \n  geom_col() + \n  geom_point(aes(x = c(0.5, 0.5)), size = 3, colour = \"red\") + \n  theme_minimal() + \n  scale_x_continuous(limits = c(0, 1))\n\n\n\n\nEach time we increase the number of draws we make, we get closer to the true underlying distribution of probabilities (as coded in our sample() function).\nSo, after all of that how can you avoid doing the dishes? Sadly (and as expected) you cannot get an edge on your friend. All possible outcomes have the same probability of eventuating.\n\n\nConditional probability\nWill it rain tomorrow? To answer this question, you may want to gather information about rain trends over time.\nYou may look at the number of times it rained this month last year. Say this is the record of rain days this month last year:\n\nmonth_rain &lt;- tibble(\n  day_of_month = 1:30,\n  rain = c(0, 1, 1, 1, 0, 0, 0, 1, 1, 0, \n           0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n           0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n)\n\nmonth_rain\n\n# A tibble: 30 × 2\n   day_of_month  rain\n          &lt;int&gt; &lt;dbl&gt;\n 1            1     0\n 2            2     1\n 3            3     1\n 4            4     1\n 5            5     0\n 6            6     0\n 7            7     0\n 8            8     1\n 9            9     1\n10           10     0\n# ℹ 20 more rows\n\n\n\n\n\n\n\n\nTip\n\n\n\nrbinom() allows us to take a random draw from 0 and 1.\n\n\nOne way you could answer this question is working out the probability of rain across that month. On what proportion of days that month did it rain?\n\ntabyl(month_rain, rain)\n\n rain  n   percent\n    0 23 0.7666667\n    1  7 0.2333333\n\n\nUsing this information, we may guess that there is a 23% chance of rain on any given day in this month. Therefore, you predict that it will not rain tomorrow.\nBut what if you notice that it tends to rain multiple days in a row? In other words, the chance of rain tomorrow is higher if it rains today. You can use this information to more accurately predict whether it will rain tomorrow. Instead of looking at all days in the month, you can look only at the days of the month that were preceded by rain days:\n\nmonth_rain |&gt; \n  filter(lag(rain) == 1)\n\n# A tibble: 7 × 2\n  day_of_month  rain\n         &lt;int&gt; &lt;dbl&gt;\n1            3     1\n2            4     1\n3            5     0\n4            9     1\n5           10     0\n6           20     1\n7           21     0\n\n\nOn what proportion of these days did it rain?\n\nmonth_rain |&gt; \n  filter(lag(rain) == 1) |&gt; \n  tabyl(rain)\n\n rain n   percent\n    0 3 0.4285714\n    1 4 0.5714286\n\n\nUsing this information, you determine that it rained on 57% of days that were preceded by rain days. If you are currently rugged up in front of a fire enjoying watching a storm raging outside your window, you will predict that there is a 57% chance of it raining tomorrow. You, therefore, predict that it will rain tomorrow.\nConditional probability can help us use all the information we have about the conditions under which an event will occur to update our beliefs about the likelihood of that event occurring.\n\n\nA note on different ways to think about probability\nIt can sometimes help to be introduced to different ways of thinking about these concepts. There are two common approaches to understanding probability: frequentist and Bayesian. Frequentists interpret probabilities as the proportion of an event occurring over an infinite number of identical trials. Therefore, if the probability of getting a head when you flip a fair coin is 50%, this is simply the proportion of times that we would get a head if we flipped a coin infinitely many times. We have adopted the frequentist approach throughout this session.\nBayesians interpret probability differently. They interpret probability to be your belief about the relative likelihood of an event occuring. Your belief is informed by your knowledge of the event and your observations of what has happened in the past.\nTo illustrate, let’s consider how what each of these schools would predict in a coin flip.\nA frequentist and a Bayesian walk into a bar. I ask them to predict the outcome of a coin flip. This coin may, or may not, be fair.\nBefore I flip the coin, I ask each to predict the outcome. The frequestist refuses to provide a prediction: they will wait until I have flipped the coin many times and they have observed the outcome of those flips to provide me with their guess. The Bayesian, on the other hand, happily offers up a prediction: heads. When I ask them why, they say that they think that assuming the coin is fair is a good starting point. The majority of coins in the world are fair coins and they have met me before and think that I am a trustworthy person. Assuming the coin is fair, they randomly decide to predict heads.\nI then flip the coin and let the two observe the outcome:\n\n\n[1] \"TAILS\"\n\n\nI ask the two if they want to update their predictions. Again, the frequentist refuses, saying that they do not have enough information to make a prediction.\nThe Bayesian considers. Their prior assumption was that the coin is fair: there is a 50% chance the coin will land on heads and a 50% chance the coin will land on tails. They have now observed a tails. This does not rule out the possibility that the coin is fair. Nor does it rule out the possibility that the coin is unfair and will always land on tails. It does; however, rule out the possibility that the coin will always land on heads. If that were the case, we would never observe tails. On balance, they decide to update their belief about the likelihood of the coin landing on tails to slightly higher than before. They now suggest that the coin will land on tails with a 55% chance.\nI flip the coin again:\n\n\n[1] \"TAILS\"\n\n\nTails again! The frequentist is still not happy to provide a prediction. The Bayesian thinks again. They increasingly believe that the coin is not biased towards heads. They update their belief again, suggesting that the coin will land on tails with a 60% chance.\nThe frequentist is getting annoyed and asks me to flip the coin a hundred times:\n\n\n\nHEADS TAILS \n   29    71 \n\n\nWe now have 102 data points: the two prior flips and the 100 most recent flips. Of those 102 flips, 69 have been tails and 33 have been heads. The Bayesian updates their beliefs once again: based on both their prior and the distribution of observed flips, they believe that the coin has a 65% chance of landing on tails.\nThe frequestist is now willing to make a prediction: they infer from the outcome of these 102 flips that the coin will land on tails 68% of the time. They therefore predict that the next flip will land on tails, but would like me to flip the coin many more times before they are confident in that inference. I oblige:\n\n\n\n HEADS  TAILS \n300190 699810 \n\n\nFlipping the coin an additional 1 million times provides the two statisticians with 1,000,102 data points. The Bayesian updates their beliefs based on their prior assumption (an even coin) and these 1,000,102 observed outcomes. They believe that there is a 70% chance that the coin will land on tails. They predict tails.\nThe frequestist will now make a confident prediction. They infer from the outcome of these 1,000,102 flips that the coin will land on tails 70% of the time. They therefore predict that the next flip will land on tails. They think that they will be right 70% of the time.\n\n\nWhat does this all have to do with political science?\nFair question.\nQuantitative social science involves statistical inference. We start with a parameter of interest. For example, what proportion of US voters approve of Joe Biden’s job as president? It would be very nice if we could go and ask all US voters what they think and if we could be confident that they are giving us their true opinions. However, this is simply not possible (even the census misses some people!). Instead, we rely on surveys.\nThese surveys (if done well) will take a representative sample of the population of US voters and ask their opinion of Biden. We can then use that sample to infer the overall level of support for Joe Biden among the population (of US voters). Think of a survey as a trial.\nWe can even use this sample to answer interesting questions about groups within the population. What do Republicans think about Joe Biden’s job as president? What about women? Or people of color?\nThis is all statistical inference. Probability theory undergirds our ability to observe or measure variables of interest and use these variables to strengthen our arguments in support of our theory. We will discuss this in more detail next week."
  },
  {
    "objectID": "content/11-regression.html",
    "href": "content/11-regression.html",
    "title": "Regression",
    "section": "",
    "text": "Pollock & Edwards, Chapter 8\n\n\n\n Pollock & Edwards R Companion, Chapter 8"
  },
  {
    "objectID": "content/11-regression.html#readings",
    "href": "content/11-regression.html#readings",
    "title": "Regression",
    "section": "",
    "text": "Pollock & Edwards, Chapter 8\n\n\n\n Pollock & Edwards R Companion, Chapter 8"
  },
  {
    "objectID": "content/11-regression.html#section",
    "href": "content/11-regression.html#section",
    "title": "Regression",
    "section": "Section",
    "text": "Section\n\ninstall.packages(\"fitzRoy\")\n\n\nlibrary(tidyverse)\nlibrary(fitzRoy)\nlibrary(broom)\nlibrary(modelsummary)\nlibrary(ggdist)\n\nset.seed(1234)\n\nToday, we are going to explore the linear relationship between two variables. At the end of this session you will be able to describe how those two variables relate to one another, and whether that relationship is statistically significant.\nAs usual, we will build our understanding using an example. As an Australian (more specifically, a Victorian), I love AFL. It’s the best sport out there! We’re going to depart from our usual political science examples to explore some interesting relationships in the way AFL is played.\nFirst, let’s watch this brief introduction to the game:\n\nLet’s explore the relationship between disposal efficiency and Dream Team points. Your disposal efficiency describes the percentage of your disposals (kicks, handballs, etc.) that lead to a positive outcome for your team. The Dream Team is a popular fantasy football competition. Each player gets points for various actions they take on the field. For example, you get six Dream Team points for scoring a goal and you lose three points for having a free kick awarded against you. This is a useful proxy measure of a player’s effectiveness on the field.\n\nLoading in our data\nWe are going to explore some different player-level outcomes from the tenth round of the most recent AFLW season. This is the last round played before finals. First, we need to collect our data. The fitzRoy::fetch_player_stats() retrieves some useful variables about each player from the official AFL website.\n\nafl_df &lt;- fetch_player_stats(2023, round = 10, comp = \"AFLW\")\nafl_df\n\n# A tibble: 378 × 70\n   providerId      utcStartTime           status compSeason.shortName round.name\n   &lt;chr&gt;           &lt;chr&gt;                  &lt;chr&gt;  &lt;chr&gt;                &lt;chr&gt;     \n 1 CD_M20232641001 2023-11-03T08:45:00.0… CONCL… 2023 NAB AFLW Season Round 10  \n 2 CD_M20232641001 2023-11-03T08:45:00.0… CONCL… 2023 NAB AFLW Season Round 10  \n 3 CD_M20232641001 2023-11-03T08:45:00.0… CONCL… 2023 NAB AFLW Season Round 10  \n 4 CD_M20232641001 2023-11-03T08:45:00.0… CONCL… 2023 NAB AFLW Season Round 10  \n 5 CD_M20232641001 2023-11-03T08:45:00.0… CONCL… 2023 NAB AFLW Season Round 10  \n 6 CD_M20232641001 2023-11-03T08:45:00.0… CONCL… 2023 NAB AFLW Season Round 10  \n 7 CD_M20232641001 2023-11-03T08:45:00.0… CONCL… 2023 NAB AFLW Season Round 10  \n 8 CD_M20232641001 2023-11-03T08:45:00.0… CONCL… 2023 NAB AFLW Season Round 10  \n 9 CD_M20232641001 2023-11-03T08:45:00.0… CONCL… 2023 NAB AFLW Season Round 10  \n10 CD_M20232641001 2023-11-03T08:45:00.0… CONCL… 2023 NAB AFLW Season Round 10  \n# ℹ 368 more rows\n# ℹ 65 more variables: round.roundNumber &lt;int&gt;, venue.name &lt;chr&gt;,\n#   home.team.name &lt;chr&gt;, home.team.club.name &lt;chr&gt;, away.team.name &lt;chr&gt;,\n#   away.team.club.name &lt;chr&gt;, player.jumperNumber &lt;int&gt;,\n#   player.photoURL &lt;chr&gt;, player.player.position &lt;chr&gt;,\n#   player.player.player.playerId &lt;chr&gt;, player.player.player.captain &lt;lgl&gt;,\n#   player.player.player.playerJumperNumber &lt;int&gt;, …\n\n\nIn this round, 378 players played in 9 games. We have access to 70 variables describing how these players performed in this round.\n\n\nUncovering the relationship between two variables\nFirst, we want to determine the direction of the relationship between our variables of interest. When a player’s disposal efficiency increases, do they tend to get a greater or fewer number of Dream Team points?\nWe can often identify the direction of the relationship by plotting our data:\n\nggplot(afl_df, aes(x = disposalEfficiency, y = dreamTeamPoints)) + \n  geom_point() + \n  theme_minimal()\n\n\n\n\nThe relationship looks positive: as your efficiency increases, so too do your Dream Team points. This makes sense. A good player is more efficient (tends to produce good results from their kicks and handballs). Our Dream Team points are a proxy for an effective player. However, there is a fair bit of noise here. This relationship doesn’t look very strong.\nNext, we want to formalize that relationship. We want to find the line that best fits all of these points. In other words, what line minimizes the distance between itself and all of the observed points marking each player’s Dream Team points and disposal efficiency?\n\n\n\n\n\n\nTip\n\n\n\nRemember, this is the basis for Ordinary Least Squares regression.\n\n\nWe can use ggplot2::geom_smooth() to fit this line graphically:\n\nggplot(afl_df, aes(x = disposalEfficiency, y = dreamTeamPoints)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = F) + \n  theme_minimal()\n\n\n\n\nWe correctly identified that there is a positive relationship between those two variables. Yay!\nOften we need more information than simply the direction of a relationship. For example, we want to know how pronounced the effect of a one-unit increase in our independent variable will be on our outcome of interest. Here, we want to learn how many additional Dream Team points are associated with a a one percentage point increase in a player’s disposal efficiency, on average.\nWe can use lm() to determine this information:\n\n\n\n\n\n\nTip\n\n\n\nRemember to put your dependent variable first, then a ~, then you independent variable(s).\n\n\n\nm &lt;- lm(dreamTeamPoints ~ disposalEfficiency, data = afl_df)\nsummary(m)\n\n\nCall:\nlm(formula = dreamTeamPoints ~ disposalEfficiency, data = afl_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-53.618 -17.270  -4.335  14.876  99.525 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        42.32863    4.57123   9.260   &lt;2e-16 ***\ndisposalEfficiency  0.13289    0.07052   1.884   0.0603 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 24.99 on 376 degrees of freedom\nMultiple R-squared:  0.009356,  Adjusted R-squared:  0.006722 \nF-statistic: 3.551 on 1 and 376 DF,  p-value: 0.06028\n\n\nGreat! We now have a linear regression model of the relationship between a player’s disposal efficiency and their Dream Team points.\nWe predict that players that have a disposal efficiency of 0 (none of their disposals result in a good outcome for their team) have, on average, 42.3 Dream Team points. Every one percentage point increase in a player’s efficiency is associated with a gain of 0.13 Dream Team points, on average.\nGreat! We can use this information to understand this relationship more meaningfully. For example, we note that a player with a disposal efficiency of 1.33 percentage points greater than another player has, on average, 10 more Dream Team points than the other player.\n\n\nUncertainty around this relationship\nHow confident can we be in this modeled relationship? Am I really sure that efficiency has a significant, positive impact on a player’s Dream Team points?\nTo answer these questions, we need to draw on everything we have learned about hypothesis testing and inference so far.\nWe want to make a claim about the relationship between some outcome and some variables that we think are important determinants of that outcome. Here, we used the observed Dream Team points gained by a player and their disposal efficiency for round 10 in the 2023 season of AFLW to make a broader statement about the relationship between efficiency and Dream Team points. We are not actually interested in the specific relationship between these variables in round 10 of the 2023 season: we want to know whether a player’s efficiency is an important determinant of their Dream Team points generally.\nIn other words, we are inferring from a sample a general relationship. You can imagine that if we had a different sample, a linear regression model would have a different set of estimates for our intercept and coefficient. They would (hopefully) look very similar to the ones we found above, but they would be slightly different. For example, imagine that some of the random elements of a game of AFL were different: the wind blew in a different way, the crowd cheered a little louder, a player ran a little faster. These would change the game played in slight and random ways. A player might make a clanger instead of a clean disposal. The wind may blow an otherwise goal into a behind. Consequently, the players’ disposal efficiency would be slightly different. We would subsequently get different model estimates.\nLet’s illustrate this. To be able to access this variation to illustrate our point, I am going to take pure random samples from our 378 players.\n\n\n\n\n\n\nNote\n\n\n\nWe are solidly in the multiverse world with this AFL example. We did not take a random sample of players who played in round 10, collect information on their Dream Team points and efficiency, and fit our regression. Rather, we have information about every player.\nThis is also often the case in political science. For example, we have full information on the number of wars fought between countries in the modern era. We also have a lot of data on those countries. We do not; therefore, need to take a sample from that population (unlike pollsters attempting to work out a politician’s level of support from all US voters).\nHowever, this does not mean that we know with certainty the relationship between our variables of interest. Just as with sport, there is so much randomness that can effect these outcomes. We want to capture and account for that randomness. So, we have to treat these data as samples.\n\n\nLet’s take a completely random sample of 250 players from our data and fit our regression against that sample:\n\nafl_sample_1 &lt;- afl_df |&gt; \n  sample_n(250) |&gt;\n  select(player.player.player.givenName, \n         player.player.player.surname,\n         dreamTeamPoints,\n         disposalEfficiency)\nafl_sample_1\n\n# A tibble: 250 × 4\n   player.player.player.givenName player.player.player.surname dreamTeamPoints\n   &lt;chr&gt;                          &lt;chr&gt;                                  &lt;dbl&gt;\n 1 Simone                         Nalder                                    21\n 2 Emelia                         Yassir                                    20\n 3 Ashleigh                       Saint                                     76\n 4 Chloe                          Dalton                                    44\n 5 Renee                          Garing                                    92\n 6 Ebony                          O'Dea                                     47\n 7 Jasmin                         Stewart                                   43\n 8 Sophie                         Conway                                    68\n 9 Hannah                         Ewings                                    22\n10 Courtney                       Jones                                     43\n# ℹ 240 more rows\n# ℹ 1 more variable: disposalEfficiency &lt;dbl&gt;\n\n\n\nm &lt;- lm(dreamTeamPoints ~ disposalEfficiency, data = afl_sample_1)\n\ntidy(m)\n\n# A tibble: 2 × 5\n  term               estimate std.error statistic  p.value\n  &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)          36.8      5.20        7.07 1.54e-11\n2 disposalEfficiency    0.215    0.0810      2.65 8.47e- 3\n\n\nOkay, so we got a different intercept and coefficient estimates than we got above. This is despite the fact that I took a pure random sample from the full set of players. The only thing driving this difference is random chance.\nLet’s do this again. I will take a different random sample of players and fit a regression for them:\n\nafl_sample_2 &lt;- afl_df |&gt; \n  sample_n(250) |&gt;\n  select(player.player.player.givenName, \n         player.player.player.surname,\n         dreamTeamPoints,\n         disposalEfficiency)\nafl_sample_2\n\n# A tibble: 250 × 4\n   player.player.player.givenName player.player.player.surname dreamTeamPoints\n   &lt;chr&gt;                          &lt;chr&gt;                                  &lt;dbl&gt;\n 1 Kodi                           Jacques                                   50\n 2 Dana                           East                                      64\n 3 Cambridge                      McCormick                                 48\n 4 Rachelle                       Martin                                    74\n 5 Madison                        Newman                                    94\n 6 Courtney                       Jones                                     43\n 7 Mackenzie                      Webb                                      30\n 8 Jessica                        Low                                       36\n 9 Erin                           Phillips                                  76\n10 Justine                        Mules                                     35\n# ℹ 240 more rows\n# ℹ 1 more variable: disposalEfficiency &lt;dbl&gt;\n\n\n\nm &lt;- lm(dreamTeamPoints ~ disposalEfficiency, data = afl_sample_2)\n\ntidy(m)\n\n# A tibble: 2 × 5\n  term               estimate std.error statistic  p.value\n  &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)          41.3      5.32        7.75 2.31e-13\n2 disposalEfficiency    0.165    0.0827      2.00 4.71e- 2\n\n\nAgain, we got a different set of coefficients. This is only the result of random chance.\nLet’s go again:\n\nafl_sample_3 &lt;- afl_df |&gt; \n  sample_n(250) |&gt;\n  select(player.player.player.givenName, \n         player.player.player.surname,\n         dreamTeamPoints,\n         disposalEfficiency)\nafl_sample_3\n\n# A tibble: 250 × 4\n   player.player.player.givenName player.player.player.surname dreamTeamPoints\n   &lt;chr&gt;                          &lt;chr&gt;                                  &lt;dbl&gt;\n 1 Taylor                         Smith                                     47\n 2 Alice                          O'Loughlin                                46\n 3 Zarlie                         Goldsworthy                              103\n 4 Chelsea                        Biddell                                   58\n 5 Courtney                       Hodder                                    92\n 6 Mim                            Strom                                     75\n 7 Jade                           Ellenger                                  92\n 8 Zoe                            Wakfer                                    32\n 9 Isabel                         Huntington                                37\n10 Brooke                         Brown                                     58\n# ℹ 240 more rows\n# ℹ 1 more variable: disposalEfficiency &lt;dbl&gt;\n\n\n\nm &lt;- lm(dreamTeamPoints ~ disposalEfficiency, data = afl_sample_3)\n\ntidy(m)\n\n# A tibble: 2 × 5\n  term               estimate std.error statistic  p.value\n  &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)          42.3      5.40        7.84 1.35e-13\n2 disposalEfficiency    0.126    0.0838      1.50 1.34e- 1\n\n\nWe can see these differences:\n\nafl_sample_1 |&gt; \n  mutate(sample = \"One\") |&gt; \n  bind_rows(\n    mutate(afl_sample_2, sample = \"Two\")\n  ) |&gt; \n  bind_rows(\n    mutate(afl_sample_3, sample = \"Three\")\n  ) |&gt; \n  ggplot(aes(x = disposalEfficiency, y = dreamTeamPoints, colour = sample)) + \n  geom_point(alpha = 0.5) + \n  geom_smooth(method = \"lm\", se = F) + \n  theme_minimal()\n\n\n\n\nLet’s do this many times:\n\nafl_sample_regression &lt;- function(i) {\n  \n  afl_sample &lt;- sample_n(afl_df, 250)\n  \n  m &lt;- lm(dreamTeamPoints ~ disposalEfficiency, data = afl_sample)\n  \n  model_results &lt;- tibble(round = i, model = tidy(m))\n  \n  return(model_results)\n  \n}\n\nafl_regressions &lt;- map(1:1000, afl_sample_regression, .progress = T) |&gt; \n  bind_rows()\n\nNow we can take a look at those different regression coefficients:\n\nafl_regressions |&gt; \n  unnest(model)\n\n# A tibble: 2,000 × 6\n   round term               estimate std.error statistic  p.value\n   &lt;int&gt; &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1     1 (Intercept)         39.1       5.25       7.45  1.51e-12\n 2     1 disposalEfficiency   0.172     0.0811     2.12  3.54e- 2\n 3     2 (Intercept)         45.6       5.74       7.94  6.85e-14\n 4     2 disposalEfficiency   0.0719    0.0871     0.826 4.10e- 1\n 5     3 (Intercept)         43.9       5.90       7.44  1.68e-12\n 6     3 disposalEfficiency   0.110     0.0912     1.21  2.28e- 1\n 7     4 (Intercept)         37.8       5.35       7.08  1.51e-11\n 8     4 disposalEfficiency   0.190     0.0829     2.29  2.29e- 2\n 9     5 (Intercept)         42.2       5.69       7.43  1.80e-12\n10     5 disposalEfficiency   0.140     0.0884     1.58  1.16e- 1\n# ℹ 1,990 more rows\n\n\nThis shows the intercept and coefficient for the relationship between disposal efficiency and Dream Team points for each of the models we generated from our 1,000 different random samples. Remember, the only thing driving the differences between each model’s intercept and coefficient is random chance.\nLet’s start with the disposal efficiency coefficients:\n\nafl_regressions |&gt; \n  unnest(model) |&gt; \n  filter(term == \"disposalEfficiency\") |&gt; \n  ggplot(aes(x = estimate)) + \n  stat_halfeye() + \n  theme_minimal() + \n  labs(caption = \"Median is shown with point. One and two standard deviations are shown by the black bars.\")\n\n\n\n\nHere is the distribution of those 1,000 different coefficients for disposal efficiency for each model.\nNext, we can look at the intercepts:\n\nafl_regressions |&gt; \n  unnest(model) |&gt; \n  filter(term == \"(Intercept)\") |&gt; \n  ggplot(aes(x = estimate)) + \n  stat_halfeye() + \n  theme_minimal() + \n  labs(caption = \"Median is shown with point. One and two standard deviations are shown by the black bars.\")\n\n\n\n\nYou can see that one coefficient and estimate is common (represented by the peak of these two density plots) and that other estimates fall roughly symmetrically around this most common estimate.\nIn fact, if we ran an infinite number of regressions from an infinite number of random samples from our data, these coefficients would be distributed following the t-distribution.\nWe can use this knowledge to focus back on that single regression model that we built right at the start of this session.\n\nm &lt;- lm(dreamTeamPoints ~ disposalEfficiency, data = afl_df)\ntidy(m)\n\n# A tibble: 2 × 5\n  term               estimate std.error statistic  p.value\n  &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)          42.3      4.57        9.26 1.59e-18\n2 disposalEfficiency    0.133    0.0705      1.88 6.03e- 2\n\n\n\n\nBuilding uncertainty into our single regression model\nGenerally, we have a single sample and we use that sample to fit our models. We still need to account for the uncertainty (demonstrated above) created by random chance. We know that from an infinite number of samples, we would fit linear models with coefficients that follow the t-distribution. So, let’s use that knowledge to work out how certain we are of our estimates found above.\nWe have our point estimates for our intercept and coefficient:\n\ntidy(m) |&gt; \n  select(term, estimate)\n\n# A tibble: 2 × 2\n  term               estimate\n  &lt;chr&gt;                 &lt;dbl&gt;\n1 (Intercept)          42.3  \n2 disposalEfficiency    0.133\n\n\nThese are our best guess at the true linear relationship between disposal efficiency and Dream Team points. Our best guess is that players who have a disposal efficiency of zero have, on average, 42.3 Dream Team points. Every one percentage point increase in a player’s efficiency is associated with a gain of 0.13 Dream Team points, on average.\nPerfect. Let’s set these best guesses as our center points.\nNow, we need to build out the plausible set of intercepts and coefficients that would result from an infinite number of random samples drawn from our population. To do this, we need to work out how spread out these intercepts and coefficients would be from their center points. Formally, this spread is called the standard deviation.\nThe standard deviation, \\(s\\), is calculated using two pieces of information. First, it looks at how well our line of best fit (our regression model) fits our observed data. How far are the predicted values (represented by the blue line on the below graph) from the observed values (represented by the black dots)?\n\nggplot(afl_df, aes(x = disposalEfficiency, y = dreamTeamPoints)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = F) + \n  theme_minimal()\n\n\n\n\nWe will have more uncertainty around our intercept and coefficient if the distance between the predicted values and observed values is large. This makes sense: if the predicted values and the observed values were very similar, we are unlikely to find a wildly different line of best fit from a different random sample from our population.\nThe standard deviation also takes into account the amount of information you have used to generate this intercept and coefficient. Greater sample sizes using less variables result in less uncertainty around those model coefficients.\nIn sum, the standard deviation is calculated as:\n\\[\ns = \\sqrt{\\frac{\\sum{e_i^2}}{n-k-1}}\n\\]\nWhere \\(e_i\\) is the difference between each predicted value and observed value (the vertical distance between each point and the blue line on the graph above).\n\\(n-k-1\\) is the degrees of freedom in the model. It accounts for the amount of information you used to build the model, with \\(n\\) equal to the number of observations you used and \\(k\\) equal to the number of independent variables you included. Here, our \\(k=1\\) because we are only using one independent variable: disposal efficiency.\nWe can use this spread to work out our standard errors around our coefficients. Broadly, the standard error places this uncertainty within the context of the model.\nFor the intercept, it is:\n\\[\nSE_{\\beta_0} = s(\\sqrt{\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum{(x_i-\\bar{x})^2}}})\n\\]\nFor the coefficients, it is:\n\\[\nSE_{\\beta_1} = \\frac{s}{\\sqrt{\\sum{(x_i-\\bar{x})^2}}}\n\\]\n\n\n\n\n\n\nTip\n\n\n\nYou will calculate all of this in the background. Do not worry about memorizing these formulas. I put them here to help build your intuition about what goes into your uncertainty around the relationship you find between your outcome of interest and the variables you think drive that outcome.\n\n\nHappily, broom::tidy() calculates all of this for us:\n\ntidy(m) |&gt; \n  select(term:std.error)\n\n# A tibble: 2 × 3\n  term               estimate std.error\n  &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)          42.3      4.57  \n2 disposalEfficiency    0.133    0.0705\n\n\nOkay, so we have a good sense of the spread around our best guess. Let’s visualize this:\n\ntibble(\n  x = rnorm(1e6, \n            mean = tidy(m) |&gt; filter(term == \"disposalEfficiency\") |&gt; pull(estimate),\n            sd = tidy(m) |&gt; filter(term == \"disposalEfficiency\") |&gt; pull(std.error))\n) |&gt; \n  ggplot(aes(x = x)) + \n  stat_halfeye() + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThere are two things to note here. The first is that I am using the normal distribution, not the t-distribution. As far as I can tell, there isn’t a way to change the center or spread of random draws from the t-distribution in R (done using rt()). So, to illustrate I am using random draws from the normal distribution (done using rnorm()). At these large degrees of freedom, these distributions are almost identical, so we can roll with this.\nSecond, I am pulling 1 million random draws from this distribution. In theory, we talk about the distribution of an infinite number of coefficients built from an infinite number of samples. I, obviously, cannot do this, so I use 1 million as my stand-in for infinity.\n\n\nAbove, we show the plausible set of coefficients describing the effect of a one percentage point increase in a player’s disposal efficiency on their Dream Team points, on average. Remember, our best guess (taken from our regression model) is that a one percentage point increase in efficiency is associated with a 0.13 increase in a player’s Dream Team points, on average. However, as we saw above when we built 1,000 different models from 1,000 different random samples, it is entirely plausible that we could get different coefficients from a model built from a different sample. These differences are the product of random chance. That’s fine! We just need to acknowledge that.\nLet’s also visualize all of the plausible intercepts we could get:\n\ntibble(\n  x = rnorm(1e6, \n            mean = tidy(m) |&gt; filter(term == \"(Intercept)\") |&gt; pull(estimate),\n            sd = tidy(m) |&gt; filter(term == \"(Intercept)\") |&gt; pull(std.error))\n) |&gt; \n  ggplot(aes(x = x)) + \n  stat_halfeye() + \n  theme_minimal()\n\n\n\n\nAgain, our best guess is that players who have a disposal efficiency of 0 have, on average, 42.3 Dream Team points. This is one of many plausible intercepts that we could generate.\n\n\nConnecting this to our research question\nWe used our data to uncover an interesting relationship between disposal efficiency and Dream Team points, on average. As your disposal efficiency increases, so too do your Dream Team points. We found that if players who increased their disposal efficiency by one percentage point saw an increase of 0.13 Dream Team points, on average.\nNow, imagine that you are a player and you want to increase your Dream Team points. Do you trust this? Do you really believe that by increasing your disposal efficiency you will increase your Dream Team points? What if, in fact, there is no relationship between these two variables?\nWe know from above that there are a range of plausible intercepts and coefficients that result from random chance. Does this plausible range include zero? In other words, is it plausible that there is no relationship between the outcome of interest (Dream Team points) and your independent variable (disposal efficiency)?\nTo answer this question, we draw on our knowledge of hypothesis testing.\nFirst, we need to define what we mean by a plausible range of relationships. Traditionally, we are willing to accept a five percent risk that we declare that there is a relationship between an outcome and an independent variable when there is, in fact, no relationship. Let’s stick with that threshold.\nOkay, so now we need to work out where the other 95 percent of plausible values sit. Let’s start with the coefficient describing the relationship between Dream Team points and disposal efficiency. Here is our representation of the coefficients drawn from an infinite number random samples pulled from our population (it is the same as printed above):\n\ntibble(\n  x = rnorm(1e6, \n            mean = tidy(m) |&gt; filter(term == \"disposalEfficiency\") |&gt; pull(estimate),\n            sd = tidy(m) |&gt; filter(term == \"disposalEfficiency\") |&gt; pull(std.error))\n) |&gt; \n  ggplot(aes(x = x)) + \n  stat_halfeye() + \n  theme_minimal()\n\n\n\n\nWhere do 95 percent of these coefficients fall around our best guess? We can use our knowledge of the t-distribution to answer this question.\nWe know that center point (our coefficient):\n\nbeta_1 &lt;- tidy(m) |&gt; \n  filter(term == \"disposalEfficiency\") |&gt; \n  pull(estimate)\nbeta_1\n\n[1] 0.1328924\n\n\nWe know how spread out our alternative coefficients are from that center point (our standard error):\n\nse_beta_1 &lt;- tidy(m) |&gt; \n  filter(term == \"disposalEfficiency\") |&gt; \n  pull(std.error)\nse_beta_1\n\n[1] 0.0705205\n\n\nAnd we know our threshold (5%). We need to translate that threshold into its t-statistic, accounting for the degrees of freedom we have access to:\n\nsample_size &lt;- nrow(afl_df)\nsample_size\n\n[1] 378\n\nno_of_IVs &lt;- 1\nno_of_IVs\n\n[1] 1\n\ndf &lt;- sample_size - no_of_IVs - 1\ndf\n\n[1] 376\n\n\nTherefore:\n\nt_stat_95 &lt;- qt(0.025, df = df, lower.tail = F)\nt_stat_95\n\n[1] 1.966293\n\n\nThis is the point beyond which 2.5% of all values along the t-distribution fall.\n\n\n\n\n\n\nTip\n\n\n\nRemember, we want to find out where 95 percent of these alternative coefficients sit around the center point. So, we need to distribute our remaining five percent between the two tails. That’s why we find the t-statistic beyond which 2.5% of the data fall.\n\n\nNow we can find the boundaries within which 95 percent of these alternative coefficients fall:\n\n\n\n\n\n\nNote\n\n\n\nRemember:\n\\[\nCI = \\beta_1 \\pm t*SE_{\\beta_1}\n\\]\n\n\n\nlower_ci &lt;- beta_1 - t_stat_95*se_beta_1\nlower_ci\n\n[1] -0.005771574\n\nupper_ci &lt;- beta_1 + t_stat_95*se_beta_1\nupper_ci\n\n[1] 0.2715564\n\n\nLet’s place those in context:\n\ntibble(\n  x = rnorm(1e6, \n            mean = tidy(m) |&gt; filter(term == \"disposalEfficiency\") |&gt; pull(estimate),\n            sd = tidy(m) |&gt; filter(term == \"disposalEfficiency\") |&gt; pull(std.error))\n) |&gt; \n  ggplot(aes(x = x)) + \n  stat_slab(aes(fill_ramp = after_stat(x &lt; lower_ci | x &gt; upper_ci))) + \n  theme_minimal() + \n  theme(legend.position = \"none\")\n\n\n\n\nBrilliant! Now we know where 95 percent of all alternative coefficients drawn from a random sample of our population could fall. These are our plausible alternative coefficients.\nSo, do these plausible alternatives include zero? In other words, is it plausible that there is no relationship between a player’s disposal efficiency and their Dream Team points?\n\ntibble(\n  x = rnorm(1e6, \n            mean = tidy(m) |&gt; filter(term == \"disposalEfficiency\") |&gt; pull(estimate),\n            sd = tidy(m) |&gt; filter(term == \"disposalEfficiency\") |&gt; pull(std.error))\n) |&gt; \n  ggplot(aes(x = x)) + \n  stat_slab(aes(fill_ramp = after_stat(x &lt; lower_ci | x &gt; upper_ci))) + \n  geom_vline(xintercept = 0) + \n  theme_minimal() + \n  theme(legend.position = \"none\")\n\n\n\n\nYes! A null relationship sits within our plausible set of coefficients for disposal efficiency. We cannot reject the notion that there is no relationship between disposal efficiency and Dream Team points at this threshold.\nWhat about a more forgiving threshold? What if we are willing to accept a 10 percent chance that we reject a true null relationship?\nFirst, we need to find this new threshold’s t-statistic:\n\nt_stat_90 &lt;- qt(0.05, df = df, lower.tail = F)\nt_stat_90\n\n[1] 1.648916\n\n\nAnd; therefore, our new boundaries (within which 90 percent of alternative coefficients sit):\n\nlower_ci &lt;- beta_1 - t_stat_90*se_beta_1\nlower_ci\n\n[1] 0.01661001\n\nupper_ci &lt;- beta_1 + t_stat_90*se_beta_1\nupper_ci\n\n[1] 0.2491748\n\n\nDo these contain a null relationship?\n\ntibble(\n  x = rnorm(1e6, \n            mean = tidy(m) |&gt; filter(term == \"disposalEfficiency\") |&gt; pull(estimate),\n            sd = tidy(m) |&gt; filter(term == \"disposalEfficiency\") |&gt; pull(std.error))\n) |&gt; \n  ggplot(aes(x = x)) + \n  stat_slab(aes(fill_ramp = after_stat(x &lt; lower_ci | x &gt; upper_ci))) + \n  geom_vline(xintercept = 0) + \n  theme_minimal() + \n  theme(legend.position = \"none\")\n\n\n\n\nNo! So, if we are happy to accept a 10 percent risk that we will reject a true null relationship, we can reject the null hypothesis that there is no relationship between disposal efficiency and Dream Team points. We can tell the players that we have found a statistically significant relationship between disposal efficiency and Dream Team points at the 0.1 threshold.\nWe can approach this question from the other direction. If the null hypothesis were true, how likely would we be to see our estimate?\nFirst, let’s set up our null world:\n\ntibble(\n  x = rt(1e6, df = df)\n) |&gt; \n  ggplot(aes(x = x)) + \n  stat_slab() + \n  theme_minimal() + \n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nI am back to random draws from our t-distribution.\n\n\nHere, we have centered our distribution of alternative coefficients (resulting only from differences in our samples brought about by random chance) at zero. We are in the null world: there is no relationship between disposal efficiency and Dream Team points.\nWhere does the estimate we found in our sample sit within this null world? First, we need to translate that observed estimate into its t-statistic:\n\\[\nt = \\frac{\\beta_1}{SE_{\\beta_1}}\n\\]\n\nt_stat &lt;- beta_1 / se_beta_1\nt_stat\n\n[1] 1.884451\n\n\nLet’s place this in the context of the null world:\n\ntibble(\n  x = rt(1e6, df = df)\n) |&gt; \n  ggplot(aes(x = x)) + \n  stat_slab() + \n  geom_vline(xintercept = t_stat) + \n  theme_minimal() + \n  theme(legend.position = \"none\")\n\n\n\n\nHow likely is it that I would get this coefficient or a more extreme coefficient if we did, in fact, live in the null world? In other words, what proportion of these alternative coefficients (highlighted in dark gray on the graph below) are equal to or greater than our observed estimate?\n\n\n\n\n\n\nTip\n\n\n\nRemember that we are conducting a two-tailed test of our null hypothesis. We need to be open to the estimate being greater or smaller than the null.\n\n\n\ntibble(\n  x = rt(1e6, df = df)\n) |&gt; \n  ggplot(aes(x = x)) + \n  stat_slab(aes(fill_ramp = after_stat(x &lt; -t_stat | x &gt; t_stat))) + \n  geom_vline(xintercept = t_stat) + \n  theme_minimal() + \n  theme(legend.position = \"none\")\n\n\n\n\nThe proportion of alternative coefficients that are equal to or more extreme than the one we observed is:\n\np_value &lt;- 2 * pt(t_stat, df = df, lower.tail = F)\np_value\n\n[1] 0.06027518\n\n\nWe would observe an estimate of 0.133 or a more extreme estimate 6% of the time if the null hypothesis were true. When we are only happy to accept a five percent chance that we would reject a true null hypothesis, we cannot reject that null hypothesis. If, on the other hand, we are happy to accept a 10 percent chance that we reject a true null hypothesis, we can reject it.\n\n\nReading our regression outputs\nLet’s return to our original model:\n\ntidy(m)\n\n# A tibble: 2 × 5\n  term               estimate std.error statistic  p.value\n  &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)          42.3      4.57        9.26 1.59e-18\n2 disposalEfficiency    0.133    0.0705      1.88 6.03e- 2\n\n\nWe now have information on every component of this output.\nWe can translate these estimates, in the estimate column. Our model suggests that players that have a disposal efficiency of 0 (none of their disposals result in a good outcome for their team) have, on average, 42.3 Dream Team points. Every one percentage point increase in a player’s efficiency is associated with a gain of 0.13 Dream Team points, on average.\nWe know that these estimates are our best guess of the true linear relationship between Dream Team points and disposal efficiency. Our best guess may be different from the true relationship because of random chance. How confident are we of that best guess? Well, we know from the standard error (std.error) how spread out around that best guess alternative coefficients sit.\nWe know where our estimate (translated into its t-statistic, or statistic) sits within the null world.\nAnd finally, we know the probability (p.value) that we would observe the estimate we found if it were actually equal to zero.\nAll of that work we did above is replicated here, in this one line of code. In fact, we can also add our confidence intervals around our estimate to the broom::tidy() output:\n\ntidy(m, conf.int = T)\n\n# A tibble: 2 × 7\n  term               estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)          42.3      4.57        9.26 1.59e-18 33.3        51.3  \n2 disposalEfficiency    0.133    0.0705      1.88 6.03e- 2 -0.00577     0.272\n\n\nThese are exactly as we manually calculated above.\nWhat would this all look like in a published article?\n\nmodelsummary(m,\n             coef_rename = c(\"disposalEfficiency\" = \"Disposal efficiency\"),\n             statistic = c(\"std.error\", \"p.value\"))\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n42.329\n\n\n\n(4.571)\n\n\n\n(&lt;0.001)\n\n\nDisposal efficiency\n0.133\n\n\n\n(0.071)\n\n\n\n(0.060)\n\n\nNum.Obs.\n378\n\n\nR2\n0.009\n\n\nR2 Adj.\n0.007\n\n\nAIC\n3510.0\n\n\nBIC\n3521.8\n\n\nLog.Lik.\n−1751.992\n\n\nF\n3.551\n\n\nRMSE\n24.93\n\n\n\n\n\n\n\nIncreasingly, people are including confidence intervals:\n\nmodelsummary(m,\n             coef_rename = c(\"disposalEfficiency\" = \"Disposal efficiency\"),\n             statistic = c(\"conf.int\", \"p.value\"))\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n42.329\n\n\n\n[33.340, 51.317]\n\n\n\n(&lt;0.001)\n\n\nDisposal efficiency\n0.133\n\n\n\n[−0.006, 0.272]\n\n\n\n(0.060)\n\n\nNum.Obs.\n378\n\n\nR2\n0.009\n\n\nR2 Adj.\n0.007\n\n\nAIC\n3510.0\n\n\nBIC\n3521.8\n\n\nLog.Lik.\n−1751.992\n\n\nF\n3.551\n\n\nRMSE\n24.93\n\n\n\n\n\n\n\nOr presenting their regression results graphically:\n\nmodelplot(m,\n          coef_rename = c(\"disposalEfficiency\" = \"Disposal efficiency\"))"
  },
  {
    "objectID": "content/01-introduction.html",
    "href": "content/01-introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Welcome to GVPT622! This semester, we will introduce you to the statistical concepts you need to provide compelling empirical evidence of your theories and to engage critically with the evidence provided by others in support of their arguments. We will illustrate how these concepts work using real-world examples. You are encouraged to ask as many questions as you have, and to work with each other to learn and apply these new skills.\nDuring the weekly lectures, you will be introduced to statistical concepts and proofs. You will then apply these new-found skills during our weekly lab sessions. These sessions will focus on two outcomes:\n\nStrengthening your understanding of the statistical concepts introduced in the lecture; and,\nDeveloping your ability to communicate clearly these complex ideas to a wide audience.\n\nYou may, therefore, be amazed at the volume of writing that will be required from you during this statistics course. Long gone are the days of Greek letters etched onto a chalk board, hanging untethered from their practical purpose. Never again will you ask “why on earth do I need to know this?!”. Instead, you will be required to work through problems that interest you. You will inevitably come up to hurdles or gaps in your knowledge. You can then work with us and your colleagues sitting around you to solve these problems. As a result, you will be armed with many of the tools required to make sense of the world around you.\nThe statistical concepts introduced to you this semester are relatively simple. That is partly by design; you require no experience in statistical analysis or “a head for numbers” to succeed in this course or PhD program. It is also the product of the simple fact that the statistical concepts and models we use to lend empirical credence to our research are intuitive. They are the product of clever and curious people like you asking practical questions of the world around them. How did Small and Singer (1976) prove Kant’s (1795) hunch that democracies are less likely to go to war with one another than with other regime types? They looked to history and compared the proportion of wars fought between democracies to those fought between and with other regimes types. At the end of this semester, you will come to recognize that a lot of the empirical work we all do is just fancy averaging.\nThis frees us up to concentrate on another critical skill you need to be a great political scientist: the ability to communicate complex ideas clearly and accessibly. In Zinsser’s (2009) words, a good non-fiction writer can “make complex subjects clear and enjoyable— and useful—to ordinary readers.” I encourage you to write clearly, concisely, and engagingly about the relationships you discover in your data. You will not be able to hide behind technical or generic language; you must be able to explain the mechanisms unpinning your claims in full. When you use simple language to communicate these ideas to a broad audience, you demonstrate your command of your research.\nBorrowing again from Zinsser (1993) who is, this time, discussing Einstein’s (1916) (surprisingly accessible) explanation of his theory of relativity:\n\n“Reduce your discipline – whatever it is - to a logical sequence of clearly thought sentences. You will thereby make it clear not only to other people but to yourself. You will find out whether you know your subject as well as you thought you did. If you don’t, writing will show you where the holes are in your knowledge or your reasoning.”\n\nLet’s get started!\n\n\n\n\nReferences\n\nEinstein, Albert. 1916. Relativity : The Special and General Theory: Original Version. CreateSpace Independent Publishing Platform.\n\n\nKant, Immanuel. 1795. Perpetual Peace: A Philosophical Essay. https://www.gutenberg.org/ebooks/50922.\n\n\nSmall, Melvin, and J. David Singer. 1976. “The War-Proneness of Democratic Regimes, 1816-1965.” The Jerusalem Journal of International Relations, The jerusalem journal of international relations. - jerusalem : Magnes press, ISSN 0363-2865, ZDB-ID 194424-1. - vol. 1.1976, 4, p. 50-69, 1 (4).\n\n\nZinsser, William. 1993. Writing To Learn. New York: Harper Perennial.\n\n\n———. 2009. “First, Use Plain English.” Yale Alumni Magazine 72 (4)."
  },
  {
    "objectID": "content/01-predicting_outcomes.html",
    "href": "content/01-predicting_outcomes.html",
    "title": "Predicting outcomes",
    "section": "",
    "text": "In November, the US voting public will cast their vote for the next US President. Who do you think will receive the most votes? What information did you use to make this guess?\nI guess that President Biden will receive the most votes across the US. Why?\n\nI assume that an individual’s demographics (their gender, level of education, income, county of residence, etc.) are good predictors of their vote in US Presidential elections.\nI assume that the factors connecting demographics to an individual’s political preferences have not changed in the intervening years between the last election and this one.\nI assume that individuals view their choice for President identically to how they viewed this choice at the last election. This is because the two major parties are fielding the same candidate they did in this previous election.\nI assume that the US voting public looks similar to how it did during the last election.\nTherefore, I predict that the distribution of votes will be identical to last election’s results; President Biden will once again receive the greatest number of votes.\n\nThere are some pretty wild assumptions undergirding my prediction. Setting any issues you have with those assumptions aside, let’s check one of my factual statements: President Biden won the majority of the votes cast in the last US Presidential Election.\n\nWhat proportion of US voters voted for President Biden in 2020? To answer this question, we will look at state-level returns for elections for the US presidency. These data are provided by the MIT Election Data and Science Lab.\n\n\n\n\n\n\nExercise\n\n\n\nHead over to the MIT Election Lab’s About and Data pages to see what information you can access.\n\n\nTo access, clean, and analyse these data, we are going to use a series of functions from the tidyverse R packages loaded in below:\n\n\n\n\n\n\nInstalling packages\n\n\n\n\n\nIf you have not already done so, please install the following packages:\n\ninstall.packages(c(\"tidyverse\", \"here\"))\n\nPlease remember that you only need to do this once (unless you want to update the package). Do not include this command in your R script. Rather, run it in your console.\n\n\n\n\nlibrary(tidyverse)\n\n\nWe are going to download and use the MIT Election Lab’s U.S. President 1976-2020 dataset. You can find it on their Data page.\nThe data are stored on the Harvard Dataverse. This is a data sharing platform used commonly by academics to share their data and other replication files. When you publish your research, you might provide your data and R scripts to the public using this platform.\nWe are going to download the 1976-2020-president.tab file from the U.S. President 1976-2020 page.\n\n\n\n\n\n\nNote\n\n\n\nYou will need to provide your information to access the data. Please use your UMD credentials (instead of your personal ones).\n\n\nOnce you have downloaded the 1976-2020-president.csv file, you need to move it into your current RProject folder. I like to store raw data in a sub-folder called data-raw (I store cleaned data in a sub-folder called data), but you can place the file anywhere you like within this project.\n\n\n\n\n\n\nNote\n\n\n\nManually downloading files can be tedious, especially when they are updated regularly. Happily, the Harvard Dataverse has an API that allows us to access its data sets programmatically. Working with APIs can be a little tricky, and require a few more R skills than you may currently have. We will learn how to work with this API in later weeks.\n\n\n\nWe now have access to the state-level returns for elections to the US presidency from 1976 to 2020. All we need to do is read them in!\nTo do that, you are going to provide the file’s location (as a string) as the first argument in the function read_csv().\n\nYour file will be stored somewhere on your local computer. This means that this file path is going to be different for each of us (and each computer!). For example, my version of 1976-2020-president.csv is stored here:\n\n\n[1] \"/Users/harrietgoers/Documents/GVPT622/content/data/1976-2020-president.csv\"\n\n\nIf I include that file path as the argument in read_csv() the code will run and I will have read the data set into my current R session. Happy days!\nHowever, when I share this code with you (or when I try to run it on a different computer) I will get an error! It will say:\n\n\n[1] \"Error: '/Users/harrietgoers/Documents/GVPT622/content/data/1976-2020-president.csv' does not exist.\"\n\n\nThat’s because this file path is unique to my computer. That’s a problem because I want my scripts to run on anyone’s computer. Happily, we have a solution: here::here()!\nThe here() function from the here R package places you within the top-level directory of your current project, no matter which computer you are working from. This makes for much more robust code.\nFor example, I am currently working from my GVPT622 RProject (the project that stores all of the scripts that build this website!). From that top-level directory, I have stored 1976-2020-president.csv in a sub-folder called data, which is located in a sub-folder called content. So, all I need to do is leave that trail of breadcrumbs for here::here() to follow:\n\nhere::here(\"content\", \"data\", \"1976-2020-president.csv\")\n\n[1] \"/Users/harrietgoers/Documents/GVPT622/content/data/1976-2020-president.csv\"\n\n\nNow, if I sent you this RProject folder and you were to run this script, here::here() would change part of the file path unique to my computer (/Users/harrietgoers/Documents/) to reflect where you have stored this RProject on your computer. No more annoying errors! Yay!\nSo, we are going to use this more robust file referencing process to tell read_csv() where our data are located:\n\n\n\n\n\n\nTip\n\n\n\nMake sure to adjust your breadcrumbs to reflect your own folder structure.\n\n\n\nread_csv(here::here(\"content\", \"data\", \"1976-2020-president.csv\"))\n\n# A tibble: 4,287 × 15\n    year state   state_po state_fips state_cen state_ic office       candidate  \n   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;      \n 1  1976 ALABAMA AL                1        63       41 US PRESIDENT \"CARTER, J…\n 2  1976 ALABAMA AL                1        63       41 US PRESIDENT \"FORD, GER…\n 3  1976 ALABAMA AL                1        63       41 US PRESIDENT \"MADDOX, L…\n 4  1976 ALABAMA AL                1        63       41 US PRESIDENT \"BUBAR, BE…\n 5  1976 ALABAMA AL                1        63       41 US PRESIDENT \"HALL, GUS\"\n 6  1976 ALABAMA AL                1        63       41 US PRESIDENT \"MACBRIDE,…\n 7  1976 ALABAMA AL                1        63       41 US PRESIDENT  &lt;NA&gt;      \n 8  1976 ALASKA  AK                2        94       81 US PRESIDENT \"FORD, GER…\n 9  1976 ALASKA  AK                2        94       81 US PRESIDENT \"CARTER, J…\n10  1976 ALASKA  AK                2        94       81 US PRESIDENT \"MACBRIDE,…\n# ℹ 4,277 more rows\n# ℹ 7 more variables: party_detailed &lt;chr&gt;, writein &lt;lgl&gt;,\n#   candidatevotes &lt;dbl&gt;, totalvotes &lt;dbl&gt;, version &lt;dbl&gt;, notes &lt;lgl&gt;,\n#   party_simplified &lt;chr&gt;\n\n\nBrilliant! You just read into R your first file. But how do we use it?\n\nWhen we read a data set into our current R session, we want to assign it as an object. We can then use and modify this object as we like.\nI am going to assign this data set to an object named prev_state_lvl_results:\n\nprev_state_lvl_results &lt;- read_csv(here::here(\"content\", \"data\", \"1976-2020-president.csv\"))\n\nThis means that I can now use the data set by simply running:\n\nprev_state_lvl_results\n\n# A tibble: 4,287 × 15\n    year state   state_po state_fips state_cen state_ic office       candidate  \n   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;      \n 1  1976 ALABAMA AL                1        63       41 US PRESIDENT \"CARTER, J…\n 2  1976 ALABAMA AL                1        63       41 US PRESIDENT \"FORD, GER…\n 3  1976 ALABAMA AL                1        63       41 US PRESIDENT \"MADDOX, L…\n 4  1976 ALABAMA AL                1        63       41 US PRESIDENT \"BUBAR, BE…\n 5  1976 ALABAMA AL                1        63       41 US PRESIDENT \"HALL, GUS\"\n 6  1976 ALABAMA AL                1        63       41 US PRESIDENT \"MACBRIDE,…\n 7  1976 ALABAMA AL                1        63       41 US PRESIDENT  &lt;NA&gt;      \n 8  1976 ALASKA  AK                2        94       81 US PRESIDENT \"FORD, GER…\n 9  1976 ALASKA  AK                2        94       81 US PRESIDENT \"CARTER, J…\n10  1976 ALASKA  AK                2        94       81 US PRESIDENT \"MACBRIDE,…\n# ℹ 4,277 more rows\n# ℹ 7 more variables: party_detailed &lt;chr&gt;, writein &lt;lgl&gt;,\n#   candidatevotes &lt;dbl&gt;, totalvotes &lt;dbl&gt;, version &lt;dbl&gt;, notes &lt;lgl&gt;,\n#   party_simplified &lt;chr&gt;\n\n\nLet’s take a look at that object. We get some pretty useful information from this print out. From it we learn that we have 4,287 rows and 15 columns of data. What does this mean?\nWell, this data set provides us with information for each election from 1976 to 2020. It provides the number of votes cast in each state for each presidential candidate in each of those races. Therefore, each row provides us with data for each candidate in each state and in each election. For example, the first row of our data set provides information on…\n\nslice(prev_state_lvl_results, 1)\n\n# A tibble: 1 × 15\n   year state   state_po state_fips state_cen state_ic office       candidate   \n  &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;       \n1  1976 ALABAMA AL                1        63       41 US PRESIDENT CARTER, JIM…\n# ℹ 7 more variables: party_detailed &lt;chr&gt;, writein &lt;lgl&gt;,\n#   candidatevotes &lt;dbl&gt;, totalvotes &lt;dbl&gt;, version &lt;dbl&gt;, notes &lt;lgl&gt;,\n#   party_simplified &lt;chr&gt;\n\n\nJimmy Carter in the 1976 Alabama race.\n\n\n\n\n\n\nExercise\n\n\n\nWhich race, state, and candidate does the 100th row refer to?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nslice(prev_state_lvl_results, 100)\n\n# A tibble: 1 × 15\n   year state  state_po state_fips state_cen state_ic office       candidate    \n  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;        \n1  1976 KANSAS KS               20        47       32 US PRESIDENT MACBRIDE, RO…\n# ℹ 7 more variables: party_detailed &lt;chr&gt;, writein &lt;lgl&gt;,\n#   candidatevotes &lt;dbl&gt;, totalvotes &lt;dbl&gt;, version &lt;dbl&gt;, notes &lt;lgl&gt;,\n#   party_simplified &lt;chr&gt;\n\n\nRoger Macbride’s race in Kansas in 1976.\n\n\n\nWe refer to this as our unit of observation. More succinctly, I would say that our unit of observation is candidate-state-election year. We have information on 4,287 different candidate-state-election years.\nHow much information do we have on each of these candidate-state-election years? This data set includes 15 variables. They include the election year (year), the candidate’s name (candidate), their party (party_detailed), the number of votes they received in each state (candidarevotes), and the total number of votes cast in that state (totalvotes).\nYou get a quick summary of these data using the function glimpse():\n\nglimpse(prev_state_lvl_results)\n\nRows: 4,287\nColumns: 15\n$ year             &lt;dbl&gt; 1976, 1976, 1976, 1976, 1976, 1976, 1976, 1976, 1976,…\n$ state            &lt;chr&gt; \"ALABAMA\", \"ALABAMA\", \"ALABAMA\", \"ALABAMA\", \"ALABAMA\"…\n$ state_po         &lt;chr&gt; \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AK\", \"AK\",…\n$ state_fips       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4,…\n$ state_cen        &lt;dbl&gt; 63, 63, 63, 63, 63, 63, 63, 94, 94, 94, 94, 86, 86, 8…\n$ state_ic         &lt;dbl&gt; 41, 41, 41, 41, 41, 41, 41, 81, 81, 81, 81, 61, 61, 6…\n$ office           &lt;chr&gt; \"US PRESIDENT\", \"US PRESIDENT\", \"US PRESIDENT\", \"US P…\n$ candidate        &lt;chr&gt; \"CARTER, JIMMY\", \"FORD, GERALD\", \"MADDOX, LESTER\", \"B…\n$ party_detailed   &lt;chr&gt; \"DEMOCRAT\", \"REPUBLICAN\", \"AMERICAN INDEPENDENT PARTY…\n$ writein          &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE…\n$ candidatevotes   &lt;dbl&gt; 659170, 504070, 9198, 6669, 1954, 1481, 308, 71555, 4…\n$ totalvotes       &lt;dbl&gt; 1182850, 1182850, 1182850, 1182850, 1182850, 1182850,…\n$ version          &lt;dbl&gt; 20210113, 20210113, 20210113, 20210113, 20210113, 202…\n$ notes            &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ party_simplified &lt;chr&gt; \"DEMOCRAT\", \"REPUBLICAN\", \"OTHER\", \"OTHER\", \"OTHER\", …\n\n\n\nWhat are those three-letter abbreviations next to the column names? They are the data type. Data types really matter, particularly when running statistical analysis in R. Here we have three different types of data:\n\n\n\n\nData type\nDescription\n\n\n\ndbl\nDouble, or numbers with decimals (1.0, 33.5, 7.67)\n\n\nchr\nCharacter, or letters, words, sentences (“A”, “cat”, “5”)\n\n\nlgl\nLogical (TRUE, FALSE)\n\n\n\n\n\nIn an effort not to be too boring, we will talk about data types as we walk through working with data (today and over the coming weeks). It is important to note now that each column can only contain one data type. For example, a column that contains doubles (or numbers) cannot also include characters (or words). This makes working with data a lot easier. Imagine trying to find the average of a column and including a character in that calculation!\n\nWe have data on the total number of people in each state who voted for each candidate and the total number of people who voted in each state. We want to find the proportion of US voters who voted for President Biden in the 2020 election. How do we get from here to there?\nLet’s start by calculating the total number of US voters nationally who voted for each candidate in each election and the total number of people who voted in each election nationally. To do this, we need to aggregate from the state-level to the national-level.\nWe are going to use group_by() to group our data together by candidate and election year. That will allow us to calculate the sum (using the aptly named sum() function) for each candidate in each election.\nBecause we are aggregating our data, we use the summarize() (or summarise(), if you aren’t American!) to return only one observation (or row) for each candidate in each election.\n\n\n\n\n\n\nExercise\n\n\n\nWhat will be our new unit of observation?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nCandidate-election year.\n\n\n\n\nprev_nat_lvl_results &lt;- prev_state_lvl_results |&gt; \n  # Divide the dataset up by election year and candidate\n  group_by(year, candidate) |&gt; \n  # Calculate the total number of voters who voted in each of these groups\n  summarise(nationalcandidatevotes = sum(candidatevotes), \n            nationaltotalvotes = sum(totalvotes)) |&gt; \n  # Ungroup our data\n  ungroup()\n\nprev_nat_lvl_results\n\n# A tibble: 370 × 4\n    year candidate                     nationalcandidatevotes nationaltotalvotes\n   &lt;dbl&gt; &lt;chr&gt;                                          &lt;dbl&gt;              &lt;dbl&gt;\n 1  1976 \"ANDERSON, THOMAS J.\"                         147835           28498006\n 2  1976 \"BUBAR, BENJAMIN \\\"\\\"BEN\\\"\\\"\"                  15888           13255917\n 3  1976 \"CAMEJO, PETER\"                                89737           59540602\n 4  1976 \"CARTER, JIMMY\"                             40825839           88269606\n 5  1976 \"FORD, GERALD\"                              39145771           88269606\n 6  1976 \"HALL, GUS\"                                    57721           43458746\n 7  1976 \"JULIUS \\\"\\\"JULES\\\"\\\", LEVIN\"                   9229           25681673\n 8  1976 \"LAROUCHE, LYNDON, JR.\"                        39148           46223231\n 9  1976 \"MACBRIDE, ROGER\"                             171266           55303046\n10  1976 \"MADDOX, LESTER\"                              168623           35723586\n# ℹ 360 more rows\n\n\nI have stored this new data set as an object called prev_nat_lvl_results. If you look over to your environment (in the box that is probably located in the top right hand corner of your RStudio), you should now see two objects: prev_state_lvl_results and prev_nat_lvl_results.\nYou might have noticed that this new data set is smaller than the state-level one. This is good! We now have an aggregate count of the number of US voters across the US who voted for each candidate in each election.\nWhat do we find? The first row tells us that across the entire US, Thomas J. Anderson received only 147,835 votes in the 1976 US Presidential Election.\nWho received the most votes that year?\n\nprev_nat_lvl_results |&gt; \n  # Only include data from the 1976 election\n  filter(year == 1976) |&gt; \n  # Find the maximum number of votes cast for a candidate in that election\n  slice_max(nationalcandidatevotes)\n\n# A tibble: 1 × 4\n   year candidate     nationalcandidatevotes nationaltotalvotes\n  &lt;dbl&gt; &lt;chr&gt;                          &lt;dbl&gt;              &lt;dbl&gt;\n1  1976 CARTER, JIMMY               40825839           88269606\n\n\nJimmy Carter, who received 40,825,839 votes.\nHow did each candidate do in this election? We can easily answer this question by visualizing our data:\n\nprev_nat_lvl_results |&gt; \n  # Only include the 1976 Presidential Election\n  filter(year == 1976) |&gt; \n  # Plot votes cast by candidate\n  ggplot(aes(x = nationalcandidatevotes / 1e06, y = candidate)) + \n  geom_col() + \n  # Make the plot more readable\n  theme_minimal() + \n  theme(plot.title.position = \"plot\",\n        plot.title = element_text(face = \"bold\")) +\n  # Include informative labels\n  labs(x = \"Number of votes cast (millions)\",\n       y = \"Candidate\",\n       title = \"Total votes cast nationally for each candidate in the 1976 Presidential Election\",\n       caption = \"Source: MIT Election Lab (2023)\")\n\n\n\n\nggplot() has (by default) ordered our y-axis by reverse alphabethical order. Let’s use a more useful order: the number of votes cast for each candidate. Let’s also get rid of those votes cast for no candidate (NA).\n\nprev_nat_lvl_results |&gt; \n  filter(year == 1976) |&gt; \n  drop_na(candidate) |&gt; \n  ggplot(aes(x = nationalcandidatevotes / 1e06, y = reorder(candidate, nationalcandidatevotes))) + \n  geom_col() + \n  theme_minimal() + \n  theme(plot.title.position = \"plot\",\n        plot.title = element_text(face = \"bold\")) +\n  labs(x = \"Number of votes cast (millions)\",\n       y = \"Candidate\",\n       title = \"Total votes cast nationally for each candidate in the 1976 Presidential Election\",\n       caption = \"Source: MIT Election Lab (2023)\")\n\n\n\n\nAs expected, the Democratic and Republican candidates received the vast majority of votes cast. What proportion of votes did they receive?\nLet’s go back to our full data set:\n\nprev_nat_lvl_results\n\n# A tibble: 370 × 4\n    year candidate                     nationalcandidatevotes nationaltotalvotes\n   &lt;dbl&gt; &lt;chr&gt;                                          &lt;dbl&gt;              &lt;dbl&gt;\n 1  1976 \"ANDERSON, THOMAS J.\"                         147835           28498006\n 2  1976 \"BUBAR, BENJAMIN \\\"\\\"BEN\\\"\\\"\"                  15888           13255917\n 3  1976 \"CAMEJO, PETER\"                                89737           59540602\n 4  1976 \"CARTER, JIMMY\"                             40825839           88269606\n 5  1976 \"FORD, GERALD\"                              39145771           88269606\n 6  1976 \"HALL, GUS\"                                    57721           43458746\n 7  1976 \"JULIUS \\\"\\\"JULES\\\"\\\", LEVIN\"                   9229           25681673\n 8  1976 \"LAROUCHE, LYNDON, JR.\"                        39148           46223231\n 9  1976 \"MACBRIDE, ROGER\"                             171266           55303046\n10  1976 \"MADDOX, LESTER\"                              168623           35723586\n# ℹ 360 more rows\n\n\nYou may have noticed that we have different numbers indicating the total number of votes cast nationally in each election (nationaltotalvotes). In 1976 alone we have 15 different answers ranging from 5,552,843 to 88,269,606 to the question “how many people voted in this election nationally?”. We should have one answer! What is going on?\n\n\n\n\n\n\nExercise\n\n\n\nBefore you read on, see if you can answer this question. Not everyone is an Americanist (or American), so I will give you some hints:\n\nNotice how the number recorded in that nationaltotalvotes column for the Democractic candidate (Jimmy Carter) and Republican candidate (Gerald Ford) are the same.\nNotice also that all other candidates have smaller numbers recorded in nationaltotalvotes.\nFinally, think back to the unit of observation with which we started: state-candidate-election year.\n\n\n\nWhy do we get several different answers to the question “how many people voted in the national election?” when we calculate the sum total of people who voted in every state? Because not all candidates were included in all states’ ballots. The independents and smaller party candidates did not run in several states. The only two candidates to run in all 50 states and DC were the Democratic and Republican ones.\nTo double check this, we can count the number of states in which each candidate ran. We go back to our state-level results:\n\nprev_state_lvl_results |&gt; \n  filter(year == 1976) |&gt;\n  distinct(candidate, state) |&gt; \n  group_by(candidate) |&gt; \n  count()\n\n# A tibble: 16 × 2\n# Groups:   candidate [16]\n   candidate                           n\n   &lt;chr&gt;                           &lt;int&gt;\n 1 \"ANDERSON, THOMAS J.\"              20\n 2 \"BUBAR, BENJAMIN \\\"\\\"BEN\\\"\\\"\"      10\n 3 \"CAMEJO, PETER\"                    27\n 4 \"CARTER, JIMMY\"                    51\n 5 \"FORD, GERALD\"                     51\n 6 \"HALL, GUS\"                        18\n 7 \"JULIUS \\\"\\\"JULES\\\"\\\", LEVIN\"      11\n 8 \"LAROUCHE, LYNDON, JR.\"            22\n 9 \"MACBRIDE, ROGER\"                  32\n10 \"MADDOX, LESTER\"                   21\n11 \"MCCARTHY, EUGENE \\\"\\\"GENE\\\"\\\"\"    28\n12 \"OTHER\"                             8\n13 \"SCATTERING\"                        4\n14 \"WRIGHT, MARGARET\"                  4\n15 \"ZEIDLER, FRANK\"                    6\n16  &lt;NA&gt;                              14\n\n\nOr in visual form:\n\nprev_state_lvl_results |&gt; \n  filter(year == 1976) |&gt; \n  distinct(candidate, state) |&gt; \n  group_by(candidate) |&gt; \n  count() |&gt; \n  ggplot(aes(x = n, y = reorder(candidate, n))) + \n  geom_col() + \n  theme_minimal() + \n  theme(plot.title.position = \"plot\",\n        plot.title = element_text(face = \"bold\")) + \n  labs(x = \"Number of states\",\n       y = \"Candidate\",\n       title = \"Number of states in which each candidate ran in the 1976 US Presidential Election\")\n\n\n\n\nSo, when we sum together the total number of votes cast in each state in which each candidate ran, we get different answers depending on the number of states in which each candidate ran.\nThe lesson here is that you need to really know your data and your subject matter. Your knowledge as a political scientist is so relevant to your empirical work.\nSo, going forward, I am going to assume that the total number of votes cast in each election is reflected by the total number of votes cast in all of the states in which either the Republican or Democratic candidate ran. This is because I assume that these parties ran in every state in every election. Is this assumption valid? Let’s use the data to find out!\nWe know that there are 51 states or territories in which votes are cast for the US President. This number has not changed between 1976 and 2020. So, we need to confirm that both the Republican and Democractic candidates ran in 51 states and territories every election.\n\nprev_state_lvl_results |&gt; \n  # Only include the official D or R candidates\n  filter(party_simplified %in% c(\"DEMOCRAT\", \"REPUBLICAN\")) |&gt; \n  distinct(year, state) |&gt; \n  count(year)\n\n# A tibble: 12 × 2\n    year     n\n   &lt;dbl&gt; &lt;int&gt;\n 1  1976    51\n 2  1980    51\n 3  1984    51\n 4  1988    51\n 5  1992    51\n 6  1996    51\n 7  2000    51\n 8  2004    51\n 9  2008    51\n10  2012    51\n11  2016    51\n12  2020    51\n\n\nYes! We know that the total number of votes cast in states and territories in which the Republican or Democrat ran represents the total number of votes cast nationally. They ran in every state or territory in which votes were cast for the US President in all elections between 1976 and 2020.\nOkay, let’s get that national total for each election:\n\nnational_total_votes &lt;- prev_state_lvl_results |&gt; \n  # Only need one candidate's data \n  filter(party_simplified == \"DEMOCRAT\") |&gt;\n  # Calculate the national total for each election\n  group_by(year) |&gt; \n  summarise(nationaltotalvotes = sum(totalvotes))\n\nnational_total_votes\n\n# A tibble: 12 × 2\n    year nationaltotalvotes\n   &lt;dbl&gt;              &lt;dbl&gt;\n 1  1976           81601344\n 2  1980           86496851\n 3  1984           92654861\n 4  1988           91586825\n 5  1992          104599780\n 6  1996           96389818\n 7  2000          105593982\n 8  2004          124733688\n 9  2008          131419253\n10  2012          129139997\n11  2016          142141798\n12  2020          158528503\n\n\nOr visually:\n\nggplot(national_total_votes, aes(x = year, y = nationaltotalvotes / 1e6)) + \n  geom_line() + \n  geom_point() + \n  theme_minimal() + \n  theme(plot.title.position = \"plot\",\n        plot.title = element_text(face = \"bold\")) + \n  labs(x = \"Year\",\n       y = \"Number of votes cast (millions)\",\n       title = \"Total number of votes cast in US Presidential Elections, 1976-2020\",\n       caption = \"Source: MIT Election Lab (2023)\")\n\n\n\n\nWe can now update our national-level data to reflect the true total number of people who voted nationally:\n\nprev_nat_lvl_results &lt;- prev_nat_lvl_results |&gt; \n  # Remove the inconsistent data\n  select(-nationaltotalvotes) |&gt;\n  # Merge with the true national total\n  left_join(national_total_votes, by = join_by(year))\n\nprev_nat_lvl_results\n\n# A tibble: 370 × 4\n    year candidate                     nationalcandidatevotes nationaltotalvotes\n   &lt;dbl&gt; &lt;chr&gt;                                          &lt;dbl&gt;              &lt;dbl&gt;\n 1  1976 \"ANDERSON, THOMAS J.\"                         147835           81601344\n 2  1976 \"BUBAR, BENJAMIN \\\"\\\"BEN\\\"\\\"\"                  15888           81601344\n 3  1976 \"CAMEJO, PETER\"                                89737           81601344\n 4  1976 \"CARTER, JIMMY\"                             40825839           81601344\n 5  1976 \"FORD, GERALD\"                              39145771           81601344\n 6  1976 \"HALL, GUS\"                                    57721           81601344\n 7  1976 \"JULIUS \\\"\\\"JULES\\\"\\\", LEVIN\"                   9229           81601344\n 8  1976 \"LAROUCHE, LYNDON, JR.\"                        39148           81601344\n 9  1976 \"MACBRIDE, ROGER\"                             171266           81601344\n10  1976 \"MADDOX, LESTER\"                              168623           81601344\n# ℹ 360 more rows\n\n\nThat looks better!\nNow we have all of the information we need to calculate the proportion of US voters who voted for each candidate in each election:\n\nprev_nat_lvl_results &lt;- mutate(prev_nat_lvl_results, \n                               prop_votes = nationalcandidatevotes / nationaltotalvotes)\n\nprev_nat_lvl_results\n\n# A tibble: 370 × 5\n    year candidate          nationalcandidatevotes nationaltotalvotes prop_votes\n   &lt;dbl&gt; &lt;chr&gt;                               &lt;dbl&gt;              &lt;dbl&gt;      &lt;dbl&gt;\n 1  1976 \"ANDERSON, THOMAS…                 147835           81601344   0.00181 \n 2  1976 \"BUBAR, BENJAMIN …                  15888           81601344   0.000195\n 3  1976 \"CAMEJO, PETER\"                     89737           81601344   0.00110 \n 4  1976 \"CARTER, JIMMY\"                  40825839           81601344   0.500   \n 5  1976 \"FORD, GERALD\"                   39145771           81601344   0.480   \n 6  1976 \"HALL, GUS\"                         57721           81601344   0.000707\n 7  1976 \"JULIUS \\\"\\\"JULES…                   9229           81601344   0.000113\n 8  1976 \"LAROUCHE, LYNDON…                  39148           81601344   0.000480\n 9  1976 \"MACBRIDE, ROGER\"                  171266           81601344   0.00210 \n10  1976 \"MADDOX, LESTER\"                   168623           81601344   0.00207 \n# ℹ 360 more rows\n\n\nAnd finally, we can answer our original question: what proportion of US voters voted for President Biden in the last US Presidential election?\n\nprev_nat_lvl_results |&gt; \n  filter(year == 2020, candidate == \"BIDEN, JOSEPH R. JR\")\n\n# A tibble: 1 × 5\n   year candidate           nationalcandidatevotes nationaltotalvotes prop_votes\n  &lt;dbl&gt; &lt;chr&gt;                                &lt;dbl&gt;              &lt;dbl&gt;      &lt;dbl&gt;\n1  2020 BIDEN, JOSEPH R. JR               81268908          158528503      0.513\n\n\nSo, following my logic set up above, my best guess of the proportion of US voters who will vote for President Biden in the 2024 US Presidential election is 0.513, or 51.3%.\nAre you skeptical of my assumptions? Why? Do you think my logic is sound? If not, how could you improve it? What additional information would you need to provide a better guess? How will you use that additional information to inform your better guess?\nWelcome to your PhD in political science!"
  },
  {
    "objectID": "content/01-predicting_outcomes.html#predicting-the-outcome-of-the-2024-us-presidential-election",
    "href": "content/01-predicting_outcomes.html#predicting-the-outcome-of-the-2024-us-presidential-election",
    "title": "Predicting outcomes",
    "section": "",
    "text": "In November, the US voting public will cast their vote for the next US President. Who do you think will receive the most votes? What information did you use to make this guess?\nI guess that President Biden will receive the most votes across the US. Why?\n\nI assume that an individual’s demographics (their gender, level of education, income, county of residence, etc.) are good predictors of their vote in US Presidential elections.\nI assume that the factors connecting demographics to an individual’s political preferences have not changed in the intervening years between the last election and this one.\nI assume that individuals view their choice for President identically to how they viewed this choice at the last election. This is because the two major parties are fielding the same candidate they did in this previous election.\nI assume that the US voting public looks similar to how it did during the last election.\nTherefore, I predict that the distribution of votes will be identical to last election’s results; President Biden will once again receive the greatest number of votes.\n\nThere are some pretty wild assumptions undergirding my prediction. Setting any issues you have with those assumptions aside, let’s check one of my factual statements: President Biden won the majority of the votes cast in the last US Presidential Election.\n\nWhat proportion of US voters voted for President Biden in 2020? To answer this question, we will look at state-level returns for elections for the US presidency. These data are provided by the MIT Election Data and Science Lab.\n\n\n\n\n\n\nExercise\n\n\n\nHead over to the MIT Election Lab’s About and Data pages to see what information you can access.\n\n\nTo access, clean, and analyse these data, we are going to use a series of functions from the tidyverse R packages loaded in below:\n\n\n\n\n\n\nInstalling packages\n\n\n\n\n\nIf you have not already done so, please install the following packages:\n\ninstall.packages(c(\"tidyverse\", \"here\"))\n\nPlease remember that you only need to do this once (unless you want to update the package). Do not include this command in your R script. Rather, run it in your console.\n\n\n\n\nlibrary(tidyverse)\n\n\nWe are going to download and use the MIT Election Lab’s U.S. President 1976-2020 dataset. You can find it on their Data page.\nThe data are stored on the Harvard Dataverse. This is a data sharing platform used commonly by academics to share their data and other replication files. When you publish your research, you might provide your data and R scripts to the public using this platform.\nWe are going to download the 1976-2020-president.tab file from the U.S. President 1976-2020 page.\n\n\n\n\n\n\nNote\n\n\n\nYou will need to provide your information to access the data. Please use your UMD credentials (instead of your personal ones).\n\n\nOnce you have downloaded the 1976-2020-president.csv file, you need to move it into your current RProject folder. I like to store raw data in a sub-folder called data-raw (I store cleaned data in a sub-folder called data), but you can place the file anywhere you like within this project.\n\n\n\n\n\n\nNote\n\n\n\nManually downloading files can be tedious, especially when they are updated regularly. Happily, the Harvard Dataverse has an API that allows us to access its data sets programmatically. Working with APIs can be a little tricky, and require a few more R skills than you may currently have. We will learn how to work with this API in later weeks.\n\n\n\nWe now have access to the state-level returns for elections to the US presidency from 1976 to 2020. All we need to do is read them in!\nTo do that, you are going to provide the file’s location (as a string) as the first argument in the function read_csv().\n\nYour file will be stored somewhere on your local computer. This means that this file path is going to be different for each of us (and each computer!). For example, my version of 1976-2020-president.csv is stored here:\n\n\n[1] \"/Users/harrietgoers/Documents/GVPT622/content/data/1976-2020-president.csv\"\n\n\nIf I include that file path as the argument in read_csv() the code will run and I will have read the data set into my current R session. Happy days!\nHowever, when I share this code with you (or when I try to run it on a different computer) I will get an error! It will say:\n\n\n[1] \"Error: '/Users/harrietgoers/Documents/GVPT622/content/data/1976-2020-president.csv' does not exist.\"\n\n\nThat’s because this file path is unique to my computer. That’s a problem because I want my scripts to run on anyone’s computer. Happily, we have a solution: here::here()!\nThe here() function from the here R package places you within the top-level directory of your current project, no matter which computer you are working from. This makes for much more robust code.\nFor example, I am currently working from my GVPT622 RProject (the project that stores all of the scripts that build this website!). From that top-level directory, I have stored 1976-2020-president.csv in a sub-folder called data, which is located in a sub-folder called content. So, all I need to do is leave that trail of breadcrumbs for here::here() to follow:\n\nhere::here(\"content\", \"data\", \"1976-2020-president.csv\")\n\n[1] \"/Users/harrietgoers/Documents/GVPT622/content/data/1976-2020-president.csv\"\n\n\nNow, if I sent you this RProject folder and you were to run this script, here::here() would change part of the file path unique to my computer (/Users/harrietgoers/Documents/) to reflect where you have stored this RProject on your computer. No more annoying errors! Yay!\nSo, we are going to use this more robust file referencing process to tell read_csv() where our data are located:\n\n\n\n\n\n\nTip\n\n\n\nMake sure to adjust your breadcrumbs to reflect your own folder structure.\n\n\n\nread_csv(here::here(\"content\", \"data\", \"1976-2020-president.csv\"))\n\n# A tibble: 4,287 × 15\n    year state   state_po state_fips state_cen state_ic office       candidate  \n   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;      \n 1  1976 ALABAMA AL                1        63       41 US PRESIDENT \"CARTER, J…\n 2  1976 ALABAMA AL                1        63       41 US PRESIDENT \"FORD, GER…\n 3  1976 ALABAMA AL                1        63       41 US PRESIDENT \"MADDOX, L…\n 4  1976 ALABAMA AL                1        63       41 US PRESIDENT \"BUBAR, BE…\n 5  1976 ALABAMA AL                1        63       41 US PRESIDENT \"HALL, GUS\"\n 6  1976 ALABAMA AL                1        63       41 US PRESIDENT \"MACBRIDE,…\n 7  1976 ALABAMA AL                1        63       41 US PRESIDENT  &lt;NA&gt;      \n 8  1976 ALASKA  AK                2        94       81 US PRESIDENT \"FORD, GER…\n 9  1976 ALASKA  AK                2        94       81 US PRESIDENT \"CARTER, J…\n10  1976 ALASKA  AK                2        94       81 US PRESIDENT \"MACBRIDE,…\n# ℹ 4,277 more rows\n# ℹ 7 more variables: party_detailed &lt;chr&gt;, writein &lt;lgl&gt;,\n#   candidatevotes &lt;dbl&gt;, totalvotes &lt;dbl&gt;, version &lt;dbl&gt;, notes &lt;lgl&gt;,\n#   party_simplified &lt;chr&gt;\n\n\nBrilliant! You just read into R your first file. But how do we use it?\n\nWhen we read a data set into our current R session, we want to assign it as an object. We can then use and modify this object as we like.\nI am going to assign this data set to an object named prev_state_lvl_results:\n\nprev_state_lvl_results &lt;- read_csv(here::here(\"content\", \"data\", \"1976-2020-president.csv\"))\n\nThis means that I can now use the data set by simply running:\n\nprev_state_lvl_results\n\n# A tibble: 4,287 × 15\n    year state   state_po state_fips state_cen state_ic office       candidate  \n   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;      \n 1  1976 ALABAMA AL                1        63       41 US PRESIDENT \"CARTER, J…\n 2  1976 ALABAMA AL                1        63       41 US PRESIDENT \"FORD, GER…\n 3  1976 ALABAMA AL                1        63       41 US PRESIDENT \"MADDOX, L…\n 4  1976 ALABAMA AL                1        63       41 US PRESIDENT \"BUBAR, BE…\n 5  1976 ALABAMA AL                1        63       41 US PRESIDENT \"HALL, GUS\"\n 6  1976 ALABAMA AL                1        63       41 US PRESIDENT \"MACBRIDE,…\n 7  1976 ALABAMA AL                1        63       41 US PRESIDENT  &lt;NA&gt;      \n 8  1976 ALASKA  AK                2        94       81 US PRESIDENT \"FORD, GER…\n 9  1976 ALASKA  AK                2        94       81 US PRESIDENT \"CARTER, J…\n10  1976 ALASKA  AK                2        94       81 US PRESIDENT \"MACBRIDE,…\n# ℹ 4,277 more rows\n# ℹ 7 more variables: party_detailed &lt;chr&gt;, writein &lt;lgl&gt;,\n#   candidatevotes &lt;dbl&gt;, totalvotes &lt;dbl&gt;, version &lt;dbl&gt;, notes &lt;lgl&gt;,\n#   party_simplified &lt;chr&gt;\n\n\nLet’s take a look at that object. We get some pretty useful information from this print out. From it we learn that we have 4,287 rows and 15 columns of data. What does this mean?\nWell, this data set provides us with information for each election from 1976 to 2020. It provides the number of votes cast in each state for each presidential candidate in each of those races. Therefore, each row provides us with data for each candidate in each state and in each election. For example, the first row of our data set provides information on…\n\nslice(prev_state_lvl_results, 1)\n\n# A tibble: 1 × 15\n   year state   state_po state_fips state_cen state_ic office       candidate   \n  &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;       \n1  1976 ALABAMA AL                1        63       41 US PRESIDENT CARTER, JIM…\n# ℹ 7 more variables: party_detailed &lt;chr&gt;, writein &lt;lgl&gt;,\n#   candidatevotes &lt;dbl&gt;, totalvotes &lt;dbl&gt;, version &lt;dbl&gt;, notes &lt;lgl&gt;,\n#   party_simplified &lt;chr&gt;\n\n\nJimmy Carter in the 1976 Alabama race.\n\n\n\n\n\n\nExercise\n\n\n\nWhich race, state, and candidate does the 100th row refer to?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nslice(prev_state_lvl_results, 100)\n\n# A tibble: 1 × 15\n   year state  state_po state_fips state_cen state_ic office       candidate    \n  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;        \n1  1976 KANSAS KS               20        47       32 US PRESIDENT MACBRIDE, RO…\n# ℹ 7 more variables: party_detailed &lt;chr&gt;, writein &lt;lgl&gt;,\n#   candidatevotes &lt;dbl&gt;, totalvotes &lt;dbl&gt;, version &lt;dbl&gt;, notes &lt;lgl&gt;,\n#   party_simplified &lt;chr&gt;\n\n\nRoger Macbride’s race in Kansas in 1976.\n\n\n\nWe refer to this as our unit of observation. More succinctly, I would say that our unit of observation is candidate-state-election year. We have information on 4,287 different candidate-state-election years.\nHow much information do we have on each of these candidate-state-election years? This data set includes 15 variables. They include the election year (year), the candidate’s name (candidate), their party (party_detailed), the number of votes they received in each state (candidarevotes), and the total number of votes cast in that state (totalvotes).\nYou get a quick summary of these data using the function glimpse():\n\nglimpse(prev_state_lvl_results)\n\nRows: 4,287\nColumns: 15\n$ year             &lt;dbl&gt; 1976, 1976, 1976, 1976, 1976, 1976, 1976, 1976, 1976,…\n$ state            &lt;chr&gt; \"ALABAMA\", \"ALABAMA\", \"ALABAMA\", \"ALABAMA\", \"ALABAMA\"…\n$ state_po         &lt;chr&gt; \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AK\", \"AK\",…\n$ state_fips       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4,…\n$ state_cen        &lt;dbl&gt; 63, 63, 63, 63, 63, 63, 63, 94, 94, 94, 94, 86, 86, 8…\n$ state_ic         &lt;dbl&gt; 41, 41, 41, 41, 41, 41, 41, 81, 81, 81, 81, 61, 61, 6…\n$ office           &lt;chr&gt; \"US PRESIDENT\", \"US PRESIDENT\", \"US PRESIDENT\", \"US P…\n$ candidate        &lt;chr&gt; \"CARTER, JIMMY\", \"FORD, GERALD\", \"MADDOX, LESTER\", \"B…\n$ party_detailed   &lt;chr&gt; \"DEMOCRAT\", \"REPUBLICAN\", \"AMERICAN INDEPENDENT PARTY…\n$ writein          &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE…\n$ candidatevotes   &lt;dbl&gt; 659170, 504070, 9198, 6669, 1954, 1481, 308, 71555, 4…\n$ totalvotes       &lt;dbl&gt; 1182850, 1182850, 1182850, 1182850, 1182850, 1182850,…\n$ version          &lt;dbl&gt; 20210113, 20210113, 20210113, 20210113, 20210113, 202…\n$ notes            &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ party_simplified &lt;chr&gt; \"DEMOCRAT\", \"REPUBLICAN\", \"OTHER\", \"OTHER\", \"OTHER\", …\n\n\n\nWhat are those three-letter abbreviations next to the column names? They are the data type. Data types really matter, particularly when running statistical analysis in R. Here we have three different types of data:\n\n\n\n\nData type\nDescription\n\n\n\ndbl\nDouble, or numbers with decimals (1.0, 33.5, 7.67)\n\n\nchr\nCharacter, or letters, words, sentences (“A”, “cat”, “5”)\n\n\nlgl\nLogical (TRUE, FALSE)\n\n\n\n\n\nIn an effort not to be too boring, we will talk about data types as we walk through working with data (today and over the coming weeks). It is important to note now that each column can only contain one data type. For example, a column that contains doubles (or numbers) cannot also include characters (or words). This makes working with data a lot easier. Imagine trying to find the average of a column and including a character in that calculation!\n\nWe have data on the total number of people in each state who voted for each candidate and the total number of people who voted in each state. We want to find the proportion of US voters who voted for President Biden in the 2020 election. How do we get from here to there?\nLet’s start by calculating the total number of US voters nationally who voted for each candidate in each election and the total number of people who voted in each election nationally. To do this, we need to aggregate from the state-level to the national-level.\nWe are going to use group_by() to group our data together by candidate and election year. That will allow us to calculate the sum (using the aptly named sum() function) for each candidate in each election.\nBecause we are aggregating our data, we use the summarize() (or summarise(), if you aren’t American!) to return only one observation (or row) for each candidate in each election.\n\n\n\n\n\n\nExercise\n\n\n\nWhat will be our new unit of observation?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nCandidate-election year.\n\n\n\n\nprev_nat_lvl_results &lt;- prev_state_lvl_results |&gt; \n  # Divide the dataset up by election year and candidate\n  group_by(year, candidate) |&gt; \n  # Calculate the total number of voters who voted in each of these groups\n  summarise(nationalcandidatevotes = sum(candidatevotes), \n            nationaltotalvotes = sum(totalvotes)) |&gt; \n  # Ungroup our data\n  ungroup()\n\nprev_nat_lvl_results\n\n# A tibble: 370 × 4\n    year candidate                     nationalcandidatevotes nationaltotalvotes\n   &lt;dbl&gt; &lt;chr&gt;                                          &lt;dbl&gt;              &lt;dbl&gt;\n 1  1976 \"ANDERSON, THOMAS J.\"                         147835           28498006\n 2  1976 \"BUBAR, BENJAMIN \\\"\\\"BEN\\\"\\\"\"                  15888           13255917\n 3  1976 \"CAMEJO, PETER\"                                89737           59540602\n 4  1976 \"CARTER, JIMMY\"                             40825839           88269606\n 5  1976 \"FORD, GERALD\"                              39145771           88269606\n 6  1976 \"HALL, GUS\"                                    57721           43458746\n 7  1976 \"JULIUS \\\"\\\"JULES\\\"\\\", LEVIN\"                   9229           25681673\n 8  1976 \"LAROUCHE, LYNDON, JR.\"                        39148           46223231\n 9  1976 \"MACBRIDE, ROGER\"                             171266           55303046\n10  1976 \"MADDOX, LESTER\"                              168623           35723586\n# ℹ 360 more rows\n\n\nI have stored this new data set as an object called prev_nat_lvl_results. If you look over to your environment (in the box that is probably located in the top right hand corner of your RStudio), you should now see two objects: prev_state_lvl_results and prev_nat_lvl_results.\nYou might have noticed that this new data set is smaller than the state-level one. This is good! We now have an aggregate count of the number of US voters across the US who voted for each candidate in each election.\nWhat do we find? The first row tells us that across the entire US, Thomas J. Anderson received only 147,835 votes in the 1976 US Presidential Election.\nWho received the most votes that year?\n\nprev_nat_lvl_results |&gt; \n  # Only include data from the 1976 election\n  filter(year == 1976) |&gt; \n  # Find the maximum number of votes cast for a candidate in that election\n  slice_max(nationalcandidatevotes)\n\n# A tibble: 1 × 4\n   year candidate     nationalcandidatevotes nationaltotalvotes\n  &lt;dbl&gt; &lt;chr&gt;                          &lt;dbl&gt;              &lt;dbl&gt;\n1  1976 CARTER, JIMMY               40825839           88269606\n\n\nJimmy Carter, who received 40,825,839 votes.\nHow did each candidate do in this election? We can easily answer this question by visualizing our data:\n\nprev_nat_lvl_results |&gt; \n  # Only include the 1976 Presidential Election\n  filter(year == 1976) |&gt; \n  # Plot votes cast by candidate\n  ggplot(aes(x = nationalcandidatevotes / 1e06, y = candidate)) + \n  geom_col() + \n  # Make the plot more readable\n  theme_minimal() + \n  theme(plot.title.position = \"plot\",\n        plot.title = element_text(face = \"bold\")) +\n  # Include informative labels\n  labs(x = \"Number of votes cast (millions)\",\n       y = \"Candidate\",\n       title = \"Total votes cast nationally for each candidate in the 1976 Presidential Election\",\n       caption = \"Source: MIT Election Lab (2023)\")\n\n\n\n\nggplot() has (by default) ordered our y-axis by reverse alphabethical order. Let’s use a more useful order: the number of votes cast for each candidate. Let’s also get rid of those votes cast for no candidate (NA).\n\nprev_nat_lvl_results |&gt; \n  filter(year == 1976) |&gt; \n  drop_na(candidate) |&gt; \n  ggplot(aes(x = nationalcandidatevotes / 1e06, y = reorder(candidate, nationalcandidatevotes))) + \n  geom_col() + \n  theme_minimal() + \n  theme(plot.title.position = \"plot\",\n        plot.title = element_text(face = \"bold\")) +\n  labs(x = \"Number of votes cast (millions)\",\n       y = \"Candidate\",\n       title = \"Total votes cast nationally for each candidate in the 1976 Presidential Election\",\n       caption = \"Source: MIT Election Lab (2023)\")\n\n\n\n\nAs expected, the Democratic and Republican candidates received the vast majority of votes cast. What proportion of votes did they receive?\nLet’s go back to our full data set:\n\nprev_nat_lvl_results\n\n# A tibble: 370 × 4\n    year candidate                     nationalcandidatevotes nationaltotalvotes\n   &lt;dbl&gt; &lt;chr&gt;                                          &lt;dbl&gt;              &lt;dbl&gt;\n 1  1976 \"ANDERSON, THOMAS J.\"                         147835           28498006\n 2  1976 \"BUBAR, BENJAMIN \\\"\\\"BEN\\\"\\\"\"                  15888           13255917\n 3  1976 \"CAMEJO, PETER\"                                89737           59540602\n 4  1976 \"CARTER, JIMMY\"                             40825839           88269606\n 5  1976 \"FORD, GERALD\"                              39145771           88269606\n 6  1976 \"HALL, GUS\"                                    57721           43458746\n 7  1976 \"JULIUS \\\"\\\"JULES\\\"\\\", LEVIN\"                   9229           25681673\n 8  1976 \"LAROUCHE, LYNDON, JR.\"                        39148           46223231\n 9  1976 \"MACBRIDE, ROGER\"                             171266           55303046\n10  1976 \"MADDOX, LESTER\"                              168623           35723586\n# ℹ 360 more rows\n\n\nYou may have noticed that we have different numbers indicating the total number of votes cast nationally in each election (nationaltotalvotes). In 1976 alone we have 15 different answers ranging from 5,552,843 to 88,269,606 to the question “how many people voted in this election nationally?”. We should have one answer! What is going on?\n\n\n\n\n\n\nExercise\n\n\n\nBefore you read on, see if you can answer this question. Not everyone is an Americanist (or American), so I will give you some hints:\n\nNotice how the number recorded in that nationaltotalvotes column for the Democractic candidate (Jimmy Carter) and Republican candidate (Gerald Ford) are the same.\nNotice also that all other candidates have smaller numbers recorded in nationaltotalvotes.\nFinally, think back to the unit of observation with which we started: state-candidate-election year.\n\n\n\nWhy do we get several different answers to the question “how many people voted in the national election?” when we calculate the sum total of people who voted in every state? Because not all candidates were included in all states’ ballots. The independents and smaller party candidates did not run in several states. The only two candidates to run in all 50 states and DC were the Democratic and Republican ones.\nTo double check this, we can count the number of states in which each candidate ran. We go back to our state-level results:\n\nprev_state_lvl_results |&gt; \n  filter(year == 1976) |&gt;\n  distinct(candidate, state) |&gt; \n  group_by(candidate) |&gt; \n  count()\n\n# A tibble: 16 × 2\n# Groups:   candidate [16]\n   candidate                           n\n   &lt;chr&gt;                           &lt;int&gt;\n 1 \"ANDERSON, THOMAS J.\"              20\n 2 \"BUBAR, BENJAMIN \\\"\\\"BEN\\\"\\\"\"      10\n 3 \"CAMEJO, PETER\"                    27\n 4 \"CARTER, JIMMY\"                    51\n 5 \"FORD, GERALD\"                     51\n 6 \"HALL, GUS\"                        18\n 7 \"JULIUS \\\"\\\"JULES\\\"\\\", LEVIN\"      11\n 8 \"LAROUCHE, LYNDON, JR.\"            22\n 9 \"MACBRIDE, ROGER\"                  32\n10 \"MADDOX, LESTER\"                   21\n11 \"MCCARTHY, EUGENE \\\"\\\"GENE\\\"\\\"\"    28\n12 \"OTHER\"                             8\n13 \"SCATTERING\"                        4\n14 \"WRIGHT, MARGARET\"                  4\n15 \"ZEIDLER, FRANK\"                    6\n16  &lt;NA&gt;                              14\n\n\nOr in visual form:\n\nprev_state_lvl_results |&gt; \n  filter(year == 1976) |&gt; \n  distinct(candidate, state) |&gt; \n  group_by(candidate) |&gt; \n  count() |&gt; \n  ggplot(aes(x = n, y = reorder(candidate, n))) + \n  geom_col() + \n  theme_minimal() + \n  theme(plot.title.position = \"plot\",\n        plot.title = element_text(face = \"bold\")) + \n  labs(x = \"Number of states\",\n       y = \"Candidate\",\n       title = \"Number of states in which each candidate ran in the 1976 US Presidential Election\")\n\n\n\n\nSo, when we sum together the total number of votes cast in each state in which each candidate ran, we get different answers depending on the number of states in which each candidate ran.\nThe lesson here is that you need to really know your data and your subject matter. Your knowledge as a political scientist is so relevant to your empirical work.\nSo, going forward, I am going to assume that the total number of votes cast in each election is reflected by the total number of votes cast in all of the states in which either the Republican or Democratic candidate ran. This is because I assume that these parties ran in every state in every election. Is this assumption valid? Let’s use the data to find out!\nWe know that there are 51 states or territories in which votes are cast for the US President. This number has not changed between 1976 and 2020. So, we need to confirm that both the Republican and Democractic candidates ran in 51 states and territories every election.\n\nprev_state_lvl_results |&gt; \n  # Only include the official D or R candidates\n  filter(party_simplified %in% c(\"DEMOCRAT\", \"REPUBLICAN\")) |&gt; \n  distinct(year, state) |&gt; \n  count(year)\n\n# A tibble: 12 × 2\n    year     n\n   &lt;dbl&gt; &lt;int&gt;\n 1  1976    51\n 2  1980    51\n 3  1984    51\n 4  1988    51\n 5  1992    51\n 6  1996    51\n 7  2000    51\n 8  2004    51\n 9  2008    51\n10  2012    51\n11  2016    51\n12  2020    51\n\n\nYes! We know that the total number of votes cast in states and territories in which the Republican or Democrat ran represents the total number of votes cast nationally. They ran in every state or territory in which votes were cast for the US President in all elections between 1976 and 2020.\nOkay, let’s get that national total for each election:\n\nnational_total_votes &lt;- prev_state_lvl_results |&gt; \n  # Only need one candidate's data \n  filter(party_simplified == \"DEMOCRAT\") |&gt;\n  # Calculate the national total for each election\n  group_by(year) |&gt; \n  summarise(nationaltotalvotes = sum(totalvotes))\n\nnational_total_votes\n\n# A tibble: 12 × 2\n    year nationaltotalvotes\n   &lt;dbl&gt;              &lt;dbl&gt;\n 1  1976           81601344\n 2  1980           86496851\n 3  1984           92654861\n 4  1988           91586825\n 5  1992          104599780\n 6  1996           96389818\n 7  2000          105593982\n 8  2004          124733688\n 9  2008          131419253\n10  2012          129139997\n11  2016          142141798\n12  2020          158528503\n\n\nOr visually:\n\nggplot(national_total_votes, aes(x = year, y = nationaltotalvotes / 1e6)) + \n  geom_line() + \n  geom_point() + \n  theme_minimal() + \n  theme(plot.title.position = \"plot\",\n        plot.title = element_text(face = \"bold\")) + \n  labs(x = \"Year\",\n       y = \"Number of votes cast (millions)\",\n       title = \"Total number of votes cast in US Presidential Elections, 1976-2020\",\n       caption = \"Source: MIT Election Lab (2023)\")\n\n\n\n\nWe can now update our national-level data to reflect the true total number of people who voted nationally:\n\nprev_nat_lvl_results &lt;- prev_nat_lvl_results |&gt; \n  # Remove the inconsistent data\n  select(-nationaltotalvotes) |&gt;\n  # Merge with the true national total\n  left_join(national_total_votes, by = join_by(year))\n\nprev_nat_lvl_results\n\n# A tibble: 370 × 4\n    year candidate                     nationalcandidatevotes nationaltotalvotes\n   &lt;dbl&gt; &lt;chr&gt;                                          &lt;dbl&gt;              &lt;dbl&gt;\n 1  1976 \"ANDERSON, THOMAS J.\"                         147835           81601344\n 2  1976 \"BUBAR, BENJAMIN \\\"\\\"BEN\\\"\\\"\"                  15888           81601344\n 3  1976 \"CAMEJO, PETER\"                                89737           81601344\n 4  1976 \"CARTER, JIMMY\"                             40825839           81601344\n 5  1976 \"FORD, GERALD\"                              39145771           81601344\n 6  1976 \"HALL, GUS\"                                    57721           81601344\n 7  1976 \"JULIUS \\\"\\\"JULES\\\"\\\", LEVIN\"                   9229           81601344\n 8  1976 \"LAROUCHE, LYNDON, JR.\"                        39148           81601344\n 9  1976 \"MACBRIDE, ROGER\"                             171266           81601344\n10  1976 \"MADDOX, LESTER\"                              168623           81601344\n# ℹ 360 more rows\n\n\nThat looks better!\nNow we have all of the information we need to calculate the proportion of US voters who voted for each candidate in each election:\n\nprev_nat_lvl_results &lt;- mutate(prev_nat_lvl_results, \n                               prop_votes = nationalcandidatevotes / nationaltotalvotes)\n\nprev_nat_lvl_results\n\n# A tibble: 370 × 5\n    year candidate          nationalcandidatevotes nationaltotalvotes prop_votes\n   &lt;dbl&gt; &lt;chr&gt;                               &lt;dbl&gt;              &lt;dbl&gt;      &lt;dbl&gt;\n 1  1976 \"ANDERSON, THOMAS…                 147835           81601344   0.00181 \n 2  1976 \"BUBAR, BENJAMIN …                  15888           81601344   0.000195\n 3  1976 \"CAMEJO, PETER\"                     89737           81601344   0.00110 \n 4  1976 \"CARTER, JIMMY\"                  40825839           81601344   0.500   \n 5  1976 \"FORD, GERALD\"                   39145771           81601344   0.480   \n 6  1976 \"HALL, GUS\"                         57721           81601344   0.000707\n 7  1976 \"JULIUS \\\"\\\"JULES…                   9229           81601344   0.000113\n 8  1976 \"LAROUCHE, LYNDON…                  39148           81601344   0.000480\n 9  1976 \"MACBRIDE, ROGER\"                  171266           81601344   0.00210 \n10  1976 \"MADDOX, LESTER\"                   168623           81601344   0.00207 \n# ℹ 360 more rows\n\n\nAnd finally, we can answer our original question: what proportion of US voters voted for President Biden in the last US Presidential election?\n\nprev_nat_lvl_results |&gt; \n  filter(year == 2020, candidate == \"BIDEN, JOSEPH R. JR\")\n\n# A tibble: 1 × 5\n   year candidate           nationalcandidatevotes nationaltotalvotes prop_votes\n  &lt;dbl&gt; &lt;chr&gt;                                &lt;dbl&gt;              &lt;dbl&gt;      &lt;dbl&gt;\n1  2020 BIDEN, JOSEPH R. JR               81268908          158528503      0.513\n\n\nSo, following my logic set up above, my best guess of the proportion of US voters who will vote for President Biden in the 2024 US Presidential election is 0.513, or 51.3%.\nAre you skeptical of my assumptions? Why? Do you think my logic is sound? If not, how could you improve it? What additional information would you need to provide a better guess? How will you use that additional information to inform your better guess?\nWelcome to your PhD in political science!"
  },
  {
    "objectID": "content/10-applications.html",
    "href": "content/10-applications.html",
    "title": "Applications & Midterm Exam Review II",
    "section": "",
    "text": "install.packages(\"peacesciencer\")\n\n\nlibrary(tidyverse)\nlibrary(peacesciencer)\nlibrary(ggdist)\nlibrary(MetBrewer)\nlibrary(scales)\nlibrary(janitor)\nlibrary(wbstats)\nlibrary(countrycode)\nlibrary(modelsummary)\n\nToday, we are going to revise what we have learnt about hypothesis testing by working through some relevant examples.\n\n\nWe know that militarized conflict between states is a very rare event: most country pairings are at peace most of the time. Was the average number of militarized interstate disputes (MIDs) fought between states in 2014 different from 0.0036?\n\nmid_df &lt;- create_dyadyears(directed = F, subset_years = 2014) |&gt; \n  add_cow_mids() |&gt; \n  select(year, ccode1, ccode2, cowmidongoing)\n\nmid_df |&gt; \n  filter(cowmidongoing == 1)\n\n# A tibble: 55 × 4\n    year ccode1 ccode2 cowmidongoing\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;\n 1  2014      2    365             1\n 2  2014      2    652             1\n 3  2014      2    700             1\n 4  2014      2    710             1\n 5  2014      2    731             1\n 6  2014      2    770             1\n 7  2014     20    365             1\n 8  2014    200    230             1\n 9  2014    200    365             1\n10  2014    210    365             1\n# ℹ 45 more rows\n\n\nWhat was the observed average number of MIDs fought between states in 2014?\n\navg_mid &lt;- mean(mid_df$cowmidongoing)\navg_mid\n\n[1] 0.002907745\n\n\nIs this difference significant, or simply the product of random noise? What would a world in which the average number of MIDs between states in 2014 was 0.0036 look like?\n\n\n\n\n\n\nNote\n\n\n\nRemember, our null world is represented by the t-distribution. We center our null world at 0, allowing this to represent our null hypothesis. We work out how certain we are of our statistic using the amount of information we have (represented by the degrees of freedom). Next, we will work out how far away from this centerpoint our observed statistic sits.\n\n\nWhat are our degrees of freedom?\n\ndf &lt;- nrow(mid_df) - 1\ndf\n\n[1] 18914\n\n\nWe can use these degrees of freedom to build our null world:\n\nggplot(tibble(x = rt(1e6, df)), aes(x = x)) + \n  stat_slab() + \n  theme_minimal() + \n  theme(legend.position = \"none\")\n\n\n\n\nThis is what the standardized average number of MIDs pulled from one million samples drawn from our hypothetical null world would look like. The only reason they are not all equal to 0 is random chance.\nWe observed an average number of MIDs of 0.003 in 2014. If the null hypothesis were true, how likely would we be to see this average number of MIDs if we pulled a random sample from that null world?\nFirst, we need to set a threshold at which we are happy to accept the risk that we reject a true null hypothesis. Let’s use the standard 5%.\nNext, we need to work out where the remaining 95% of these null world means fall. I am going to focus on where 95% of these means fall around the null statistic (of 0). Because we are working with the standardized t-distribution, we can use what we know about that distribution to answer this question.\n\n\n\n\n\n\nTip\n\n\n\nThe base function qt() gives us the t-statistic that sits at a given probability.\nThis is simply the area under the t-distribution curve, which represents the proportion of hypothetical t-statistics that are equal to or more extreme than our given point. For example, we want to find the t-statistic corresponding to the point beyond which 97.5% of all possible t-statistics fall. See lower_boundary and the graph below.\n\n\n\nlower_boundary &lt;- qt(0.025, df = df, lower.tail = T)\nlower_boundary\n\n[1] -1.960089\n\nupper_boundary &lt;- qt(0.025, df = df, lower.tail = F)\nupper_boundary\n\n[1] 1.960089\n\n\n\nggplot(tibble(x = rt(1e6, df)), aes(x = x)) + \n  stat_slab(aes(fill_ramp = after_stat(x &lt; lower_boundary | x &gt; upper_boundary)),\n            fill = met.brewer(\"Egypt\")[2]) + \n  theme_minimal() + \n  theme(legend.position = \"none\")\n\n\n\n\nWhere does our observed mean fall in relation to this null world? First, we need to translate our observed mean into its corresponding t-statistic:\n\\[\nt = \\frac{\\bar{x} - \\mu_0}{\\frac{s}{\\sqrt{n}}}\n\\]\n\n\n\n\n\n\nNote\n\n\n\nAbove, we centered our null world at 0. So, we need to translate our observed mean into its distance from the null hypothesis mean (represented by 0).\n\n\nWe know our observed mean, \\(\\bar{x}\\):\n\navg_mid\n\n[1] 0.002907745\n\n\nAnd our null mean, \\(\\mu_0\\):\n\nnull_mean &lt;- 0.0036\nnull_mean\n\n[1] 0.0036\n\n\nAnd our sample size, \\(n\\):\n\nsample_size &lt;- nrow(mid_df)\nsample_size\n\n[1] 18915\n\n\nAnd our sample standard deviation, \\(s\\):\n\nsample_sd &lt;- sd(mid_df$cowmidongoing)\nsample_sd\n\n[1] 0.05384648\n\n\nTherefore, our translated t-statistic is:\n\nt_stat &lt;- (avg_mid - null_mean) / (sample_sd / sqrt(sample_size))\nt_stat\n\n[1] -1.76812\n\n\nWhere does this sit within our null world?\n\nggplot(tibble(x = rt(1e6, df)), aes(x = x)) + \n  stat_slab(aes(fill_ramp = after_stat(x &lt; lower_boundary | x &gt; upper_boundary)),\n            fill = met.brewer(\"Egypt\")[2]) + \n  geom_vline(xintercept = t_stat) + \n  theme_minimal() + \n  theme(legend.position = \"none\")\n\n\n\n\nIn other worlds, if the null hypothesis were true, how likely would we be to draw a SRS that was as extreme or more extreme than the one we observed?\n\n2*pt(t_stat, df = df)\n\n[1] 0.07705689\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe base function pt() gives is the probability of observing a value equal to or as extreme as the t-statistic we provided. It does the opposite of qt().\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe need to double this probability because it is only providing us the p-value that corresponds with the first t-statistic (the dark blue area shaded to the left of the graph above).\n\n\nIf the null hypothesis were true, we would observe an average number of MIDs between states of 0.003 in 7.71% of an infinite number of samples from that null world. Therefore, we cannot reject the null hypothesis that the average number of MIDs between states in 2014 was 0.0036 with a two-tailed test at the 5% threshold.\n\n\nWhat if our hypothesis was directional? What if I believe that the average number of MIDs between states is less than 0.0036?\nWhen we talk about the direction of our significance test, we are talking about those boundaries around the null hypothesis. I am still only willing to accept a 5% chance that I reject the null hypothesis when it is in fact true. However, I no longer need to split that 5% chance evenly above and below the null hypothesis. Instead, I can place that whole 5% either above or below my null hypothesis (depending on the direction of my hypothesis).\nIn this example, I think that the average number of MIDs between states is less than 0.0036. If my sample has an average number of MIDs greater than 0.0036, I should more readily reject my alternative hypothesis that the average number of MIDs is smaller than 0.0036. Therefore, I can concentrate my 5% threshold below the null hypothesis:\n\nlower_boundary &lt;- qt(0.05, df = df, lower.tail = T)\nlower_boundary\n\n[1] -1.644934\n\n\nVisually:\n\nggplot(tibble(x = rt(1e6, df)), aes(x = x)) + \n  stat_slab(aes(fill_ramp = after_stat(x &lt; lower_boundary)),\n            fill = met.brewer(\"Egypt\")[2]) + \n  theme_minimal() + \n  theme(legend.position = \"none\")\n\n\n\n\nRemember, I cannot reject the null hypothesis if it is plausible (there is a 95% chance) that I would observe the mean that I did even if the null hypothesis were true. That 95% is represented by the light blue area on the graph above. The dark blue shaded area is still highlighting where the average count of MIDs drawn from 5% of all possible samples pulled from the null world would fall.\nOur observed average has not changed. Let’s put it in this new context:\n\nggplot(tibble(x = rt(1e6, df)), aes(x = x)) + \n  stat_slab(aes(fill_ramp = after_stat(x &lt; lower_boundary)),\n            fill = met.brewer(\"Egypt\")[2]) + \n  geom_vline(xintercept = t_stat) + \n  theme_minimal() + \n  theme(legend.position = \"none\")\n\n\n\n\nIt is unlikely that we would observe this average number of MIDs if, in fact, the null hypothesis were true. How unlikely?\n\npt(t_stat, df = df)\n\n[1] 0.03852845\n\n\nIf the null hypothesis were true, we would observe an average number of MIDs of 0.003 in 3.85% of an infinite number of samples from that null world. Therefore, we can reject the null hypothesis that the average number of MIDs between states in 2014 was 0.0036 with a one-tailed test at the 5% threshold.\n\n\n\n\nAre the majority of states in the world democracies?\n\ndem_df &lt;- ccode_democracy |&gt; \n  drop_na(polity2) |&gt; \n  slice_max(year) |&gt; \n  transmute(ccode, democracy = if_else(polity2 &gt; 5, 1, 0))\n\ndem_df\n\n# A tibble: 166 × 2\n   ccode democracy\n   &lt;dbl&gt;     &lt;dbl&gt;\n 1     2         1\n 2    20         1\n 3    40         0\n 4    41         0\n 5    42         1\n 6    51         1\n 7    52         1\n 8    70         1\n 9    90         1\n10    91         0\n# ℹ 156 more rows\n\n\nWhat proportion of states were democracies in 2017?\n\ntabyl(dem_df, democracy)\n\n democracy  n   percent\n         0 70 0.4216867\n         1 96 0.5783133\n\n\nIs this proportion significantly different from 0.5, or is it simply the product of random noise?\nTo answer this question, we will set up our null world:\n\nggplot(tibble(x = rnorm(1e6)), aes(x = x)) + \n  stat_slab() + \n  theme_minimal() + \n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nRemember, we use the normal distribution when examining the difference of proportions.\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe base function rnorm() gives us random values drawn from a normal distribution. By default, rnorm() draws from a normal distribution centered at 0 and with a standard deviation of 1. This is perfect for working with z-scores.\n\n\nThis is what the standardized proportion of democracies are for 1 million samples drawn from our null world.\nLet’s see whether we can reject the null hypothesis that the proportion of democracies globally is 0.5 or 50%. I am willing to accept a 5% risk that I reject this null hypothesis when it is, in fact, true. So I now need to work out where 95% of plausible proportions fall in samples drawn from a world in which the proportion of democracies is 0.5.\n\nlower_boundary &lt;- qnorm(0.025)\nlower_boundary\n\n[1] -1.959964\n\nupper_boundary &lt;- qnorm(0.025, lower.tail = F)\nupper_boundary\n\n[1] 1.959964\n\n\nLet’s see this:\n\nggplot(tibble(x = rnorm(1e6)), aes(x = x)) + \n  stat_slab(aes(fill_ramp = after_stat(x &lt; lower_boundary | x &gt; upper_boundary)),\n            fill = met.brewer(\"Egypt\")[2]) + \n  theme_minimal() + \n  theme(legend.position = \"none\")\n\n\n\n\nSo, if our observed statistic sits so far from the null hypothesis that there is less than a 5% chance that I could draw a sample with its proportion of democracies, then we have sufficient evidence to reject the null hypothesis at this threshold. We just need to work out where our observed statistic sits on this standardized distribution.\nTo do this, we need to translate our observed statistic into its z-score:\n\\[\nz = \\frac{\\hat{p} - p_0}{\\sqrt{\\frac{p_0(1-p_0)}{n}}}\n\\]\nWe know our observed proportion, \\(\\hat{p}\\):\n\nsample_proportion &lt;- dem_df |&gt; \n  tabyl(democracy) |&gt; \n  filter(democracy == 1) |&gt; \n  pull(percent)\n\nsample_proportion\n\n[1] 0.5783133\n\n\nAnd our null proportion, \\(p_0\\):\n\nnull_proportion &lt;- 0.5\nnull_proportion\n\n[1] 0.5\n\n\nAnd our sample size, \\(n\\):\n\nsample_size &lt;- nrow(dem_df)\nsample_size\n\n[1] 166\n\n\nNow we can translate our observed proportion into its distance from the null proportion:\n\nz_score &lt;- (sample_proportion - null_proportion) / sqrt((null_proportion * (1 - null_proportion)) / sample_size)\nz_score\n\n[1] 2.017991\n\n\nThis z-score represents how far from the null proportion our observed proportion sits. Let’s take a look:\n\nggplot(tibble(x = rnorm(1e6)), aes(x = x)) + \n  stat_slab(aes(fill_ramp = after_stat(x &lt; lower_boundary | x &gt; upper_boundary)),\n            fill = met.brewer(\"Egypt\")[2]) + \n  geom_vline(xintercept = z_score) + \n  theme_minimal() + \n  theme(legend.position = \"none\")\n\n\n\n\nIn fact, our observed statistic is sufficiently far from the null hypothesis. We can reject that null hypothesis with 95% confidence.\n\n2*pnorm(z_score, lower.tail = F)\n\n[1] 0.04359216\n\n\nThere is a 4% chance that we would observe a proportion of democracies that was either this far above or below a proportion of 0.5 if the true proportion of democracies was 0.5. This is less than the threshold by which I am willing to reject a true null hypothesis. Therefore, I have sufficient evidence to reject the null hypothesis.\n\n\n\nAre democracies richer, on average, than non-democracies?\n\ngdp_df &lt;- wb_data(\"NY.GDP.MKTP.CD\", \n                  start_date = 2017, \n                  end_date = 2017, \n                  return_wide = F) |&gt; \n  transmute(ccode = countrycode(country, \"country.name\", \"cown\"),\n            gdp = value)\n\ndem_gdp_df &lt;- dem_df |&gt; \n  left_join(gdp_df) |&gt; \n  drop_na(gdp)\n\nWhat is the average GDP of democracies and non-democracies?\n\ndem_gdp_summary &lt;- dem_gdp_df |&gt; \n  group_by(democracy) |&gt; \n  summarise(n = n(), \n            avg_gdp = mean(gdp)) |&gt; \n  mutate(diff_avg_gdp = avg_gdp - lag(avg_gdp))\n\ndem_gdp_summary\n\n# A tibble: 2 × 4\n  democracy     n       avg_gdp  diff_avg_gdp\n      &lt;dbl&gt; &lt;int&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1         0    66 311030589116.           NA \n2         1    94 631669975302. 320639386186.\n\n\nIt appears that we have support for our hypothesis that democracies are richer than non-democracies on average. How much richer?\n\ndiff_avg &lt;- dem_gdp_summary |&gt; \n  filter(democracy == 1) |&gt; \n  pull(diff_avg_gdp)\n\ndollar(diff_avg)\n\n[1] \"$320,639,386,186\"\n\n\nHowever, we need to confirm that this difference is not the product of random chance.\nFirst, we need to build our null world. In this world, there is no difference in the average wealth of democracies and non-democracies. We will center our null world at 0. We can use our degrees of freedom to build in uncertainty around that central point.\n\n\n\n\n\n\nNote\n\n\n\nThe degrees of freedom for the difference of means is equal to the smaller of the number of observations within each group minus one.\n\n\n\nn_democracies &lt;- dem_gdp_summary |&gt; \n  filter(democracy == 1) |&gt; \n  pull(n)\n\nn_democracies\n\n[1] 94\n\nn_non_democracies &lt;- dem_gdp_summary |&gt; \n  filter(democracy == 0) |&gt; \n  pull(n)\n\nn_non_democracies\n\n[1] 66\n\n\nOur degrees of freedom are equal to the smaller of these two, minus one:\n\ndf &lt;- min(n_democracies, n_non_democracies) - 1\ndf\n\n[1] 65\n\n\nNow we have all the information that we need to build our null world:\n\nggplot(tibble(x = rt(1e6, df)), aes(x = x)) + \n  stat_slab() + \n  theme_minimal() + \n  theme(legend.position = \"none\")\n\n\n\n\nI am now happy to accept a 10% risk that I will reject a true null hypothesis. However, I will continue to work with two-tailed tests. Let’s work out where 90% of our hypothetical differences of means sit around our null mean of 0.\n\nlower_boundary &lt;- qt(0.05, df = df)\nlower_boundary\n\n[1] -1.668636\n\nupper_boundary &lt;- qt(0.05, df = df, lower.tail = F)\nupper_boundary\n\n[1] 1.668636\n\n\nLet’s visualize those boundaries:\n\nggplot(tibble(x = rt(1e6, df)), aes(x = x)) + \n  stat_slab(aes(fill_ramp = after_stat(x &lt; lower_boundary | x &gt; upper_boundary)),\n            fill = met.brewer(\"Egypt\")[2]) + \n  theme_minimal() + \n  theme(legend.position = \"none\")\n\n\n\n\nWhere does our observed difference of means sit along this standardized distribution? To answer this question, we need to translate our observed difference of $320,639,386,186 into its t-statistic:\n\\[\nt = \\frac{\\bar{x_1} - \\bar{x_2}}{\\sqrt{\\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2}}}\n\\] We know our difference of means, \\(\\bar{x_1} - \\bar{x_2}\\):\n\ndollar(diff_avg)\n\n[1] \"$320,639,386,186\"\n\n\nAnd our sample sizes, \\(n_1\\) and \\(n_2\\):\n\nn_democracies\n\n[1] 94\n\nn_non_democracies\n\n[1] 66\n\n\nAnd our standard deviations, \\(s_1\\) and \\(s_2\\):\n\ns_1 &lt;- dem_gdp_df |&gt; \n  filter(democracy == 1) |&gt;\n  pull(gdp) |&gt; \n  sd()\n\ndollar(s_1)\n\n[1] \"$2,149,439,341,731\"\n\ns_2 &lt;- dem_gdp_df |&gt; \n  filter(democracy == 0) |&gt;\n  pull(gdp) |&gt; \n  sd()\n\ndollar(s_2)\n\n[1] \"$1,519,811,642,705\"\n\n\nTherefore, the t-statistic for our observed difference of means is:\n\nt_stat &lt;- diff_avg / sqrt((s_1^2 / n_democracies) + (s_2^2 / n_non_democracies))\nt_stat\n\n[1] 1.105342\n\n\nLet’s place this observed statistic in the context of the null world:\n\nggplot(tibble(x = rt(1e6, df)), aes(x = x)) + \n  stat_slab(aes(fill_ramp = after_stat(x &lt; lower_boundary | x &gt; upper_boundary)),\n            fill = met.brewer(\"Egypt\")[2]) + \n  geom_vline(xintercept = t_stat) + \n  theme_minimal() + \n  theme(legend.position = \"none\")\n\n\n\n\nIt is plausible that we could observe a difference of means equal to $320,639,386,186 even if there is no significant difference of means between the wealth of democracies and non-democracies. In fact, there is the following chance that we would observe this difference:\n\npercent(2 * pt(t_stat, df = df, lower.tail = F))\n\n[1] \"27%\"\n\n\nThis sits well above our threshold of a 10% risk that we would reject a true null hypothesis.\n\n\nWe get the same result using the base function t.test():\n\nt.test(gdp ~ democracy, data = dem_gdp_df)\n\n\n    Welch Two Sample t-test\n\ndata:  gdp by democracy\nt = -1.1053, df = 157.99, p-value = 0.2707\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -893577418352  252298645981\nsample estimates:\nmean in group 0 mean in group 1 \n   311030589116    631669975302 \n\n\n\n\n\n\n\n\nNote\n\n\n\nt.test() finds the difference of means by subtracting the average GDP of democracies from that of non-democracies. That is why the t-statistic is negative.\n\n\n\n\n\n\nIs the proportion of democratic dyads with defense alliances greater than that of non-democratic dyads?\n\nalliance_df &lt;- create_dyadyears(directed = F, subset_years = 2010) |&gt; \n  add_cow_alliance() |&gt; \n  add_democracy() |&gt; \n  transmute(ccode1, \n            ccode2, \n            cow_defense, \n            dem_dyad = if_else(polity21 &gt; 5 & polity22 &gt; 5, 1, 0)) |&gt; \n  drop_na(cow_defense, dem_dyad)\n\nalliance_df\n\n# A tibble: 15,496 × 4\n   ccode1 ccode2 cow_defense dem_dyad\n    &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n 1      2     20           1        1\n 2      2     40           0        0\n 3      2     41           1        0\n 4      2     42           1        1\n 5      2     51           1        1\n 6      2     52           1        1\n 7      2     70           1        1\n 8      2     90           1        1\n 9      2     91           1        1\n10      2     92           1        1\n# ℹ 15,486 more rows\n\n\nWhat proportion of democratic and non-democratic dyads have defense alliances?\n\nalliance_summary &lt;- alliance_df |&gt; \n  group_by(dem_dyad) |&gt; \n  summarise(n = n(),\n            n_defense = sum(cow_defense)) |&gt; \n  mutate(prop = n_defense / n,\n         diff_prop = prop - lag(prop))\n\nalliance_summary\n\n# A tibble: 2 × 5\n  dem_dyad     n n_defense   prop diff_prop\n     &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n1        0 11218       491 0.0438   NA     \n2        1  4278       402 0.0940    0.0502\n\n\nThe proportion of democratic dyads with defense alliances is greater than that of non-democratic dyads. Is this difference statistically significant?\nLet’s set up our null world:\n\nggplot(tibble(x = rnorm(1e6)), aes(x = x)) + \n  stat_slab() + \n  theme_minimal() + \n  theme(legend.position = \"none\")\n\n\n\n\nWe will stick with our two-tailed test at the 0.1 threshold. Let’s find where the other 90% of difference of proportions fall in our null world:\n\nlower_boundary &lt;- qnorm(0.05)\nlower_boundary\n\n[1] -1.644854\n\nupper_boundary &lt;- qnorm(0.05, lower.tail = F)\nupper_boundary\n\n[1] 1.644854\n\n\n\nggplot(tibble(x = rnorm(1e6)), aes(x = x)) + \n  stat_slab(aes(fill_ramp = after_stat(x &lt; lower_boundary | x &gt; upper_boundary)),\n            fill = met.brewer(\"Egypt\")[2]) + \n  theme_minimal() + \n  theme(legend.position = \"none\")\n\n\n\n\nWhere does our observed difference of 5% fall?\nWe need to convert this observed difference into its z-score:\n\\[\nz = \\frac{\\hat{p}_1 - \\hat{p}_2}{SE_{D_p}}\n\\]\nWhere:\n\\[\nSE_{D_p} = \\sqrt{\\hat{p}(1-\\hat{p})(\\frac{1}{n_1} + \\frac{1}{n_2})}\n\\]\nAnd:\n\\[\n\\hat{p} = \\frac{x_1 + x_2}{n_1 + n_2}\n\\]\nWe will take this step-by-step:\nWe know the proportion of democratic and non-democratic dyads that have defense alliances:\n\nalliance_summary\n\n# A tibble: 2 × 5\n  dem_dyad     n n_defense   prop diff_prop\n     &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n1        0 11218       491 0.0438   NA     \n2        1  4278       402 0.0940    0.0502\n\n\nTherefore, we can calculate our pooled estimate: \\(\\hat{p}\\):\n\npooled_p &lt;- sum(alliance_summary$prop)\npooled_p\n\n[1] 0.1377381\n\n\nAnd our sample sizes:\n\nn_dem_dyad &lt;- alliance_summary |&gt; \n  filter(dem_dyad == 1) |&gt; \n  pull(n)\n\nn_dem_dyad\n\n[1] 4278\n\nn_non_dem_dyad &lt;- alliance_summary |&gt; \n  filter(dem_dyad == 0) |&gt; \n  pull(n)\n\nn_non_dem_dyad\n\n[1] 11218\n\n\nWhich allows us to calculate our standard error:\n\nse &lt;- sqrt(pooled_p * (1 - pooled_p) * ((1 / n_dem_dyad) + (1 / n_non_dem_dyad)))\nse\n\n[1] 0.006192675\n\n\nWhich allows us to translate our observed difference into its z-score:\n\ndiff_prop &lt;- alliance_summary |&gt; \n  filter(dem_dyad == 1) |&gt; \n  pull(diff_prop)\n\nz_score &lt;- diff_prop / se\nz_score\n\n[1] 8.106383\n\n\nWhich we can then put into the context of the null world:\n\nggplot(tibble(x = rnorm(1e6)), aes(x = x)) + \n  stat_slab(aes(fill_ramp = after_stat(x &lt; lower_boundary | x &gt; upper_boundary)),\n            fill = met.brewer(\"Egypt\")[2]) + \n  geom_vline(xintercept = z_score) + \n  theme_minimal() + \n  theme(legend.position = \"none\")\n\n\n\n\nWe are very unlikely to have seen a difference in the proportion of democracies and non-democracies with defense alliances of 5% if there were, in fact, no difference in these proportions. How unlikely?\n\n2 * pnorm(z_score, lower.tail = F)\n\n[1] 5.214871e-16\n\n\nVery unlikely!\n\n\n\nLet’s look at this same question, but ask whether democracy shared between countries in a dyad significantly effects the number of defense alliances held by them.\nFirst, we need to find the observed count of defense alliances split between democratic and non-democratic dyads:\n\ndatasummary_crosstab(cow_defense ~ dem_dyad, data = alliance_df)\n\n\n\n\ncow_defense\n\n0\n1\nAll\n\n\n\n\n0\nN\n10727\n3876\n14603\n\n\n\n% row\n73.5\n26.5\n100.0\n\n\n1\nN\n491\n402\n893\n\n\n\n% row\n55.0\n45.0\n100.0\n\n\nAll\nN\n11218\n4278\n15496\n\n\n\n% row\n72.4\n27.6\n100.0\n\n\n\n\n\n\n\nWhat would our cross tab look like if democratic dyads had no effect on the number of defense alliances?\n\n# Calculate the observed count for each category\nobs_values &lt;- count(alliance_df, cow_defense, dem_dyad, name = \"obs_n\")\nobs_values\n\n# A tibble: 4 × 3\n  cow_defense dem_dyad obs_n\n        &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;\n1           0        0 10727\n2           0        1  3876\n3           1        0   491\n4           1        1   402\n\n# Calculate the total number of dyads that are democratic or non-democratic\ndem_dyad_totals &lt;- count(alliance_df, dem_dyad, name = \"dem_dyad_total\")\ndem_dyad_totals\n\n# A tibble: 2 × 2\n  dem_dyad dem_dyad_total\n     &lt;dbl&gt;          &lt;int&gt;\n1        0          11218\n2        1           4278\n\n# Calculate the total number of defense alliances\nalliance_totals &lt;- count(alliance_df, cow_defense, name = \"defense_total\")\nalliance_totals\n\n# A tibble: 2 × 2\n  cow_defense defense_total\n        &lt;dbl&gt;         &lt;int&gt;\n1           0         14603\n2           1           893\n\n# Work out your observed counts\nobs_exp_counts &lt;- alliance_totals |&gt; \n  expand_grid(dem_dyad_totals) |&gt; \n  relocate(dem_dyad) |&gt; \n  # Calculated the expected values\n  mutate(exp_n = (defense_total * dem_dyad_total) / nrow(alliance_df)) |&gt;\n  # Add the observed values for comparison\n  left_join(obs_values)\nobs_exp_counts\n\n# A tibble: 4 × 6\n  dem_dyad cow_defense defense_total dem_dyad_total  exp_n obs_n\n     &lt;dbl&gt;       &lt;dbl&gt;         &lt;int&gt;          &lt;int&gt;  &lt;dbl&gt; &lt;int&gt;\n1        0           0         14603          11218 10572. 10727\n2        1           0         14603           4278  4031.  3876\n3        0           1           893          11218   646.   491\n4        1           1           893           4278   247.   402\n\n\nHow different are our observed counts from our expected counts?\n\nchi_sq &lt;- obs_exp_counts |&gt; \n  mutate(diff = obs_n - exp_n,\n         diff_2 = diff^2,\n         diff_2_standard = diff_2 / exp_n) |&gt; \n  summarise(chi_sq = sum(diff_2_standard)) |&gt; \n  pull()\n\nchi_sq\n\n[1] 143.7121\n\n\nVery different! But is this difference statistically significant?\n\npchisq(chi_sq, df = 1, lower.tail = F)\n\n[1] 4.1072e-33\n\n\nYes! We are highly unlikely to see such large differences of counts if being a democratic dyad had no effect on states’ propensity to enter into a defense agreement.\n\n\n\nchisq.test(alliance_df$cow_defense, alliance_df$dem_dyad)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  alliance_df$cow_defense and alliance_df$dem_dyad\nX-squared = 142.79, df = 1, p-value &lt; 2.2e-16"
  },
  {
    "objectID": "content/10-applications.html#section",
    "href": "content/10-applications.html#section",
    "title": "Applications & Midterm Exam Review II",
    "section": "",
    "text": "install.packages(\"peacesciencer\")\n\n\nlibrary(tidyverse)\nlibrary(peacesciencer)\nlibrary(ggdist)\nlibrary(MetBrewer)\nlibrary(scales)\nlibrary(janitor)\nlibrary(wbstats)\nlibrary(countrycode)\nlibrary(modelsummary)\n\nToday, we are going to revise what we have learnt about hypothesis testing by working through some relevant examples.\n\n\nWe know that militarized conflict between states is a very rare event: most country pairings are at peace most of the time. Was the average number of militarized interstate disputes (MIDs) fought between states in 2014 different from 0.0036?\n\nmid_df &lt;- create_dyadyears(directed = F, subset_years = 2014) |&gt; \n  add_cow_mids() |&gt; \n  select(year, ccode1, ccode2, cowmidongoing)\n\nmid_df |&gt; \n  filter(cowmidongoing == 1)\n\n# A tibble: 55 × 4\n    year ccode1 ccode2 cowmidongoing\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;\n 1  2014      2    365             1\n 2  2014      2    652             1\n 3  2014      2    700             1\n 4  2014      2    710             1\n 5  2014      2    731             1\n 6  2014      2    770             1\n 7  2014     20    365             1\n 8  2014    200    230             1\n 9  2014    200    365             1\n10  2014    210    365             1\n# ℹ 45 more rows\n\n\nWhat was the observed average number of MIDs fought between states in 2014?\n\navg_mid &lt;- mean(mid_df$cowmidongoing)\navg_mid\n\n[1] 0.002907745\n\n\nIs this difference significant, or simply the product of random noise? What would a world in which the average number of MIDs between states in 2014 was 0.0036 look like?\n\n\n\n\n\n\nNote\n\n\n\nRemember, our null world is represented by the t-distribution. We center our null world at 0, allowing this to represent our null hypothesis. We work out how certain we are of our statistic using the amount of information we have (represented by the degrees of freedom). Next, we will work out how far away from this centerpoint our observed statistic sits.\n\n\nWhat are our degrees of freedom?\n\ndf &lt;- nrow(mid_df) - 1\ndf\n\n[1] 18914\n\n\nWe can use these degrees of freedom to build our null world:\n\nggplot(tibble(x = rt(1e6, df)), aes(x = x)) + \n  stat_slab() + \n  theme_minimal() + \n  theme(legend.position = \"none\")\n\n\n\n\nThis is what the standardized average number of MIDs pulled from one million samples drawn from our hypothetical null world would look like. The only reason they are not all equal to 0 is random chance.\nWe observed an average number of MIDs of 0.003 in 2014. If the null hypothesis were true, how likely would we be to see this average number of MIDs if we pulled a random sample from that null world?\nFirst, we need to set a threshold at which we are happy to accept the risk that we reject a true null hypothesis. Let’s use the standard 5%.\nNext, we need to work out where the remaining 95% of these null world means fall. I am going to focus on where 95% of these means fall around the null statistic (of 0). Because we are working with the standardized t-distribution, we can use what we know about that distribution to answer this question.\n\n\n\n\n\n\nTip\n\n\n\nThe base function qt() gives us the t-statistic that sits at a given probability.\nThis is simply the area under the t-distribution curve, which represents the proportion of hypothetical t-statistics that are equal to or more extreme than our given point. For example, we want to find the t-statistic corresponding to the point beyond which 97.5% of all possible t-statistics fall. See lower_boundary and the graph below.\n\n\n\nlower_boundary &lt;- qt(0.025, df = df, lower.tail = T)\nlower_boundary\n\n[1] -1.960089\n\nupper_boundary &lt;- qt(0.025, df = df, lower.tail = F)\nupper_boundary\n\n[1] 1.960089\n\n\n\nggplot(tibble(x = rt(1e6, df)), aes(x = x)) + \n  stat_slab(aes(fill_ramp = after_stat(x &lt; lower_boundary | x &gt; upper_boundary)),\n            fill = met.brewer(\"Egypt\")[2]) + \n  theme_minimal() + \n  theme(legend.position = \"none\")\n\n\n\n\nWhere does our observed mean fall in relation to this null world? First, we need to translate our observed mean into its corresponding t-statistic:\n\\[\nt = \\frac{\\bar{x} - \\mu_0}{\\frac{s}{\\sqrt{n}}}\n\\]\n\n\n\n\n\n\nNote\n\n\n\nAbove, we centered our null world at 0. So, we need to translate our observed mean into its distance from the null hypothesis mean (represented by 0).\n\n\nWe know our observed mean, \\(\\bar{x}\\):\n\navg_mid\n\n[1] 0.002907745\n\n\nAnd our null mean, \\(\\mu_0\\):\n\nnull_mean &lt;- 0.0036\nnull_mean\n\n[1] 0.0036\n\n\nAnd our sample size, \\(n\\):\n\nsample_size &lt;- nrow(mid_df)\nsample_size\n\n[1] 18915\n\n\nAnd our sample standard deviation, \\(s\\):\n\nsample_sd &lt;- sd(mid_df$cowmidongoing)\nsample_sd\n\n[1] 0.05384648\n\n\nTherefore, our translated t-statistic is:\n\nt_stat &lt;- (avg_mid - null_mean) / (sample_sd / sqrt(sample_size))\nt_stat\n\n[1] -1.76812\n\n\nWhere does this sit within our null world?\n\nggplot(tibble(x = rt(1e6, df)), aes(x = x)) + \n  stat_slab(aes(fill_ramp = after_stat(x &lt; lower_boundary | x &gt; upper_boundary)),\n            fill = met.brewer(\"Egypt\")[2]) + \n  geom_vline(xintercept = t_stat) + \n  theme_minimal() + \n  theme(legend.position = \"none\")\n\n\n\n\nIn other worlds, if the null hypothesis were true, how likely would we be to draw a SRS that was as extreme or more extreme than the one we observed?\n\n2*pt(t_stat, df = df)\n\n[1] 0.07705689\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe base function pt() gives is the probability of observing a value equal to or as extreme as the t-statistic we provided. It does the opposite of qt().\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe need to double this probability because it is only providing us the p-value that corresponds with the first t-statistic (the dark blue area shaded to the left of the graph above).\n\n\nIf the null hypothesis were true, we would observe an average number of MIDs between states of 0.003 in 7.71% of an infinite number of samples from that null world. Therefore, we cannot reject the null hypothesis that the average number of MIDs between states in 2014 was 0.0036 with a two-tailed test at the 5% threshold.\n\n\nWhat if our hypothesis was directional? What if I believe that the average number of MIDs between states is less than 0.0036?\nWhen we talk about the direction of our significance test, we are talking about those boundaries around the null hypothesis. I am still only willing to accept a 5% chance that I reject the null hypothesis when it is in fact true. However, I no longer need to split that 5% chance evenly above and below the null hypothesis. Instead, I can place that whole 5% either above or below my null hypothesis (depending on the direction of my hypothesis).\nIn this example, I think that the average number of MIDs between states is less than 0.0036. If my sample has an average number of MIDs greater than 0.0036, I should more readily reject my alternative hypothesis that the average number of MIDs is smaller than 0.0036. Therefore, I can concentrate my 5% threshold below the null hypothesis:\n\nlower_boundary &lt;- qt(0.05, df = df, lower.tail = T)\nlower_boundary\n\n[1] -1.644934\n\n\nVisually:\n\nggplot(tibble(x = rt(1e6, df)), aes(x = x)) + \n  stat_slab(aes(fill_ramp = after_stat(x &lt; lower_boundary)),\n            fill = met.brewer(\"Egypt\")[2]) + \n  theme_minimal() + \n  theme(legend.position = \"none\")\n\n\n\n\nRemember, I cannot reject the null hypothesis if it is plausible (there is a 95% chance) that I would observe the mean that I did even if the null hypothesis were true. That 95% is represented by the light blue area on the graph above. The dark blue shaded area is still highlighting where the average count of MIDs drawn from 5% of all possible samples pulled from the null world would fall.\nOur observed average has not changed. Let’s put it in this new context:\n\nggplot(tibble(x = rt(1e6, df)), aes(x = x)) + \n  stat_slab(aes(fill_ramp = after_stat(x &lt; lower_boundary)),\n            fill = met.brewer(\"Egypt\")[2]) + \n  geom_vline(xintercept = t_stat) + \n  theme_minimal() + \n  theme(legend.position = \"none\")\n\n\n\n\nIt is unlikely that we would observe this average number of MIDs if, in fact, the null hypothesis were true. How unlikely?\n\npt(t_stat, df = df)\n\n[1] 0.03852845\n\n\nIf the null hypothesis were true, we would observe an average number of MIDs of 0.003 in 3.85% of an infinite number of samples from that null world. Therefore, we can reject the null hypothesis that the average number of MIDs between states in 2014 was 0.0036 with a one-tailed test at the 5% threshold.\n\n\n\n\nAre the majority of states in the world democracies?\n\ndem_df &lt;- ccode_democracy |&gt; \n  drop_na(polity2) |&gt; \n  slice_max(year) |&gt; \n  transmute(ccode, democracy = if_else(polity2 &gt; 5, 1, 0))\n\ndem_df\n\n# A tibble: 166 × 2\n   ccode democracy\n   &lt;dbl&gt;     &lt;dbl&gt;\n 1     2         1\n 2    20         1\n 3    40         0\n 4    41         0\n 5    42         1\n 6    51         1\n 7    52         1\n 8    70         1\n 9    90         1\n10    91         0\n# ℹ 156 more rows\n\n\nWhat proportion of states were democracies in 2017?\n\ntabyl(dem_df, democracy)\n\n democracy  n   percent\n         0 70 0.4216867\n         1 96 0.5783133\n\n\nIs this proportion significantly different from 0.5, or is it simply the product of random noise?\nTo answer this question, we will set up our null world:\n\nggplot(tibble(x = rnorm(1e6)), aes(x = x)) + \n  stat_slab() + \n  theme_minimal() + \n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nRemember, we use the normal distribution when examining the difference of proportions.\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe base function rnorm() gives us random values drawn from a normal distribution. By default, rnorm() draws from a normal distribution centered at 0 and with a standard deviation of 1. This is perfect for working with z-scores.\n\n\nThis is what the standardized proportion of democracies are for 1 million samples drawn from our null world.\nLet’s see whether we can reject the null hypothesis that the proportion of democracies globally is 0.5 or 50%. I am willing to accept a 5% risk that I reject this null hypothesis when it is, in fact, true. So I now need to work out where 95% of plausible proportions fall in samples drawn from a world in which the proportion of democracies is 0.5.\n\nlower_boundary &lt;- qnorm(0.025)\nlower_boundary\n\n[1] -1.959964\n\nupper_boundary &lt;- qnorm(0.025, lower.tail = F)\nupper_boundary\n\n[1] 1.959964\n\n\nLet’s see this:\n\nggplot(tibble(x = rnorm(1e6)), aes(x = x)) + \n  stat_slab(aes(fill_ramp = after_stat(x &lt; lower_boundary | x &gt; upper_boundary)),\n            fill = met.brewer(\"Egypt\")[2]) + \n  theme_minimal() + \n  theme(legend.position = \"none\")\n\n\n\n\nSo, if our observed statistic sits so far from the null hypothesis that there is less than a 5% chance that I could draw a sample with its proportion of democracies, then we have sufficient evidence to reject the null hypothesis at this threshold. We just need to work out where our observed statistic sits on this standardized distribution.\nTo do this, we need to translate our observed statistic into its z-score:\n\\[\nz = \\frac{\\hat{p} - p_0}{\\sqrt{\\frac{p_0(1-p_0)}{n}}}\n\\]\nWe know our observed proportion, \\(\\hat{p}\\):\n\nsample_proportion &lt;- dem_df |&gt; \n  tabyl(democracy) |&gt; \n  filter(democracy == 1) |&gt; \n  pull(percent)\n\nsample_proportion\n\n[1] 0.5783133\n\n\nAnd our null proportion, \\(p_0\\):\n\nnull_proportion &lt;- 0.5\nnull_proportion\n\n[1] 0.5\n\n\nAnd our sample size, \\(n\\):\n\nsample_size &lt;- nrow(dem_df)\nsample_size\n\n[1] 166\n\n\nNow we can translate our observed proportion into its distance from the null proportion:\n\nz_score &lt;- (sample_proportion - null_proportion) / sqrt((null_proportion * (1 - null_proportion)) / sample_size)\nz_score\n\n[1] 2.017991\n\n\nThis z-score represents how far from the null proportion our observed proportion sits. Let’s take a look:\n\nggplot(tibble(x = rnorm(1e6)), aes(x = x)) + \n  stat_slab(aes(fill_ramp = after_stat(x &lt; lower_boundary | x &gt; upper_boundary)),\n            fill = met.brewer(\"Egypt\")[2]) + \n  geom_vline(xintercept = z_score) + \n  theme_minimal() + \n  theme(legend.position = \"none\")\n\n\n\n\nIn fact, our observed statistic is sufficiently far from the null hypothesis. We can reject that null hypothesis with 95% confidence.\n\n2*pnorm(z_score, lower.tail = F)\n\n[1] 0.04359216\n\n\nThere is a 4% chance that we would observe a proportion of democracies that was either this far above or below a proportion of 0.5 if the true proportion of democracies was 0.5. This is less than the threshold by which I am willing to reject a true null hypothesis. Therefore, I have sufficient evidence to reject the null hypothesis.\n\n\n\nAre democracies richer, on average, than non-democracies?\n\ngdp_df &lt;- wb_data(\"NY.GDP.MKTP.CD\", \n                  start_date = 2017, \n                  end_date = 2017, \n                  return_wide = F) |&gt; \n  transmute(ccode = countrycode(country, \"country.name\", \"cown\"),\n            gdp = value)\n\ndem_gdp_df &lt;- dem_df |&gt; \n  left_join(gdp_df) |&gt; \n  drop_na(gdp)\n\nWhat is the average GDP of democracies and non-democracies?\n\ndem_gdp_summary &lt;- dem_gdp_df |&gt; \n  group_by(democracy) |&gt; \n  summarise(n = n(), \n            avg_gdp = mean(gdp)) |&gt; \n  mutate(diff_avg_gdp = avg_gdp - lag(avg_gdp))\n\ndem_gdp_summary\n\n# A tibble: 2 × 4\n  democracy     n       avg_gdp  diff_avg_gdp\n      &lt;dbl&gt; &lt;int&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1         0    66 311030589116.           NA \n2         1    94 631669975302. 320639386186.\n\n\nIt appears that we have support for our hypothesis that democracies are richer than non-democracies on average. How much richer?\n\ndiff_avg &lt;- dem_gdp_summary |&gt; \n  filter(democracy == 1) |&gt; \n  pull(diff_avg_gdp)\n\ndollar(diff_avg)\n\n[1] \"$320,639,386,186\"\n\n\nHowever, we need to confirm that this difference is not the product of random chance.\nFirst, we need to build our null world. In this world, there is no difference in the average wealth of democracies and non-democracies. We will center our null world at 0. We can use our degrees of freedom to build in uncertainty around that central point.\n\n\n\n\n\n\nNote\n\n\n\nThe degrees of freedom for the difference of means is equal to the smaller of the number of observations within each group minus one.\n\n\n\nn_democracies &lt;- dem_gdp_summary |&gt; \n  filter(democracy == 1) |&gt; \n  pull(n)\n\nn_democracies\n\n[1] 94\n\nn_non_democracies &lt;- dem_gdp_summary |&gt; \n  filter(democracy == 0) |&gt; \n  pull(n)\n\nn_non_democracies\n\n[1] 66\n\n\nOur degrees of freedom are equal to the smaller of these two, minus one:\n\ndf &lt;- min(n_democracies, n_non_democracies) - 1\ndf\n\n[1] 65\n\n\nNow we have all the information that we need to build our null world:\n\nggplot(tibble(x = rt(1e6, df)), aes(x = x)) + \n  stat_slab() + \n  theme_minimal() + \n  theme(legend.position = \"none\")\n\n\n\n\nI am now happy to accept a 10% risk that I will reject a true null hypothesis. However, I will continue to work with two-tailed tests. Let’s work out where 90% of our hypothetical differences of means sit around our null mean of 0.\n\nlower_boundary &lt;- qt(0.05, df = df)\nlower_boundary\n\n[1] -1.668636\n\nupper_boundary &lt;- qt(0.05, df = df, lower.tail = F)\nupper_boundary\n\n[1] 1.668636\n\n\nLet’s visualize those boundaries:\n\nggplot(tibble(x = rt(1e6, df)), aes(x = x)) + \n  stat_slab(aes(fill_ramp = after_stat(x &lt; lower_boundary | x &gt; upper_boundary)),\n            fill = met.brewer(\"Egypt\")[2]) + \n  theme_minimal() + \n  theme(legend.position = \"none\")\n\n\n\n\nWhere does our observed difference of means sit along this standardized distribution? To answer this question, we need to translate our observed difference of $320,639,386,186 into its t-statistic:\n\\[\nt = \\frac{\\bar{x_1} - \\bar{x_2}}{\\sqrt{\\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2}}}\n\\] We know our difference of means, \\(\\bar{x_1} - \\bar{x_2}\\):\n\ndollar(diff_avg)\n\n[1] \"$320,639,386,186\"\n\n\nAnd our sample sizes, \\(n_1\\) and \\(n_2\\):\n\nn_democracies\n\n[1] 94\n\nn_non_democracies\n\n[1] 66\n\n\nAnd our standard deviations, \\(s_1\\) and \\(s_2\\):\n\ns_1 &lt;- dem_gdp_df |&gt; \n  filter(democracy == 1) |&gt;\n  pull(gdp) |&gt; \n  sd()\n\ndollar(s_1)\n\n[1] \"$2,149,439,341,731\"\n\ns_2 &lt;- dem_gdp_df |&gt; \n  filter(democracy == 0) |&gt;\n  pull(gdp) |&gt; \n  sd()\n\ndollar(s_2)\n\n[1] \"$1,519,811,642,705\"\n\n\nTherefore, the t-statistic for our observed difference of means is:\n\nt_stat &lt;- diff_avg / sqrt((s_1^2 / n_democracies) + (s_2^2 / n_non_democracies))\nt_stat\n\n[1] 1.105342\n\n\nLet’s place this observed statistic in the context of the null world:\n\nggplot(tibble(x = rt(1e6, df)), aes(x = x)) + \n  stat_slab(aes(fill_ramp = after_stat(x &lt; lower_boundary | x &gt; upper_boundary)),\n            fill = met.brewer(\"Egypt\")[2]) + \n  geom_vline(xintercept = t_stat) + \n  theme_minimal() + \n  theme(legend.position = \"none\")\n\n\n\n\nIt is plausible that we could observe a difference of means equal to $320,639,386,186 even if there is no significant difference of means between the wealth of democracies and non-democracies. In fact, there is the following chance that we would observe this difference:\n\npercent(2 * pt(t_stat, df = df, lower.tail = F))\n\n[1] \"27%\"\n\n\nThis sits well above our threshold of a 10% risk that we would reject a true null hypothesis.\n\n\nWe get the same result using the base function t.test():\n\nt.test(gdp ~ democracy, data = dem_gdp_df)\n\n\n    Welch Two Sample t-test\n\ndata:  gdp by democracy\nt = -1.1053, df = 157.99, p-value = 0.2707\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -893577418352  252298645981\nsample estimates:\nmean in group 0 mean in group 1 \n   311030589116    631669975302 \n\n\n\n\n\n\n\n\nNote\n\n\n\nt.test() finds the difference of means by subtracting the average GDP of democracies from that of non-democracies. That is why the t-statistic is negative.\n\n\n\n\n\n\nIs the proportion of democratic dyads with defense alliances greater than that of non-democratic dyads?\n\nalliance_df &lt;- create_dyadyears(directed = F, subset_years = 2010) |&gt; \n  add_cow_alliance() |&gt; \n  add_democracy() |&gt; \n  transmute(ccode1, \n            ccode2, \n            cow_defense, \n            dem_dyad = if_else(polity21 &gt; 5 & polity22 &gt; 5, 1, 0)) |&gt; \n  drop_na(cow_defense, dem_dyad)\n\nalliance_df\n\n# A tibble: 15,496 × 4\n   ccode1 ccode2 cow_defense dem_dyad\n    &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n 1      2     20           1        1\n 2      2     40           0        0\n 3      2     41           1        0\n 4      2     42           1        1\n 5      2     51           1        1\n 6      2     52           1        1\n 7      2     70           1        1\n 8      2     90           1        1\n 9      2     91           1        1\n10      2     92           1        1\n# ℹ 15,486 more rows\n\n\nWhat proportion of democratic and non-democratic dyads have defense alliances?\n\nalliance_summary &lt;- alliance_df |&gt; \n  group_by(dem_dyad) |&gt; \n  summarise(n = n(),\n            n_defense = sum(cow_defense)) |&gt; \n  mutate(prop = n_defense / n,\n         diff_prop = prop - lag(prop))\n\nalliance_summary\n\n# A tibble: 2 × 5\n  dem_dyad     n n_defense   prop diff_prop\n     &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n1        0 11218       491 0.0438   NA     \n2        1  4278       402 0.0940    0.0502\n\n\nThe proportion of democratic dyads with defense alliances is greater than that of non-democratic dyads. Is this difference statistically significant?\nLet’s set up our null world:\n\nggplot(tibble(x = rnorm(1e6)), aes(x = x)) + \n  stat_slab() + \n  theme_minimal() + \n  theme(legend.position = \"none\")\n\n\n\n\nWe will stick with our two-tailed test at the 0.1 threshold. Let’s find where the other 90% of difference of proportions fall in our null world:\n\nlower_boundary &lt;- qnorm(0.05)\nlower_boundary\n\n[1] -1.644854\n\nupper_boundary &lt;- qnorm(0.05, lower.tail = F)\nupper_boundary\n\n[1] 1.644854\n\n\n\nggplot(tibble(x = rnorm(1e6)), aes(x = x)) + \n  stat_slab(aes(fill_ramp = after_stat(x &lt; lower_boundary | x &gt; upper_boundary)),\n            fill = met.brewer(\"Egypt\")[2]) + \n  theme_minimal() + \n  theme(legend.position = \"none\")\n\n\n\n\nWhere does our observed difference of 5% fall?\nWe need to convert this observed difference into its z-score:\n\\[\nz = \\frac{\\hat{p}_1 - \\hat{p}_2}{SE_{D_p}}\n\\]\nWhere:\n\\[\nSE_{D_p} = \\sqrt{\\hat{p}(1-\\hat{p})(\\frac{1}{n_1} + \\frac{1}{n_2})}\n\\]\nAnd:\n\\[\n\\hat{p} = \\frac{x_1 + x_2}{n_1 + n_2}\n\\]\nWe will take this step-by-step:\nWe know the proportion of democratic and non-democratic dyads that have defense alliances:\n\nalliance_summary\n\n# A tibble: 2 × 5\n  dem_dyad     n n_defense   prop diff_prop\n     &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n1        0 11218       491 0.0438   NA     \n2        1  4278       402 0.0940    0.0502\n\n\nTherefore, we can calculate our pooled estimate: \\(\\hat{p}\\):\n\npooled_p &lt;- sum(alliance_summary$prop)\npooled_p\n\n[1] 0.1377381\n\n\nAnd our sample sizes:\n\nn_dem_dyad &lt;- alliance_summary |&gt; \n  filter(dem_dyad == 1) |&gt; \n  pull(n)\n\nn_dem_dyad\n\n[1] 4278\n\nn_non_dem_dyad &lt;- alliance_summary |&gt; \n  filter(dem_dyad == 0) |&gt; \n  pull(n)\n\nn_non_dem_dyad\n\n[1] 11218\n\n\nWhich allows us to calculate our standard error:\n\nse &lt;- sqrt(pooled_p * (1 - pooled_p) * ((1 / n_dem_dyad) + (1 / n_non_dem_dyad)))\nse\n\n[1] 0.006192675\n\n\nWhich allows us to translate our observed difference into its z-score:\n\ndiff_prop &lt;- alliance_summary |&gt; \n  filter(dem_dyad == 1) |&gt; \n  pull(diff_prop)\n\nz_score &lt;- diff_prop / se\nz_score\n\n[1] 8.106383\n\n\nWhich we can then put into the context of the null world:\n\nggplot(tibble(x = rnorm(1e6)), aes(x = x)) + \n  stat_slab(aes(fill_ramp = after_stat(x &lt; lower_boundary | x &gt; upper_boundary)),\n            fill = met.brewer(\"Egypt\")[2]) + \n  geom_vline(xintercept = z_score) + \n  theme_minimal() + \n  theme(legend.position = \"none\")\n\n\n\n\nWe are very unlikely to have seen a difference in the proportion of democracies and non-democracies with defense alliances of 5% if there were, in fact, no difference in these proportions. How unlikely?\n\n2 * pnorm(z_score, lower.tail = F)\n\n[1] 5.214871e-16\n\n\nVery unlikely!\n\n\n\nLet’s look at this same question, but ask whether democracy shared between countries in a dyad significantly effects the number of defense alliances held by them.\nFirst, we need to find the observed count of defense alliances split between democratic and non-democratic dyads:\n\ndatasummary_crosstab(cow_defense ~ dem_dyad, data = alliance_df)\n\n\n\n\ncow_defense\n\n0\n1\nAll\n\n\n\n\n0\nN\n10727\n3876\n14603\n\n\n\n% row\n73.5\n26.5\n100.0\n\n\n1\nN\n491\n402\n893\n\n\n\n% row\n55.0\n45.0\n100.0\n\n\nAll\nN\n11218\n4278\n15496\n\n\n\n% row\n72.4\n27.6\n100.0\n\n\n\n\n\n\n\nWhat would our cross tab look like if democratic dyads had no effect on the number of defense alliances?\n\n# Calculate the observed count for each category\nobs_values &lt;- count(alliance_df, cow_defense, dem_dyad, name = \"obs_n\")\nobs_values\n\n# A tibble: 4 × 3\n  cow_defense dem_dyad obs_n\n        &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;\n1           0        0 10727\n2           0        1  3876\n3           1        0   491\n4           1        1   402\n\n# Calculate the total number of dyads that are democratic or non-democratic\ndem_dyad_totals &lt;- count(alliance_df, dem_dyad, name = \"dem_dyad_total\")\ndem_dyad_totals\n\n# A tibble: 2 × 2\n  dem_dyad dem_dyad_total\n     &lt;dbl&gt;          &lt;int&gt;\n1        0          11218\n2        1           4278\n\n# Calculate the total number of defense alliances\nalliance_totals &lt;- count(alliance_df, cow_defense, name = \"defense_total\")\nalliance_totals\n\n# A tibble: 2 × 2\n  cow_defense defense_total\n        &lt;dbl&gt;         &lt;int&gt;\n1           0         14603\n2           1           893\n\n# Work out your observed counts\nobs_exp_counts &lt;- alliance_totals |&gt; \n  expand_grid(dem_dyad_totals) |&gt; \n  relocate(dem_dyad) |&gt; \n  # Calculated the expected values\n  mutate(exp_n = (defense_total * dem_dyad_total) / nrow(alliance_df)) |&gt;\n  # Add the observed values for comparison\n  left_join(obs_values)\nobs_exp_counts\n\n# A tibble: 4 × 6\n  dem_dyad cow_defense defense_total dem_dyad_total  exp_n obs_n\n     &lt;dbl&gt;       &lt;dbl&gt;         &lt;int&gt;          &lt;int&gt;  &lt;dbl&gt; &lt;int&gt;\n1        0           0         14603          11218 10572. 10727\n2        1           0         14603           4278  4031.  3876\n3        0           1           893          11218   646.   491\n4        1           1           893           4278   247.   402\n\n\nHow different are our observed counts from our expected counts?\n\nchi_sq &lt;- obs_exp_counts |&gt; \n  mutate(diff = obs_n - exp_n,\n         diff_2 = diff^2,\n         diff_2_standard = diff_2 / exp_n) |&gt; \n  summarise(chi_sq = sum(diff_2_standard)) |&gt; \n  pull()\n\nchi_sq\n\n[1] 143.7121\n\n\nVery different! But is this difference statistically significant?\n\npchisq(chi_sq, df = 1, lower.tail = F)\n\n[1] 4.1072e-33\n\n\nYes! We are highly unlikely to see such large differences of counts if being a democratic dyad had no effect on states’ propensity to enter into a defense agreement.\n\n\n\nchisq.test(alliance_df$cow_defense, alliance_df$dem_dyad)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  alliance_df$cow_defense and alliance_df$dem_dyad\nX-squared = 142.79, df = 1, p-value &lt; 2.2e-16"
  },
  {
    "objectID": "content/12-multiple_regression.html",
    "href": "content/12-multiple_regression.html",
    "title": "Multiple Regression",
    "section": "",
    "text": "Pollock & Edwards, Chapter 9\n\n\n\n Pollock & Edwards R Companion, Chapter 9"
  },
  {
    "objectID": "content/12-multiple_regression.html#readings",
    "href": "content/12-multiple_regression.html#readings",
    "title": "Multiple Regression",
    "section": "",
    "text": "Pollock & Edwards, Chapter 9\n\n\n\n Pollock & Edwards R Companion, Chapter 9"
  },
  {
    "objectID": "content/12-multiple_regression.html#section",
    "href": "content/12-multiple_regression.html#section",
    "title": "Multiple Regression",
    "section": "Section",
    "text": "Section\n\ninstall.packages(\"plotly\")\n\n\nlibrary(tidyverse)\nlibrary(wbstats)\nlibrary(broom)\nlibrary(modelsummary)\nlibrary(marginaleffects)\nlibrary(plotly)\nlibrary(ggdist)\n\nLet’s take a look at the determinants of citizens’ average life expectancy. Suppose that we hypothesize that the greater proportion of a country’s GDP that it spends on healthcare, the more years its citizens should expect to live, on average. Let’s build a simple linear regression model of some observed data.\n\nBinary linear regression model\nFirst, we can gather our data from the World Bank Data Center:\n\nhealth_df &lt;- wb_data(\n  indicator = c(\"SP.DYN.LE00.IN\", \"SH.XPD.CHEX.GD.ZS\"),\n  start_date = 2016,\n  end_date = 2016\n) |&gt; \n  rename(\n    life_exp = SP.DYN.LE00.IN,\n    health_expend = SH.XPD.CHEX.GD.ZS\n  ) \n\nhealth_df\n\n# A tibble: 217 × 6\n   iso2c iso3c country               date health_expend life_exp\n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;                &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n 1 AW    ABW   Aruba                 2016         NA        75.6\n 2 AF    AFG   Afghanistan           2016         11.8      63.1\n 3 AO    AGO   Angola                2016          2.71     61.1\n 4 AL    ALB   Albania               2016          6.73     78.9\n 5 AD    AND   Andorra               2016          6.91     NA  \n 6 AE    ARE   United Arab Emirates  2016          3.90     79.3\n 7 AR    ARG   Argentina             2016          9.86     76.3\n 8 AM    ARM   Armenia               2016          9.95     74.7\n 9 AS    ASM   American Samoa        2016         NA        NA  \n10 AG    ATG   Antigua and Barbuda   2016          5.12     78.2\n# ℹ 207 more rows\n\n\nNext, we fit our linear regression model:\n\nm &lt;- lm(life_exp ~ health_expend, data = health_df)\n\nsummary(m)\n\n\nCall:\nlm(formula = life_exp ~ health_expend, data = health_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-21.506  -4.942   1.611   5.804  12.768 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    66.7411     1.3838  48.231  &lt; 2e-16 ***\nhealth_expend   0.7604     0.1932   3.936 0.000119 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.419 on 179 degrees of freedom\n  (36 observations deleted due to missingness)\nMultiple R-squared:  0.07965,   Adjusted R-squared:  0.07451 \nF-statistic: 15.49 on 1 and 179 DF,  p-value: 0.0001185\n\n\nLet’s make this a bit easier to read:\n\nmodelsummary(m, \n             coef_rename = c(health_expend = \"Health expenditure (% GDP)\"),\n             statistic = c(\"t = {statistic}\", \"SE = {std.error}\", \"conf.int\"))\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n66.741\n\n\n\nt = 48.231\n\n\n\nSE = 1.384\n\n\n\n[64.010, 69.472]\n\n\nHealth expenditure (% GDP)\n0.760\n\n\n\nt = 3.936\n\n\n\nSE = 0.193\n\n\n\n[0.379, 1.142]\n\n\nNum.Obs.\n181\n\n\nR2\n0.080\n\n\nR2 Adj.\n0.075\n\n\nAIC\n1243.1\n\n\nBIC\n1252.7\n\n\nLog.Lik.\n−618.547\n\n\nRMSE\n7.38\n\n\n\n\n\n\n\nOur model suggests a positive and significant relationship between the proportion that a country spends of its GDP on its healthcare and its citizens’ average life expectancy. This relationship is statistically significant. Every one percentage point increase in a country’s health expenditure is associated with an increase of years for the average citizen’s life expectancy, on average.\nWe can plot the predicted life expectancy of a country for all plausible values of health expenditure using marginaleffects::plot_predictions():\n\nplot_predictions(m, condition = \"health_expend\")\n\n\n\n\nBut remember all the way back to our session on the relationship between two variables. We know from the Gapminder Project that a country’s life expectancy is strongly associated with its wealth (measured in terms of its GDP per capita). What if the wealth of a country’s citizens also contributes to their average life expectancy? Further, what if the relationship between a country’s health expenditure and its life expectancy is, in fact, driven by its citizen’s wealth? We can use multiple linear regression to answer these questions.\n\n\nMultiple linear regression model\nWe can easily incorporate additional independent variables into our linear regression model. First, we need to collect data on each country’s GDP per capita.\n\ngdp_per_cap_df &lt;- wb_data(\n  indicator = \"NY.GDP.PCAP.CD\",\n  start_date = 2016,\n  end_date = 2016\n) |&gt; \n  transmute(iso3c, date, gdp_per_cap = NY.GDP.PCAP.CD, logged_gdp_per_cap = log(gdp_per_cap))\n\ngdp_per_cap_df\n\n# A tibble: 217 × 4\n   iso3c  date gdp_per_cap logged_gdp_per_cap\n   &lt;chr&gt; &lt;dbl&gt;       &lt;dbl&gt;              &lt;dbl&gt;\n 1 ABW    2016      28450.              10.3 \n 2 AFG    2016        523.               6.26\n 3 AGO    2016       1810.               7.50\n 4 ALB    2016       4124.               8.32\n 5 AND    2016      39931.              10.6 \n 6 ARE    2016      41055.              10.6 \n 7 ARG    2016      12790.               9.46\n 8 ARM    2016       3680.               8.21\n 9 ASM    2016      13301.               9.50\n10 ATG    2016      16449.               9.71\n# ℹ 207 more rows\n\n\nWe can join those data to our previous dataset using dplyr::left_join():\n\nhealth_gdp_df &lt;- health_df |&gt; \n  left_join(gdp_per_cap_df, by = c(\"iso3c\", \"date\"))\n\nhealth_gdp_df\n\n# A tibble: 217 × 8\n   iso2c iso3c country               date health_expend life_exp gdp_per_cap\n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;                &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;\n 1 AW    ABW   Aruba                 2016         NA        75.6      28450.\n 2 AF    AFG   Afghanistan           2016         11.8      63.1        523.\n 3 AO    AGO   Angola                2016          2.71     61.1       1810.\n 4 AL    ALB   Albania               2016          6.73     78.9       4124.\n 5 AD    AND   Andorra               2016          6.91     NA        39931.\n 6 AE    ARE   United Arab Emirates  2016          3.90     79.3      41055.\n 7 AR    ARG   Argentina             2016          9.86     76.3      12790.\n 8 AM    ARM   Armenia               2016          9.95     74.7       3680.\n 9 AS    ASM   American Samoa        2016         NA        NA        13301.\n10 AG    ATG   Antigua and Barbuda   2016          5.12     78.2      16449.\n# ℹ 207 more rows\n# ℹ 1 more variable: logged_gdp_per_cap &lt;dbl&gt;\n\n\n\n\n\n\n\n\nNote\n\n\n\nRemember, the relationship between a country’s average life expectancy and its GDP per capita is not linear:\n\nggplot(health_gdp_df, aes(x = gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  theme_minimal()\n\n\n\n\nWe can log transform the GDP per capita to create a more linear relationship:\n\nggplot(health_gdp_df, aes(x = logged_gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  theme_minimal()\n\n\n\n\n\n\n\n\nFitting a multiple linear regression model\nLet’s update our previous binary linear regression model to include each country’s logged GDP per capita:\n\nm_multi &lt;- lm(life_exp ~ logged_gdp_per_cap + health_expend, data = health_gdp_df)\n\nsummary(m_multi)\n\n\nCall:\nlm(formula = life_exp ~ logged_gdp_per_cap + health_expend, data = health_gdp_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.0753  -1.9420   0.5623   2.5805   8.3517 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         30.8036     1.9340  15.928   &lt;2e-16 ***\nlogged_gdp_per_cap   4.6836     0.2310  20.275   &lt;2e-16 ***\nhealth_expend        0.1064     0.1115   0.954    0.341    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.091 on 177 degrees of freedom\n  (37 observations deleted due to missingness)\nMultiple R-squared:  0.7225,    Adjusted R-squared:  0.7194 \nF-statistic: 230.4 on 2 and 177 DF,  p-value: &lt; 2.2e-16\n\n\n\nmodelsummary(m_multi, \n             coef_rename = c(logged_gdp_per_cap = \"GDP per capita (logged)\",\n                             health_expend = \"Health expenditure (% GDP)\"),\n             statistic = c(\"t = {statistic}\", \"SE = {std.error}\", \"conf.int\"))\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n30.804\n\n\n\nt = 15.928\n\n\n\nSE = 1.934\n\n\n\n[26.987, 34.620]\n\n\nGDP per capita (logged)\n4.684\n\n\n\nt = 20.275\n\n\n\nSE = 0.231\n\n\n\n[4.228, 5.139]\n\n\nHealth expenditure (% GDP)\n0.106\n\n\n\nt = 0.954\n\n\n\nSE = 0.112\n\n\n\n[−0.114, 0.326]\n\n\nNum.Obs.\n180\n\n\nR2\n0.723\n\n\nR2 Adj.\n0.719\n\n\nAIC\n1022.9\n\n\nBIC\n1035.7\n\n\nLog.Lik.\n−507.466\n\n\nRMSE\n4.06\n\n\n\n\n\n\n\nYou can immediately see the rather stark effect of including a country’s (logged) GDP per capita on the statistical significance of the relationship between health expenditure and life expectancy. We will get to that shortly, but first let’s explore what we have built.\n\n\nThe line(ear plane) of best fit\nLike our binary linear regression model, this multiple regression model finds the linear equation that minimizes the distance between itself and the observed values. However, this model minimizes the distance between itself and each observed value for both the country’s logged GDP per capita and health expenditure.\n\nplot_ly(health_gdp_df, \n        x = ~ logged_gdp_per_cap, \n        y = ~ health_expend, \n        z = ~ life_exp, \n        type = \"scatter3d\", \n        mode = \"markers\",\n        alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis graph is interactive! Have a play around.\n\n\nOur multiple linear regression model finds the linear plane that minimizes the distance between itself and each of these observed values:\n\n\nShow the code\n# Get all plotted points for logged GDP per capita\npoints_gdp &lt;- seq(min(health_gdp_df$logged_gdp_per_cap, na.rm = T), \n                  max(health_gdp_df$logged_gdp_per_cap, na.rm = T), \n                  by = 1)\n\n# Get all plotted points for health expenditure\npoints_health &lt;- seq(min(health_gdp_df$health_expend, na.rm = T),\n                     max(health_gdp_df$health_expend, na.rm = T),\n                     by = 1)\n\n# Get the predicted values for life expectancy from the model\nnew_df &lt;- crossing(\n  logged_gdp_per_cap = points_gdp,\n  health_expend = points_health\n)\n\npred_values &lt;- augment(m_multi, newdata = new_df) |&gt; \n  pull(.fitted)\n\npred_values_matrix &lt;- matrix(pred_values, nrow = length(points_health), ncol = length(points_gdp))\n\n# Plot the plane\nplot_ly() |&gt; \n  add_surface(x = points_gdp, \n              y = points_health,\n              z = pred_values_matrix,\n              colors = \"pink\") |&gt; \n  add_markers(x = health_gdp_df$logged_gdp_per_cap, \n              y = health_gdp_df$health_expend,\n              z = health_gdp_df$life_exp,\n              type = \"scatter3d\",\n              alpha = 0.75) |&gt; \n  layout(showlegend = FALSE)\n\n\n\n\n\n\nThis line of best fit provides us with an expected average life expectancy for every combination of a country’s plausible logged GDP per capita and heath expenditure.\n\n\nInterpreting the coefficients\nLet’s return to our model:\n\nmodelsummary(m_multi, \n             coef_rename = c(logged_gdp_per_cap = \"GDP per capita (logged)\",\n                             health_expend = \"Health expenditure (% GDP)\"),\n             statistic = c(\"t = {statistic}\", \"SE = {std.error}\", \"conf.int\"))\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n30.804\n\n\n\nt = 15.928\n\n\n\nSE = 1.934\n\n\n\n[26.987, 34.620]\n\n\nGDP per capita (logged)\n4.684\n\n\n\nt = 20.275\n\n\n\nSE = 0.231\n\n\n\n[4.228, 5.139]\n\n\nHealth expenditure (% GDP)\n0.106\n\n\n\nt = 0.954\n\n\n\nSE = 0.112\n\n\n\n[−0.114, 0.326]\n\n\nNum.Obs.\n180\n\n\nR2\n0.723\n\n\nR2 Adj.\n0.719\n\n\nAIC\n1022.9\n\n\nBIC\n1035.7\n\n\nLog.Lik.\n−507.466\n\n\nRMSE\n4.06\n\n\n\n\n\n\n\nHow can we interpret the association between each independent variable and our outcome of interest?\n\nThe intercept\nGenerally:\n\nThe expected value of \\(Y\\), on average and holding all else at zero.\n\nOur model:\n\nA country with a logged GDP per capita of zero and which spends zero percent of its GDP on healthcare is expected to have an average life expectancy of 30.8 years, on average.\n\n\n\nThe coefficients\nGenerally:\n\nA one-unit change in \\(X_j\\) is associated with a \\(\\beta_j\\)-unit effect on \\(Y\\), on average and holding all else constant.\n\nOur model:\n\nA one-unit increase in a country’s logged GDP per capita is associated with an increase in its average life expectancy of 4.68 years, on average and holding all else constant.\n\n\nA one percentage point increase in a country’s healthcare expenditure is associated with an increase in its average life expectancy of 0.11 years, on average and holding all else constant.\n\n\n\n\nPredicting our outcome\nWe can use our model to predict a country’s average life expectancy based on its (logged) GDP per capita and health expenditure.\nFor example, what is our predicted average life expectancy for a country with a GDP per capita of $20,000 and a health expenditure of 10 percent of its GDP? We can use broom::augment() to answer this question:\n\npred &lt;- augment(m_multi, \n                newdata = tibble(logged_gdp_per_cap = log(20000), \n                                 health_expend = c(10, 11)))\npred\n\n# A tibble: 2 × 3\n  logged_gdp_per_cap health_expend .fitted\n               &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt;\n1               9.90            10    78.3\n2               9.90            11    78.4\n\n\nWe expect that this country would have an average life expectancy of 78.3, 78.4 years.\n\n\n\n\n\n\nTip\n\n\n\nIf you head back up to the interactive linear plane graph, you can find the predicted values for all of the included combinations of logged GDP per capita and health expenditure.\n\n\n\n\nHow confident are we in our estimates?\n\nStatistical significance of our estimates\nWe interpret the statistical significance of our estimates exactly as we did with our binary regression model. If the null hypothesis were true, our estimates scaled against their standard errors would fall along a t-distribution. We can see how far our observed estimates scaled against their standard errors (or the t-statistic) falls from zero. In other words, we can calculate the probability that we would observe our estimate or a more extreme estimate if there was, in fact, no relationship between our independent variable and the outcome.\nWe can use broom::tidy() to calculate these t-statistics and their associated p-values:\n\ntidy(m_multi)\n\n# A tibble: 3 × 5\n  term               estimate std.error statistic  p.value\n  &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)          30.8       1.93     15.9   5.16e-36\n2 logged_gdp_per_cap    4.68      0.231    20.3   5.06e-48\n3 health_expend         0.106     0.112     0.954 3.41e- 1\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe will not discuss how to calculate the standard error for our estimates in this course. You can look forward to that in GVPT722.\n\n\n\n\nConfidence intervals around our estimates\nWe know that our coefficient estimates are merely point estimates of the true association between a country’s logged GDP per capita or health expenditure and its average life expectancy. Using a different simple random sample from our population, we would find different point estimates of these relationships. Let’s build out the range of plausible coefficient estimates.\nWe are working with the expected average value of \\(Y\\) for each \\(X_j\\). Therefore, our coefficient estimates drawn from our null world will follow a t-distribution. We can calculate our confidence interval around our estimated \\(\\beta_j\\) using the usual (if more generalized):\n\\[\nCI_{\\beta_j} = \\beta_j \\pm t*SE_{\\beta_j}\n\\]\nWe can also use broom::tidy() to calculate our confidence intervals:\n\ntidy(m_multi, conf.int = T)\n\n# A tibble: 3 × 7\n  term               estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)          30.8       1.93     15.9   5.16e-36   27.0      34.6  \n2 logged_gdp_per_cap    4.68      0.231    20.3   5.06e-48    4.23      5.14 \n3 health_expend         0.106     0.112     0.954 3.41e- 1   -0.114     0.326\n\n\n\n\n\nHow confident are we in our model overall?\n\n\\(R^2\\)\nThe \\(R^2\\) value tells us the total amount of variation in our outcome that is explained by the model as a whole. In other words, how much variation in \\(Y\\) is explained by variation in all \\(X_j\\)s.\nWe can use broom::glance() to calculate this:\n\nglance(m_multi) |&gt; \n  select(r.squared)\n\n# A tibble: 1 × 1\n  r.squared\n      &lt;dbl&gt;\n1     0.723\n\n\n\n\nF-test\nThe F-test asks whether the entire regression model adds predictive power. Formally, it tests whether all of the coefficients are equal to zero.\n\\[\nH_0: \\beta_1 = \\beta_2 = \\beta_3 = ... \\beta_k = 0\n\\]\nGenerally, when the null hypothesis is true, the ratio of the total sum of squares (the F-statistic) follows the F-distribution. Therefore, we can determine the likelihood that we would find our observed ratio of the total sum of squares if the true ratio of the total sum of squares were zero.\n\n\n\n\n\n\nTip\n\n\n\nThis is the same logic as the T-test.\n\n\n\n\n\n\n\n\nNote\n\n\n\nGelman, Hill, and Vehtari (2020) do not recommend using an F-test. Like the t-test, the F-test asks whether our coefficients are significantly different from zero. There are two issues with this approach. First, there are very few circumstances in which we would expect the association between two variables to be exactly zero. Therefore, this approach asks us to reject something that we do not seriously believe to ever be true. Second, significance tests are sensitive to the amount of data you use. With a large enough \\(n\\) you can reject any null hypothesis.\nFor their recommended approaches, please refer to their section on cross validation.\n\n\nThe F-test is printed at the bottom of the summary() output:\n\nsummary(m_multi)\n\n\nCall:\nlm(formula = life_exp ~ logged_gdp_per_cap + health_expend, data = health_gdp_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.0753  -1.9420   0.5623   2.5805   8.3517 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         30.8036     1.9340  15.928   &lt;2e-16 ***\nlogged_gdp_per_cap   4.6836     0.2310  20.275   &lt;2e-16 ***\nhealth_expend        0.1064     0.1115   0.954    0.341    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.091 on 177 degrees of freedom\n  (37 observations deleted due to missingness)\nMultiple R-squared:  0.7225,    Adjusted R-squared:  0.7194 \nF-statistic: 230.4 on 2 and 177 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "content/13-conclusion.html#section",
    "href": "content/13-conclusion.html#section",
    "title": "Regression Analysis Extensions & Final Exam Review",
    "section": "Section",
    "text": "Section"
  },
  {
    "objectID": "content/04-predicting_outcomes.html",
    "href": "content/04-predicting_outcomes.html",
    "title": "Predicting Outcomes Using Linear Regression",
    "section": "",
    "text": "library(tidyverse)\nlibrary(poliscidata)\n\n\nggplot(states, aes(x = ba_or_more / 100, y = vep12_turnout / 100)) + \n  geom_point() + \n  theme_minimal() + \n  labs(x = \"Percent of population with at least a Bachelors degree\",\n       y = \"Voter turnout rate\") + \n  scale_x_continuous(labels = scales::percent) + \n  scale_y_continuous(labels = scales::percent)\n\n\n\n\n\nggplot(states, aes(x = ba_or_more / 100, y = vep12_turnout / 100)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = F) + \n  theme_minimal() + \n  labs(x = \"Percent of population with at least a Bachelors degree\",\n       y = \"Voter turnout rate\") + \n  scale_x_continuous(labels = scales::percent) + \n  scale_y_continuous(labels = scales::percent)"
  },
  {
    "objectID": "content/02-experiments.html",
    "href": "content/02-experiments.html",
    "title": "Research Design",
    "section": "",
    "text": "library(tidyverse)"
  },
  {
    "objectID": "content/02-experiments.html#set-up",
    "href": "content/02-experiments.html#set-up",
    "title": "Research Design",
    "section": "",
    "text": "library(tidyverse)"
  },
  {
    "objectID": "content/02-experiments.html#introduction",
    "href": "content/02-experiments.html#introduction",
    "title": "Research Design",
    "section": "Introduction",
    "text": "Introduction\nPolitical scientists use empirical analysis to determine whether their theories play out in the real-world. Broadly, there are two ways to do this: experiments and observational studies. In experiments, the researcher controls how important variables change, observing their effects in a controlled setting. In observational studies, the researcher has no control over how important variables change. They must simply observe these changes and their effects.\nExperiments remain the “gold standard” for evaluating causal explanations of outcomes. However, observational studies are much more common. We will discuss both this week."
  },
  {
    "objectID": "content/02-experiments.html#experiments",
    "href": "content/02-experiments.html#experiments",
    "title": "Research Design",
    "section": "Experiments",
    "text": "Experiments\nIn The Observer Effect in International Politics: Evidence from a Natural Experiment, Hyde (2007) explores whether international election monitoring increases the chances that those elections will be free and fair. Specifically, she wants to determine whether international monitors reduce the prevalence of fraud occurring at the polling stations to which they are assigned. To test this, she used an experiment.\nDuring the 2003 presidential elections in Armenia, international monitors were randomly assigned to polling stations. If election monitors significantly reduce a bad actor’s ability to rig an election on election day, their vote share in monitored booths should be lower than that in which their behavior was not observed. In her words (page 39):\n\n“If the presence of international observers causes a reduction in election-day fraud, the effect of observers should be visible at the subnational level by comparing polling stations that were visited by observers with those that were not visited. More specifically, if international monitoring reduces election day fraud directly, all else held equal, the cheating parties should gain less of their ill-gotten vote share in polling stations that were visited by international monitors.”\n\nIn summary, following from her theory (provided in the paper), she argues that the presence of international observers at a polling station causes less election-day fraud at that station:\n\\[\nElection\\ monitoring \\rightarrow Less\\ election\\ day\\ fraud\n\\]\nHow can experiments help prove this? Hyde was interested in estimating the effect of election monitoring on each individual polling station. Was the vote share the cheating parties received at a monitored station significantly smaller than it would have been if the monitors were not there? Unfortunately for Hyde (and all researchers hoping to identify the causes of outcomes of interest), we cannot observe the counter-factual: the vote share cheating parties would have received if the monitor were not there.\nIdeally, we could operate in parallel worlds: one in which all stations were monitored, and one in which none were. For each polling station, \\(i\\), of the 1,763 that were set up for the 2003 election we would record the vote share the cheating party received without election monitors and that they received with election monitors:\n\n\n\n\n\n\nNote\n\n\n\nI made these data up! These are not the data collected by Hyde (2007). I have made them extreme for illustrative purposes.\n\n\n\nvec_cheating_party_monitoring &lt;- round(rbeta(1763, 2, 11) * 100, 2)\nvec_cheating_party_no_monitoring &lt;- round(rbeta(1763, 6, 4) * 100, 2)\n\nsim_election_monitoring &lt;- tibble(station_id = 1:1763, \n                                  cheating_party_monitoring = vec_cheating_party_monitoring,\n                                  cheating_party_no_monitoring = vec_cheating_party_no_monitoring)\n\nsim_election_monitoring\n\n# A tibble: 1,763 × 3\n   station_id cheating_party_monitoring cheating_party_no_monitoring\n        &lt;int&gt;                     &lt;dbl&gt;                        &lt;dbl&gt;\n 1          1                     40.2                          72.7\n 2          2                      5.11                         51.0\n 3          3                     29.8                          73.1\n 4          4                      5.81                         83.1\n 5          5                     24.7                          73.0\n 6          6                     18.6                          60.0\n 7          7                     20.0                          72.0\n 8          8                      6.94                         41.3\n 9          9                      6.18                         36.3\n10         10                     16.4                          46.2\n# ℹ 1,753 more rows\n\n\nWe could then compare these two vote shares and determine whether the election monitors substantially drove down the cheating parties’ vote shares:\n\nsim_election_monitoring |&gt; \n  mutate(difference = cheating_party_monitoring - cheating_party_no_monitoring)\n\n# A tibble: 1,763 × 4\n   station_id cheating_party_monitoring cheating_party_no_monitoring difference\n        &lt;int&gt;                     &lt;dbl&gt;                        &lt;dbl&gt;      &lt;dbl&gt;\n 1          1                     40.2                          72.7      -32.5\n 2          2                      5.11                         51.0      -45.9\n 3          3                     29.8                          73.1      -43.2\n 4          4                      5.81                         83.1      -77.3\n 5          5                     24.7                          73.0      -48.3\n 6          6                     18.6                          60.0      -41.4\n 7          7                     20.0                          72.0      -52.1\n 8          8                      6.94                         41.3      -34.4\n 9          9                      6.18                         36.3      -30.2\n10         10                     16.4                          46.2      -29.8\n# ℹ 1,753 more rows\n\n\nThe election monitors are the only difference between these two parallel worlds. Therefore, we could definitively state that they caused these differences in vote shares. Those differences are large, so we could confidently say that election monitors substantially reduce the vote share received by the cheating party at each polling station.\nWe cannot, of course, create parallel worlds. We only observe the factual outcome: each party’s vote share at a given station under whichever condition (monitored or not) it received in reality. We never observe the counter-factual outcome: each party’s vote share at a given station under the other condition.\nIn Hyde’s case, she only has the cheating party’s vote share at a given station under whichever condition it received:\n\nvec_monitored &lt;- rbinom(1763, 1, 0.5)\n\nsim_factual_monitoring &lt;- sim_election_monitoring |&gt; \n  mutate(monitored = vec_monitored,\n         cheating_party_monitoring = if_else(monitored == 1, cheating_party_monitoring, NA_real_),\n         cheating_party_no_monitoring = if_else(monitored == 0, cheating_party_no_monitoring, NA_real_)) |&gt; \n  relocate(monitored, .after = station_id)\n\nsim_factual_monitoring\n\n# A tibble: 1,763 × 4\n   station_id monitored cheating_party_monitoring cheating_party_no_monitoring\n        &lt;int&gt;     &lt;int&gt;                     &lt;dbl&gt;                        &lt;dbl&gt;\n 1          1         1                     40.2                          NA  \n 2          2         1                      5.11                         NA  \n 3          3         1                     29.8                          NA  \n 4          4         1                      5.81                         NA  \n 5          5         1                     24.7                          NA  \n 6          6         0                     NA                            60.0\n 7          7         1                     20.0                          NA  \n 8          8         0                     NA                            41.3\n 9          9         0                     NA                            36.3\n10         10         0                     NA                            46.2\n# ℹ 1,753 more rows\n\n\nSo, how do we work out whether election monitors deter or prevent election-day fraud?\nAverage causal effects\nWe need to find good approximations of the counter-factuals for each polling station. To do this, we need to move away from focusing on the individual polling stations and, instead, look at the average vote share across all stations that were monitored compared to that across all stations that were not monitored.\nLet’s look back at our impossible parallel worlds:\n\nsim_election_monitoring\n\n# A tibble: 1,763 × 3\n   station_id cheating_party_monitoring cheating_party_no_monitoring\n        &lt;int&gt;                     &lt;dbl&gt;                        &lt;dbl&gt;\n 1          1                     40.2                          72.7\n 2          2                      5.11                         51.0\n 3          3                     29.8                          73.1\n 4          4                      5.81                         83.1\n 5          5                     24.7                          73.0\n 6          6                     18.6                          60.0\n 7          7                     20.0                          72.0\n 8          8                      6.94                         41.3\n 9          9                      6.18                         36.3\n10         10                     16.4                          46.2\n# ℹ 1,753 more rows\n\n\nThe cheating party’s average vote share if all stations were monitored is 15.19%.\nIn R:\n\nsim_election_monitoring |&gt; \n  summarise(mean(cheating_party_monitoring)) |&gt; \n  pull()\n\n[1] 15.18991\n\n\nAnd its average vote share if no stations were monitored is a whopping 59.3%.\nIn R:\n\nsim_election_monitoring |&gt; \n  summarise(mean(cheating_party_no_monitoring)) |&gt; \n  pull()\n\n[1] 59.30277\n\n\nWe can conclude from these parallel worlds that, on average, election monitors caused a 44.11% reduction in the cheating party’s vote share at any given polling station.\nAgain, we can’t split the world into two parallel worlds for our political science experiments. Instead we need to find an appropriate proxy for those counter-factual vote shares. The best way to do this is using a randomized experiment (also sometimes referred to as a randomized control trial).\nThe ultimate goal is to find two groups that are, on average, identical prior to the treatment. Then, once we treat one of the groups, we can infer that any differences between these two previously identical groups were caused by our treatment.\nThe best way to do this is to randomly assign individual actors to a treatment or control group. Here, we randomly decide whether each given polling station will be monitored. Above, I used R to do this (using the rbinom() function). Those stations that will be monitored make up the treatment group. Those that will not be monitored make up the control group.\nWhy is randomization the best way to ensure that both of our groups are identical prior to treatment? Because it ensures that the only thing that systematically distinguishes the treatment from the control group is that those stations will be monitored. Yes, there are differences between each individual polling station (for example, its geographic location, size, or turn-out) that will have an effect on each party’s vote share. But, because the treatment was dispursed randomly, the two groups (monitored or not) can be compared, on average. Random treatment makes the treatment and control groups, on average, identical to each other.1\n\n\n\n\n\n\nDon’t take my word for it\n\n\n\nTo illustrate this point, let’s briefly step away from the 2003 Armenian election. Say I have a group of 1,000 different people. All I know or care about them is their height.\nLet’s create this group of 1,000 people:\n\nsim_heights &lt;- tibble(\n  id = 1:1000,\n  height = rnorm(1000, 170, 6)\n)\n\nsim_heights\n\n# A tibble: 1,000 × 2\n      id height\n   &lt;int&gt;  &lt;dbl&gt;\n 1     1   168.\n 2     2   167.\n 3     3   170.\n 4     4   174.\n 5     5   165.\n 6     6   161.\n 7     7   166.\n 8     8   168.\n 9     9   160.\n10    10   178.\n# ℹ 990 more rows\n\n\nThese 1,000 have heights ranging from 148 cm to a towering 187 cm. Let’s take a look at them:\n\nggplot(sim_heights, aes(x = height)) + \n  geom_histogram() + \n  theme_minimal() + \n  labs(x = \"Height (cm)\",\n       y = \"Count\")\n\n\n\n\nLet’s split this group up entirely randomly:\n\nassigned_heights &lt;- sim_heights |&gt; \n  rowwise() |&gt; \n  mutate(assignment = rbinom(1, 1, 0.5))\n\nassigned_heights\n\n# A tibble: 1,000 × 3\n# Rowwise: \n      id height assignment\n   &lt;int&gt;  &lt;dbl&gt;      &lt;int&gt;\n 1     1   168.          1\n 2     2   167.          0\n 3     3   170.          1\n 4     4   174.          0\n 5     5   165.          0\n 6     6   161.          0\n 7     7   166.          1\n 8     8   168.          0\n 9     9   160.          0\n10    10   178.          0\n# ℹ 990 more rows\n\n\nAnd calculate the average heights of those groups:\n\nassigned_heights |&gt; \n  group_by(assignment) |&gt; \n  summarise(mean_height = mean(height))\n\n# A tibble: 2 × 2\n  assignment mean_height\n       &lt;int&gt;       &lt;dbl&gt;\n1          0        170.\n2          1        170.\n\n\nPretty close to identical (they are off by a small number of hundreths of a centimeter)! Remember, the only difference between these groups is their random assignment.\nLet’s do it again to make sure it wasn’t a fluke:\n\nsim_heights |&gt; \n  rowwise() |&gt; \n  mutate(assignment = rbinom(1, 1, 0.5)) |&gt; \n  group_by(assignment) |&gt; \n  summarise(mean_height = mean(height))\n\n# A tibble: 2 × 2\n  assignment mean_height\n       &lt;int&gt;       &lt;dbl&gt;\n1          0        169.\n2          1        170.\n\n\nClose again! We will talk about the law of large numbers and the central limit theorem in coming classes, but it is sufficient for now to say that if we did this many, many times, the averages of these two groups would be increasingly similar. For now, I will do this an additional 1,000 times:\n\nrandom_assignments &lt;- function(trial) {\n  \n  sim_heights |&gt; \n    rowwise() |&gt; \n    mutate(assignment = rbinom(1, 1, 0.5)) |&gt; \n    group_by(assignment) |&gt; \n    summarise(mean_height = mean(height)) |&gt; \n    mutate(trial_no = trial)\n  \n}\n\nmap(1:1000, random_assignments) |&gt; \n  bind_rows() |&gt; \n  group_by(assignment) |&gt; \n  summarise(mean_mean_height = mean(mean_height))\n\n# A tibble: 2 × 2\n  assignment mean_mean_height\n       &lt;int&gt;            &lt;dbl&gt;\n1          0             170.\n2          1             170.\n\n\nIn theory, if we did this an infinite number of times, those averages would be identical.\n\n\nRemember, our goal is to create two groups of polling stations that are, on average, identical prior to the treatment. We can do this by randomly assigning each station to be monitored or not.\nThis allows us to treat the cheating party’s average vote share across all monitored stations as indicative of its vote share across all stations if they were all monitored. Similarly, we can treat the cheating party’s average vote share across all non-monitored stations as indicative of its vote share across all stations if none were monitored. The difference between these two average vote shares is our average treatment effect. In other words, the difference between the average vote share across monitored stations and that across non-monitored stations represents the effect of international monitoring on election-day fraud. This method is call the difference-of-means test.\nLet’s look at our observed data set:\n\nsim_factual_monitoring\n\n# A tibble: 1,763 × 4\n   station_id monitored cheating_party_monitoring cheating_party_no_monitoring\n        &lt;int&gt;     &lt;int&gt;                     &lt;dbl&gt;                        &lt;dbl&gt;\n 1          1         1                     40.2                          NA  \n 2          2         1                      5.11                         NA  \n 3          3         1                     29.8                          NA  \n 4          4         1                      5.81                         NA  \n 5          5         1                     24.7                          NA  \n 6          6         0                     NA                            60.0\n 7          7         1                     20.0                          NA  \n 8          8         0                     NA                            41.3\n 9          9         0                     NA                            36.3\n10         10         0                     NA                            46.2\n# ℹ 1,753 more rows\n\n\nFirst, we find the average across our treatment and control groups:\n\nsim_averages &lt;- sim_factual_monitoring |&gt; \n  summarise(cheating_party_monitoring = mean(cheating_party_monitoring, na.rm = T),\n            cheating_party_no_monitoring = mean(cheating_party_no_monitoring, na.rm = T))\n\nsim_averages\n\n# A tibble: 1 × 2\n  cheating_party_monitoring cheating_party_no_monitoring\n                      &lt;dbl&gt;                        &lt;dbl&gt;\n1                      15.1                         59.1\n\n\nThen we find the difference between these two vote shares:\n\nsim_averages |&gt; \n  mutate(difference = cheating_party_monitoring - cheating_party_no_monitoring)\n\n# A tibble: 1 × 3\n  cheating_party_monitoring cheating_party_no_monitoring difference\n                      &lt;dbl&gt;                        &lt;dbl&gt;      &lt;dbl&gt;\n1                      15.1                         59.1      -44.0\n\n\nWe can say that, on average, election monitors reduced the cheating party’s vote share by 43.98 percentage points. This difference is substantial!2 It is the difference between a land-slide win and an embarrassing loss.\nExperiments are wonderful because they allow us to control which variables change and which are held constant. In turn, this allows us to isolate which variables cause changes to our outcome of interest. However, we cannot always run experiments to test our theories. When this is the case (which it often is), we need to rely on observational data to test whether our theories play out in the real-world."
  },
  {
    "objectID": "content/02-experiments.html#observational-studies",
    "href": "content/02-experiments.html#observational-studies",
    "title": "Research Design",
    "section": "Observational studies",
    "text": "Observational studies\nObservational studies are attempts to learn about the effect of a change in some variable on our outcome of interest. However, unlike in experiments, we cannot control when or how those variables change. We can only observe and record their changes and corresponding changes to our outcome of interest.\nGenerally, you cannot make causal statements about findings made using observational studies. For example, if election monitors were not assigned randomly to polling stations in the 2003 Armenian Presidential elections, Hyde could not claim that these monitors caused, on average, a reducing in vote share for the cheating party. Instead, she could only claim that monitors were associated with a reduction in vote share for the cheating party.\nWhy is this the case? In observational studies, it is rarely the case that our groups (treatment and control) were identical prior to their treatment. For example, imagine if the decision to monitor each polling station was not random. Instead, the international monitoring group made the decision as to whether to monitor a station based on logistical reasons.\nImagine that the group (which has limited funds) decides to monitor stations in cities, which are close to transport hubs and have lots of accommodation options. Imagine also that our cheating party actually enjoys a lot of legitimate support in cities. Instead, it directs its limited pool of thugs and ballot-stuffers out to the regions where it anticipates having a very low vote share.\nHere, we would observe that the cheating party receives a high vote share in both monitored and non-monitored stations. It received high levels of support from voters in the city (who voted at stations that were disproportionately likely to be monitored and less likely to be ballot-stuffed) and (ill-gotten) high levels of support from voters in the regions (who voted at stations that were disproportionately less likely to be monitored and more likely to be ballot-stuffed). From this we might conclude that election monitors have no substantial effect on election-day fraud: there is no large difference in the cheating party’s vote share between those stations that were monitored and those that were not.\nHowever, we have not actually tested our theory! Not all is lost with observational data. We just need to be very careful about controlling for those variables that might be influencing both our treatment (monitored or not) and outcome (vote share for the cheating party).\nLet’s create some data to illustrate this point.\n\n\n\n\n\n\nTip\n\n\n\nI will discuss the various methods I am using to generate these data in class. For now, you can note that I make the cheating party’s vote share at stations increasingly small under the following conditions: from stations that are in the city and not monitored, in the city and monitored, in the regions and not monitored, and in the regions and monitored.\n\n\n\nvec_in_city &lt;- rbinom(1763, 1, 0.3)\n\nsim_observed_data &lt;- tibble(station_id = 1:1763,\n       in_city = vec_in_city) |&gt; \n  rowwise() |&gt; \n  mutate(monitored = if_else(in_city == 1, rbinom(1, 1, 0.6), rbinom(1, 1, 0.2)), \n         cheating_party_vote_share = case_when(in_city == 0 & monitored == 1 ~ rbeta(1, 2, 11),\n                                               in_city == 1 & monitored == 1 ~ rbeta(1, 4, 4),\n                                               in_city == 0 & monitored == 0 ~ rbeta(1, 2, 4),\n                                               in_city == 1 & monitored == 0 ~ rbeta(1, 6, 4)))\n\nsim_observed_data\n\n# A tibble: 1,763 × 4\n# Rowwise: \n   station_id in_city monitored cheating_party_vote_share\n        &lt;int&gt;   &lt;int&gt;     &lt;int&gt;                     &lt;dbl&gt;\n 1          1       1         0                    0.512 \n 2          2       0         0                    0.345 \n 3          3       1         1                    0.571 \n 4          4       0         0                    0.0254\n 5          5       0         0                    0.349 \n 6          6       0         1                    0.104 \n 7          7       1         0                    0.313 \n 8          8       0         0                    0.189 \n 9          9       0         0                    0.297 \n10         10       0         1                    0.0811\n# ℹ 1,753 more rows\n\n\nWhat is the difference in average vote shares between stations that were monitored and those that were not?\n\navg_diff_observed &lt;- sim_observed_data |&gt; \n  group_by(monitored) |&gt; \n  summarise(avg = mean(cheating_party_vote_share))\n\navg_diff_observed\n\n# A tibble: 2 × 2\n  monitored   avg\n      &lt;int&gt; &lt;dbl&gt;\n1         0 0.376\n2         1 0.344\n\n\nA relatively small 3% reduction in vote share. But what happens if we control for whether the station was in the city or not?\n\nsim_observed_data |&gt; \n  group_by(monitored, in_city) |&gt; \n  summarise(avg = mean(cheating_party_vote_share)) |&gt; \n  arrange(in_city) |&gt; \n  group_by(in_city) |&gt; \n  mutate(difference = scales::percent(avg - lag(avg)))\n\n# A tibble: 4 × 4\n# Groups:   in_city [2]\n  monitored in_city   avg difference\n      &lt;int&gt;   &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;     \n1         0       0 0.326 &lt;NA&gt;      \n2         1       0 0.149 -18%      \n3         0       1 0.601 &lt;NA&gt;      \n4         1       1 0.502 -10%      \n\n\nWhen we compare similar (if not identical) groups, we find a much larger effect. We have attempted to correct for dissimilarities between our groups prior to treatment. In doing so, we get a much better understanding of the role international election monitors play in deterring or preventing election-day fraud."
  },
  {
    "objectID": "content/02-experiments.html#conclusion",
    "href": "content/02-experiments.html#conclusion",
    "title": "Research Design",
    "section": "Conclusion",
    "text": "Conclusion\nAt the end of the day, we are attempting to prove that our theories of how a change in some variable - for example, international election monitoring - causes a change in some outcome of interest - here, election-day fraud. We can only do this when we identify two groups (our treatment and control) that are identical to each other except for the treatment.\nIn absence of our ability to create parallel worlds, our best shot at this is through randomized experiments. When we assign our subjects to be treated (monitored) or stay as controls (not monitored) in a completely random way, we end up with two groups that are identical prior to the treatment. We can then confidently say that our treatment caused the difference between the average outcome in our two groups.\nOftentimes, we cannot run experiments to test our theories. Instead, we must rely on observational studies. We use careful analysis to ensure that we get as close to identical groups as we can prior to their treatment. We can then look at the average differences between these carefully constructed groups to glean whether our outcome of interest is substantially different (on average) between those that were treated and those that were not."
  },
  {
    "objectID": "content/02-experiments.html#footnotes",
    "href": "content/02-experiments.html#footnotes",
    "title": "Research Design",
    "section": "Footnotes",
    "text": "Footnotes\n\nSometimes this doesn’t work. Sometimes, you will randomly assign two groups and they will be very different from each other. Please don’t get distracted by this just yet. As long as you have a large population from which you are drawing, this should be very unlikely.↩︎\nPlease remember, I made these data up. Do not go away from this thinking that the 2003 Armenian election was this badly corrupt.↩︎"
  }
]