[
  {
    "objectID": "content/06-probability_theory.html",
    "href": "content/06-probability_theory.html",
    "title": "Probability Theory",
    "section": "",
    "text": "Pollock & Edwards, Chapter 5\n\n\n\n Pollock & Edwards R Companion, Chapter 6"
  },
  {
    "objectID": "content/06-probability_theory.html#readings",
    "href": "content/06-probability_theory.html#readings",
    "title": "Probability Theory",
    "section": "",
    "text": "Pollock & Edwards, Chapter 5\n\n\n\n Pollock & Edwards R Companion, Chapter 6"
  },
  {
    "objectID": "content/06-probability_theory.html#class-slides",
    "href": "content/06-probability_theory.html#class-slides",
    "title": "Probability Theory",
    "section": "Class slides",
    "text": "Class slides"
  },
  {
    "objectID": "content/06-probability_theory.html#section",
    "href": "content/06-probability_theory.html#section",
    "title": "Probability Theory",
    "section": "Section",
    "text": "Section\n\nPrerequisites\n\nlibrary(tidyverse)\nlibrary(janitor)\n\nset.seed(1234)\n\n\nToday, we are working with randomness and chance. To make sure we are all working with the same randomness and chance, you need to set your seed! set.seed() sets the random number generator state in which R will operate for this session. The draws we make in the session are still random, but they will be the same random draw each time we take it. This is important for replication, so you will use it outside of class.\n\n\n\nRandomness and avoiding doing the dishes\nImagine you and a friend are trying to decide who will do the dishes after you have both cooked a very large and very messy meal. You agree to flip a coin. Should you pick heads or tails?\nYou really don’t want to do the dishes. Therefore, you want to maximize your chances of winning the coin flip. If you could predict the outcome of the coin flip with certainty, you would simply pick the winning side. Even if you don’t know for certain which side will land on top, you want to pick the side that has the highest chance of winning. How can you work this out?\nFirst, you need to work out all the possible outcomes. This task is simple for a coin flip: heads or tails.\n\npossible_outcomes &lt;- c(\"HEADS\", \"TAILS\")\npossible_outcomes\n\n[1] \"HEADS\" \"TAILS\"\n\n\nThen, you need to work out how likely each of those outcomes are to eventuate. How can we do this? One option available to us here is repeated trials. Flip your coin many times and record how many heads and tails you get. This provides you with a rough understanding of the chance that your coin will land on heads or tails for any given flip.\nFor example, you can flip the coin 10 times and record the results of each flip.\n\nrepeat_trials &lt;- sample(possible_outcomes, size = 10, replace = T, prob = c(0.5, 0.5))\nrepeat_trials\n\n [1] \"TAILS\" \"HEADS\" \"HEADS\" \"HEADS\" \"HEADS\" \"HEADS\" \"TAILS\" \"TAILS\" \"HEADS\"\n[10] \"HEADS\"\n\n\n\nWe want to sample with replacement, so we include the argument replace = T. This just means that we include all possible outcomes in every draw. If we sampled without replacement, we would remove each outcome from the sample after it has been selected in a previous draw.\nFor example, imagine you have 10 different colored marbles in a bag. You pull out a marble and record its color. If you want to sample with replacement, you put the marble you just pulled out back into the bag before you take your next draw. This means that you can draw that same marble out again in the subsequent draws.\n\nYou can then tally up those results to get your baseline understanding of the chances of heads vs tails.\n\ntable(repeat_trials)\n\nrepeat_trials\nHEADS TAILS \n    7     3 \n\n\nGiven the results of this trial, I would expect that for every 10 coin flips, I should get 7 heads and 3 tails. If I want to use this information to determine the chance that the coin will land on its head after one flip, I can convert this to percentages. There is a 70% chance that the coin will land on heads. Therefore, based solely on the 10 flip trial, you should pick heads.\nHmm… but aren’t fair coins meant to land on heads or tails with equal probability? In fact, if you look back to our sample (drawn using the sample() function) you will see that I explicitly set the probability of landing on heads and tails to be an even 0.5 and 0.5 each (using the prob argument). Why then are we getting 70% for heads and 30% for tails instead of 50% and 50%?\nTo answer this question, we need to build up some foundations in probability theory. Let’s start with independence.\n\n\nIndependence\nYou want to maximize your chances of not doing the dishes (i.e. of picking the winning side of the coin). To do this, you need to know all possible outcomes (heads and tails) and the probability that each of those outcomes will eventuate. To learn this, you ran a trial in which you flipped the coin 10 times and recorded the outcome of each flip. How can you trust that this trial is revealing the true underlying probabilities of heads vs. tails?\nEach time you flipped that coin, you undertook the very process you will eventually take to decide who has to do the dishes. You will only flip that deciding coin once, so you need to know what the chances are that the coin will land on heads or tails that one time. You can’t ever observe that. If you flip a coin once, you will either see heads or tails. But we know that if we flip it again we might get a different outcome. In other words, if you flip a coin once and it lands on heads, this does not necessarily mean that the probability of heads is 1 and the probability of tails is 0 (or that you will always get heads). We use trials to try to estimate the unobservable underlying probabilities of each possible outcome of a single coin flip.\nTo make sure that we can infer from our observed flips the underlying and unobservable probability of heads vs. tails of one coin flip, we need to make sure that our trials meet certain conditions. The first is independence: the outcome of any other flips cannot impact the outcome of the current flip.\nFor example, let’s go back to our bag of 10 different marbles. Say there are 2 red, 3 blue, and 5 green marbles in your bag. You want to know the probability of drawing out a green marble. You pull out a marble. We know that there is a 20% chance your marble will be red, a 30% chance it will be blue, and a 50% chance it will be green. It is green. You then do not replace the marble before your next draw. Now, there is a 22% chance that marble will be red, a 33% chance it is blue, and a 44% chance it will be green (there are now only nine marbles in your bag: 2 red, 3, blue, and 4 green). These draws are not independent of each other! Your first draw changed the underlying probability of drawing a green marble in your second draw.\nIf your draws are independent of one another, you can infer from the results of the trial the underlying probability of each outcome eventuating.\nBut hold on: we did that and we still got uneven results!\n\ntable(repeat_trials)\n\nrepeat_trials\nHEADS TAILS \n    7     3 \n\n\nWhy?\n\n\nThe law of large numbers\nIn short, our trial was too small.\nEven if our underlying probability is {0.5, 0.5} (which it is: remember that prob = c(0.5, 0.5) argument), we may observe a set of outcomes in our trial that do not reflect this true distribution.\nTo illustrate, think of the outcome you could observe from only one draw: heads or tails. If you draw heads and then use that trial to infer the underlying probability of drawing heads vs. tails, you will state that the underlying probability of drawing a head and tail is equal to {1, 0}. You will be very surprised if you subsequently flip a tail.\nNow, what if you run a trial of two flips?\n\nsample(possible_outcomes, 2, replace = T, prob = c(0.5, 0.5))\n\n[1] \"HEADS\" \"HEADS\"\n\n\nBoth heads!\nIn fact, if we flip a coin twice many times (say, 10 times), we will probably get a couple of trials in which we flip two heads or two tails:\n\ntrial_two_flips &lt;- tibble(trial = 1:10) |&gt; \n  rowwise() |&gt; \n  mutate(outcome = list(sample(possible_outcomes, 2, replace = T, prob = c(0.5, 0.5)))) |&gt; \n  unnest_wider(outcome) |&gt; \n  rename(\"flip_1\" = `...1`, \"flip_2\" = `...2`)\n\ntrial_two_flips\n\n# A tibble: 10 × 3\n   trial flip_1 flip_2\n   &lt;int&gt; &lt;chr&gt;  &lt;chr&gt; \n 1     1 TAILS  HEADS \n 2     2 TAILS  HEADS \n 3     3 TAILS  TAILS \n 4     4 TAILS  TAILS \n 5     5 TAILS  TAILS \n 6     6 TAILS  TAILS \n 7     7 TAILS  HEADS \n 8     8 HEADS  HEADS \n 9     9 HEADS  TAILS \n10    10 TAILS  TAILS \n\n\n6 or 60% of our 10 trials resulted in two of the same outcomes. How can we be confident that our trials are good reflections of the actual distribution of probabilities?\nThe law of large numbers suggests that when your population of independent observations has a finite mean, as the number of observations drawn increases, the mean of the observed values in the sample approaches the mean of the population.\nIn other words, the more flips you do, the closer you will get to the true underlying distribution of probabilities. Cool!\nLet’s try this out.\nFirst, let’s flip the coin 10 times:\n\nThe red dots are sitting at {0.5,0.5}. We are aiming for this true probability distribution.\n\n\ntibble(outcome = sample(possible_outcomes, 10, replace = T, prob = c(0.5, 0.5))) |&gt; \n  tabyl(outcome) |&gt; \n  ggplot(aes(y = outcome, x = percent)) + \n  geom_col() + \n  geom_point(aes(x = c(0.5, 0.5)), size = 3, colour = \"red\") + \n  theme_minimal() + \n  scale_x_continuous(limits = c(0, 1))\n\n\n\n\nNow, let’s flip it 100 times:\n\ntibble(outcome = sample(possible_outcomes, 100, replace = T, prob = c(0.5, 0.5))) |&gt; \n  tabyl(outcome) |&gt; \n  ggplot(aes(y = outcome, x = percent)) + \n  geom_col() + \n  geom_point(aes(x = c(0.5, 0.5)), size = 3, colour = \"red\") + \n  theme_minimal() + \n  scale_x_continuous(limits = c(0, 1))\n\n\n\n\nNow, let’s flip it 1,000 times:\n\ntibble(outcome = sample(possible_outcomes, 1000, replace = T, prob = c(0.5, 0.5))) |&gt; \n  tabyl(outcome) |&gt; \n  ggplot(aes(y = outcome, x = percent)) + \n  geom_col() + \n  geom_point(aes(x = c(0.5, 0.5)), size = 3, colour = \"red\") + \n  theme_minimal() + \n  scale_x_continuous(limits = c(0, 1))\n\n\n\n\nNow, let’s flip it 10,000 times:\n\ntibble(outcome = sample(possible_outcomes, 10000, replace = T, prob = c(0.5, 0.5))) |&gt; \n  tabyl(outcome) |&gt; \n  ggplot(aes(y = outcome, x = percent)) + \n  geom_col() + \n  geom_point(aes(x = c(0.5, 0.5)), size = 3, colour = \"red\") + \n  theme_minimal() + \n  scale_x_continuous(limits = c(0, 1))\n\n\n\n\nFinally, let’s flip it 100,000 times:\n\ntibble(outcome = sample(possible_outcomes, 100000, replace = T, prob = c(0.5, 0.5))) |&gt; \n  tabyl(outcome) |&gt; \n  ggplot(aes(y = outcome, x = percent)) + \n  geom_col() + \n  geom_point(aes(x = c(0.5, 0.5)), size = 3, colour = \"red\") + \n  theme_minimal() + \n  scale_x_continuous(limits = c(0, 1))\n\n\n\n\nEach time we increase the number of draws we make, we get closer to the true underlying distribution of probabilities (as coded in our sample() function).\nSo, after all of that how can you avoid doing the dishes? Sadly (and as expected) you cannot get an edge on your friend. All possible outcomes have the same probability of eventuating.\n\n\nWhat does this all have to do with political science?\nFair question.\nQuantitative social science involves statistical inference. We start with a parameter of interest. For example, what proportion of US voters approve of Joe Biden’s job as president? It would be very nice if we could go and ask all US voters what they think and if we could be confident that they are giving us their true opinions. However, this is simply not possible (even the census misses some people!). Instead, we rely on surveys.\nThese surveys (if done well) will take a representative sample of the population of US voters and ask their opinion of Biden. We can then use that sample to infer the overall level of support for Joe Biden among the population (of US voters). Think of a survey as a trial.\nWe can even use this sample to answer interesting questions about groups within the population. What do Republicans think about Joe Biden’s job as president? What about women? Or people of color?\nThis is all statistical inference. Probability theory undergrids our ability to observe or measure variables of interest and use these variables to strengthen our arguments in support of our theory. We will discuss this in more detail next week."
  },
  {
    "objectID": "content/08_hypothesis_testing.html",
    "href": "content/08_hypothesis_testing.html",
    "title": "Hypothesis Testing II",
    "section": "",
    "text": "Pollock & Edwards, Chapter 7\n\n\n\n Pollock & Edwards R Companion, Chapter 7"
  },
  {
    "objectID": "content/08_hypothesis_testing.html#readings",
    "href": "content/08_hypothesis_testing.html#readings",
    "title": "Hypothesis Testing II",
    "section": "",
    "text": "Pollock & Edwards, Chapter 7\n\n\n\n Pollock & Edwards R Companion, Chapter 7"
  },
  {
    "objectID": "content/08_hypothesis_testing.html#class-slides",
    "href": "content/08_hypothesis_testing.html#class-slides",
    "title": "Hypothesis Testing II",
    "section": "Class slides",
    "text": "Class slides"
  },
  {
    "objectID": "content/08_hypothesis_testing.html#section",
    "href": "content/08_hypothesis_testing.html#section",
    "title": "Hypothesis Testing II",
    "section": "Section",
    "text": "Section\n\nPrerequisites\n\nlibrary(tidyverse)\nlibrary(wbstats)\n\n\nregion_gdp_df &lt;- wb_data(\n  \"NY.GDP.MKTP.CD\",\n        start_date = 2021,\n        end_date = 2021,\n        return_wide = F\n) |&gt; \n  transmute(\n    country,\n            region = countrycode::countrycode(iso3c, \"iso3c\", \"region\"),\n            gdp = value\n  )\n\nregion_gdp_df\n\n# A tibble: 217 × 3\n   country             region                               gdp\n   &lt;chr&gt;               &lt;chr&gt;                              &lt;dbl&gt;\n 1 Afghanistan         South Asia                  14583135237.\n 2 Albania             Europe & Central Asia       17930565119.\n 3 Algeria             Middle East & North Africa 163472233246.\n 4 American Samoa      East Asia & Pacific           709000000 \n 5 Andorra             Europe & Central Asia        3325145407.\n 6 Angola              Sub-Saharan Africa          65685435100.\n 7 Antigua and Barbuda Latin America & Caribbean    1560518519.\n 8 Argentina           Latin America & Caribbean  487227125386.\n 9 Armenia             Europe & Central Asia       13861409969.\n10 Aruba               Latin America & Caribbean    3126019385.\n# ℹ 207 more rows\n\n\n\nggplot(region_gdp_df, aes(x = gdp, y = region, fill = region)) + \n  geom_boxplot()"
  },
  {
    "objectID": "content/11-regression.html",
    "href": "content/11-regression.html",
    "title": "Regression I",
    "section": "",
    "text": "Pollock & Edwards, Chapter 8\n\n\n\n Pollock & Edwards R Companion, Chapter 8"
  },
  {
    "objectID": "content/11-regression.html#readings",
    "href": "content/11-regression.html#readings",
    "title": "Regression I",
    "section": "",
    "text": "Pollock & Edwards, Chapter 8\n\n\n\n Pollock & Edwards R Companion, Chapter 8"
  },
  {
    "objectID": "content/11-regression.html#class-slides",
    "href": "content/11-regression.html#class-slides",
    "title": "Regression I",
    "section": "Class slides",
    "text": "Class slides"
  },
  {
    "objectID": "content/11-regression.html#section",
    "href": "content/11-regression.html#section",
    "title": "Regression I",
    "section": "Section",
    "text": "Section"
  },
  {
    "objectID": "content/02-descriptive_statistics.html",
    "href": "content/02-descriptive_statistics.html",
    "title": "Descriptive Statistics",
    "section": "",
    "text": "Pollock & Edwards, Chapters 1-2\n\n\n\n Pollock & Edwards R Companion, Chapters 2-3"
  },
  {
    "objectID": "content/02-descriptive_statistics.html#readings",
    "href": "content/02-descriptive_statistics.html#readings",
    "title": "Descriptive Statistics",
    "section": "",
    "text": "Pollock & Edwards, Chapters 1-2\n\n\n\n Pollock & Edwards R Companion, Chapters 2-3"
  },
  {
    "objectID": "content/02-descriptive_statistics.html#class-slides",
    "href": "content/02-descriptive_statistics.html#class-slides",
    "title": "Descriptive Statistics",
    "section": "Class slides",
    "text": "Class slides"
  },
  {
    "objectID": "content/02-descriptive_statistics.html#section",
    "href": "content/02-descriptive_statistics.html#section",
    "title": "Descriptive Statistics",
    "section": "Section",
    "text": "Section\n\nPrerequisites\n\nlibrary(tidyverse)\nlibrary(poliscidata)\nlibrary(wbstats)\nlibrary(janitor)\nlibrary(skimr)\nlibrary(countrycode)\nlibrary(ggridges)\nlibrary(ggdist)\nlibrary(MetBrewer)\nlibrary(patchwork)\n\n\n\nHow will what you learn this week help your research?\nYou have an interesting question that you want to explore. You have some data that relate to that question. Included in these data are information on your outcome of interest and information on the things that you think determine or shape that outcome. You think that one (or more) of the drivers is particularly important, but no one has yet written about it or proven its importance. Brilliant! What do you do now?\nThe first step in any empirical analysis is getting to know your data. I mean, really getting to know your data. You want to dig into it with a critical eye. You want to understand any patterns lurking beneath the surface.\nUltimately, you want to get a really good understanding of the data generation process. This process can be thought of in two different and important ways. First, you want to understand how, out there in the real world, your outcome and drivers come to be. For example, if you are interested in political voting patterns, you want to know the nitty gritty process of how people actually vote. Do they have to travel long distances, stand in long queues, fill out a lot of paperwork? Are there age restrictions on their ability to vote? Are there more insidious restrictions that might suppress voting for one particular group in the electorate?\nYou can use the skills we will discuss this week to help you answer these questions. For example, you can determine whether there are relatively few young voters compared to older voters. Then you can explore why this might be. In turn, your growing expertise in and understanding of the data generation process should inform your exploration of the data. You might note that people have to wait in long queues on a Tuesday to vote. Does this impact the number of workers vs. retirees who vote?\nNow, this is made slightly more tricky by the second part of your exploration. You need to understand how your variables are actually measured. How do we know who turns out to vote? Did you get access to the voter file, which records each individual who voted and some interesting and potentially relevant demographic information about them? Or are you relying on exit polls, that only include a portion of those who voted? Were the people included in the polls reflective of the total voting population? What or whom is missing from this survey? Of course, if your sample is not representative, you might find some patterns that appear to be very important to your outcome of interest but are, in fact, just an artifact of a poorly drawn sample.\nThis week you will be introduced to the first part of this process: data exploration. We use descriptive statistics to describe patterns in our data. These are incredibly powerful tools that will arm you with an intimate knowledge of the shape of your variables of interest. With this knowledge, you will be able to start to answer your important question and potentially identify new ones. You will also be able to sense-check your more complex models and pick up on odd or incorrect relationships that they may find.\nAs you make your frequency tables and histograms and very elaborate dot plots and box charts, keep in mind that these tools are useful for your interrogation of the data generation process. Be critical. Continue to ask whether your data allow you to detect true relationships between your variables of interest. Build your intuition for what is really going on and what factors are really driving your outcome of interest.\nLet’s get started.\n\n\nDescribing your data\nDescriptive statistics can be used to understand single variables and how any number of variables of interest relate to one another. The level of complexity involved in understanding these data and their relationships tend to increase with the number of variables you include. Today, we will look at single variables (for example, the age of respondents to a survey or how much each country spends on education in a year). Next week we will explore the relationship between two variables (for example, an individual’s party affiliation and their level of support for abortion access).\nYour variables are defined in terms of a unit of observation or analysis. These could include individuals, households, congressional districts, states, or countries. The unit of analysis you adopt should be relevant to your theory. For example, if you are interested in understanding which individuals are more susceptable to the different strategies rebel groups use to recruit individuals to fight, you probably want individual-level data. If you are interested in determining what drives countries to war with one another, you probably want country-level data (or leader-level data, or voter-level data?).\nThe important take-away here is that you should start with your theory. You should build a data set that reflects that theory.\n\n\nDifferent types of variables\nA variable is an empirical measurement of a characteristic of each observation. The ways of describing your data depend on the type of variable. You can have categorical or continuous variables. Categorical variables are discrete. They can be unordered (nominal) - for example, the colour of cars - or ordered (ordinal) - for example, whether you strongly dislike, dislike, are neutral about, like, or strongly like Taylor Swift.\n\n\n\n\n\n\nNote\n\n\n\nDichotomous (or binary) variables are a special type of categorical variable. They take on one of two values. For example: yes or no; at war or not at war; is a Swifty, or is not a Swifty.\n\n\nContinuous variables are, well, continuous. For example, your height or weight, a country’s GDP or population, or the number of fatalities in a battle.\n\n\n\n\n\n\nNote\n\n\n\nContinuous variables can be made into (usually ordered) categorical variables. This process is called binning. For example, you can take individuals’ ages and reduce them to 0 - 18 years old, 18 - 45 years old, 45 - 65 years old, and 65+ years old.\nYou lose information in this process: you cannot go from 45 - 65 years old back to the individuals’ precise age. In other words, you cannot go from a categorical to continuous variable.\n\n\nLet’s take a look at how you can describe these different types of variables in turn, using real-world political science examples.\n\n\nDescribing categorical variables\nSimply put, for categorical variables we are interested in the count and/or percentage of cases that fall into each category.\n\n\n\n\n\n\nNote\n\n\n\n\n\nLater, we will ask interesting questions using these summaries. These include whether differences between the counts and/or percentages of cases that fall into each category are meaningfully (and/or statistically significantly) different from one another. This deceptively simple question serves as the foundation for a lot of political science (particularly comparative) research.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe can represent categorical variables numerically whilest still letting R know not to treat them as continuous variables. We use factors to do this.\n\n\n\nCase study: American National Election Survey\nFor this section, we will be working with the American National Election Survey to explore how to produce useful descriptive statistics for categorical variables using R. The ANES polls annually individual Americans about their political beliefs and behavior.\nWe can access the 2012 survey using the poliscidata package:\n\npoliscidata::nes\n\n\n\n\n\n\n\nExercise\n\n\n\nTake a look at the many different bits of information collected about each respondent using ?nes.\n\n\nLet’s look at how many individuals of different ages took part in the survey in 2012. The survey records the age of each respondent within six different brackets and reports that information in the dem_age6 variable.\n\n\nCategorical variables as factors\nRemember, there are many different data types that R recognizes. These include characters (\"A\", \"B\", \"C\"), integers (1, 2, 3), and logical values (TRUE or FALSE).\nFactors are another data type. R uses factors to deal with categorical variables.\nTo illustrate, let’s look at the dem_age6 variable in the nes data set. This value takes on six different values, recording the age range into which the respondent to the NES falls. These six categories are:\n\ndistinct(nes, dem_age6)\n\n  dem_age6\n1 70-older\n2    60-69\n3    30-39\n4    50-59\n5    17-29\n6    40-49\n7     &lt;NA&gt;\n\n\nThis is an ordinal categorical variable. It is discrete and ordered. We can take a look at the variable itself using the helpful skimr::skim() function:\n\nskimr::skim(nes$dem_age6)\n\n\nData summary\n\n\nName\nnes$dem_age6\n\n\nNumber of rows\n5916\n\n\nNumber of columns\n1\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\ndata\n61\n0.99\nFALSE\n6\n50-: 1312, 60-: 1105, 40-: 948, 17-: 936\n\n\n\n\n\nHappily, the authors of the poliscidata package have already converted this variable into a factor. However, they have left it unordered (see the ordered variable in the printed out summary). We can easily convert this into an ordered factor:\n\nnes &lt;- mutate(nes, dem_age6 = factor(dem_age6, ordered = T, levels = c(\"17-29\",\n                                                                       \"30-39\", \n                                                                       \"40-49\", \n                                                                       \"50-59\",\n                                                                       \"60-69\",\n                                                                       \"70-older\")))\n\nskimr::skim(nes, dem_age6)\n\n\nData summary\n\n\nName\nnes\n\n\nNumber of rows\n5916\n\n\nNumber of columns\n399\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\ndem_age6\n61\n0.99\nTRUE\n6\n50-: 1312, 60-: 1105, 40-: 948, 17-: 936\n\n\n\n\n\nHere, we used the ordered argument to tell R that this factor is ordinal. We then used the levels argument to tell R the order of the categories.\n\n\n\n\n\n\nTip\n\n\n\nBy ensuring that R understands which variables are categorical, we prevent it from giving us nonsense statistics (such as a mean or median for our categories).\n\n\n\n\nFrequency distribution\nWe can take advantage of janitor::tabyl() to quickly calculate the number and proportion of respondents in each age bracket.\n\ntabyl(nes, dem_age6)\n\n dem_age6    n    percent valid_percent\n    17-29  936 0.15821501     0.1598634\n    30-39  862 0.14570656     0.1472246\n    40-49  948 0.16024341     0.1619129\n    50-59 1312 0.22177147     0.2240820\n    60-69 1105 0.18678161     0.1887276\n 70-older  692 0.11697093     0.1181896\n     &lt;NA&gt;   61 0.01031102            NA\n\n\n\n\n\n\n\n\nTip\n\n\n\nvalid_percent provides the proportion of respondents in each age bracket with missing values removed from the denominator. For example, the NES survey had 5,916 respondents in 2012, but only 5,855 of them provided their age. 936 responded that they are 17 - 29 years old. Therefore, the 17-29 proportion (which is bounded by 0 and 1, whereas percents are bounded by 0 and 100) is 936 / 5,916 and its valid proportion is 936 / 5,855.\n\n\n\n\nVisualizing this frequency\nIt is a bit difficult to quickly determine relative counts. Which age bracket has the most respondents? Which has the least? Are these counts very different from each other.\nI highly recommend visualizing your data. You will get a much better sense of it. We can easily visualize this frequency table. I recommend using a bar chart to show clearly relative counts.\n\nnes |&gt; \n  tabyl(dem_age6) |&gt; \n  ggplot(aes(x = n, y = dem_age6)) + \n  geom_col() +\n  geom_text(aes(label = n), hjust = -0.5) + \n  theme_minimal() + \n  labs(\n    x = \"Count of respondents\",\n    y = \"Age bracket\"\n  ) + \n  scale_x_continuous(limits = c(0, 1500))\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou can use geom_text() to add labels to your plots. Remember to include label as a aes() argument to specify which variable you would like to use as your label. The hjust argument provided to the geom_text() function moves the label slightly to the right to avoid overlap with the bars. Vertical adjustments can be made using the vjust argument.\n\n\n\n\n\n\n\n\nTip\n\n\n\nAnother useful side-effect of properly specifying our ordinal categorical variable as an ordered factor is that it will correctly order our plots according to the order of our variables.\n\n\n\n\n\nDescribing continuous variables\nWe need to treat continuous variables differently from categorical ones because they cannot be meaningfully bound together and compared. For example, imagine making a frequency table or bar chart that counts the number of countries with each observed GDP. You would have 193 different counts of one. Not very helpful!\nWe can get a much better sense of our continuous variables by looking at characteristics of the distribution of these variables across the range of all possible values they could take on. Phew! Let’s make sense of this using some real-world data.\n\nCase study: Comparing countries’ spending on education\nFor this section, we will look at each country’s spending on education as a percent of its gross domestic product. We will use wbstats::wb_data() to collect these data.\n\nperc_edu &lt;- wb_data(\n  \"SE.XPD.TOTL.GD.ZS\", start_date = 2020, end_date = 2020, return_wide = F\n) |&gt; \n  transmute(\n    country, \n    region = countrycode(country, \"country.name\", \"region\"),\n    year = date, \n    value = value / 100\n  )\n\nperc_edu\n\n# A tibble: 217 × 4\n   country             region                      year   value\n   &lt;chr&gt;               &lt;chr&gt;                      &lt;dbl&gt;   &lt;dbl&gt;\n 1 Afghanistan         South Asia                  2020  0.0286\n 2 Albania             Europe & Central Asia       2020  0.0310\n 3 Algeria             Middle East & North Africa  2020  0.0704\n 4 American Samoa      East Asia & Pacific         2020 NA     \n 5 Andorra             Europe & Central Asia       2020 NA     \n 6 Angola              Sub-Saharan Africa          2020  0.0242\n 7 Antigua and Barbuda Latin America & Caribbean   2020  0.0345\n 8 Argentina           Latin America & Caribbean   2020  0.0502\n 9 Armenia             Europe & Central Asia       2020  0.0271\n10 Aruba               Latin America & Caribbean   2020 NA     \n# ℹ 207 more rows\n\n\nI have converted these percentages (0 - 100) to proportions (0 - 1) for ease of interpretation. I have also added each country’s region (using countrycode::countrycode()) so that we can explore regional trends in our data.\nWe can get a good sense of how expenditure varied by country by looking at the center, spread, and shape of the distribution.\n\n\nHistogram\n\nggplot(perc_edu, aes(x = value)) + \n  geom_histogram() + \n  theme_minimal() + \n  labs(\n    x = \"Expenditure on education as a proportion of GDP\",\n    y = \"Count\"\n  ) + \n  scale_x_continuous(labels = label_percent())\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe scales package includes a lot of helpful functions that allow you to format numbers. Here, I am using scales::label_percent() to convert our raw proportions into the percentage format on the x-axis of the plot.\n\n\n\n\n\n\n\n\nExercise\n\n\n\nTake a look at ?geom_histogram to find the arguments needed to change the bin width of your histograms.\n\n\n\n\nDensity curves\n\nggplot(perc_edu, aes(x = value)) + \n  geom_density() + \n  theme_minimal() + \n  labs(\n    x = \"Expenditure on education as a proportion of GDP\",\n    y = \"Density\"\n  ) + \n  scale_x_continuous(labels = label_percent())\n\n\n\n\n\n\nUnderstanding distributions\nBecause continuous variables are best described using their distribution, we can use the shape of that distribution to better understand our individual variables and compare them to others. Is the distribution symmetric or skewed? Where are the majority of observations clustered? Are there multiple distinct clusters, or high points, in the distribution?\n\nNormal distribution\n\ntibble(x = rnorm(n = 1e6)) |&gt; \n  ggplot(aes(x = x)) + \n  geom_histogram() + \n  theme_minimal()\n\n\n\n\n\n\nRight skewed distribution\n\ntibble(x = rbeta(1e6, 1, 10)) |&gt; \n  ggplot(aes(x = x)) + \n  geom_histogram() + \n  theme_minimal()\n\n\n\n\n\n\nLeft skewed distribution\n\ntibble(x = rbeta(1e6, 10, 1)) |&gt; \n  ggplot(aes(x = x)) + \n  geom_histogram() + \n  theme_minimal()\n\n\n\n\n\n\n\nMeasures of central tendency: mean, median, and mode\nWe can also use measures of central tendency to quickly describe and compare our variables.\n\nMean\nThe mean is the average of all values:\n\\[\n\\bar{x} = \\frac{\\Sigma x_i}{n}\n\\]\nIn other words, add all of your values together and then divide that total by the number of values you have.\nIn R:\n\nmean(perc_edu$value, na.rm = T) |&gt; \n  percent()\n\n[1] \"5%\"\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you do not use the argument na.rm (read NA remove!), you will get an NA if any exist in your vector of values. This is a good default! You should be very aware of missing data points.\n\n\n\n\nMedian\nThe median is the mid-point of all values.\nTo calculate it, put all of your values in order from smallest to largest. Identify the value in the middle. That’s your median.\nIn R:\n\nmedian(perc_edu$value, na.rm = T) |&gt; \n  percent()\n\n[1] \"4%\"\n\n\n\n\nMode\nThe mode is the most frequent of all values.\nTo calculate it, count how many times each value occurs in your data set. The one that occurs the most is your mode.\n\n\n\n\n\n\nNote\n\n\n\nThis is usually a more useful summary statistic for categorical variables than continuous ones. For example, which colour of car is most popular? Which political party has the most members?\n\n\nIn R:\n\nx &lt;- c(1, 1, 2, 4, 5, 32, 5, 1, 10, 3, 4, 6, 10)\n\ntable(x)\n\nx\n 1  2  3  4  5  6 10 32 \n 3  1  1  2  2  1  2  1 \n\n\n\n\nUsing central tendency to describe and understand distributions\nNormally distributed values have the same mean and median.\n\nnorm_dist &lt;- tibble(x = rnorm(n = 1e6))\n\nggplot(norm_dist, aes(x = x)) + \n  geom_histogram() + \n  geom_vline(xintercept = mean(norm_dist$x), colour = \"#EEABC4\", size = 2) + \n  geom_vline(xintercept = median(norm_dist$x), colour = \"#4582EC\", size = 2) + \n  theme_minimal()\n\n\n\n\nFor right skewed data, the mean is greater than the median.\n\nright_dist &lt;- tibble(x = rbeta(1e6, 2, 10))\n\nggplot(right_dist, aes(x = x)) + \n  geom_histogram() + \n  geom_vline(xintercept = mean(right_dist$x), colour = \"#EEABC4\", size = 2) + \n  geom_vline(xintercept = median(right_dist$x), colour = \"#4582EC\", size = 2) + \n  theme_minimal()\n\n\n\n\nFor left skewed data, the mean is smaller than the median.\n\nleft_dist &lt;- tibble(x = rbeta(1e6, 10, 2))\n\nggplot(left_dist, aes(x = x)) + \n  geom_histogram() + \n  geom_vline(xintercept = mean(left_dist$x), colour = \"#EEABC4\", size = 2) + \n  geom_vline(xintercept = median(left_dist$x), colour = \"#4582EC\", size = 2) + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nHow is the percentage spent on education data skewed?\n\n\n\n\n\nFive number summary\nAs you can see, we are attempting to summarise our continuous data to give us a meaningful but manageable sense of it. Means and medians are useful for continuous data.\nWe can provide more context to our understanding using more summary statistics. A common approach is the five number summary. This includes:\n\nThe smallest value;\nThe 25th percentile value, or the median of the lower half of the data;\nThe median;\nThe 75th percentile value, or the median of the upper half of the data;\nThe largest value.\n\nWe can use skimr::skim() to quickly get useful information about our continuous variable.\n\nskim(perc_edu$value)\n\n\nData summary\n\n\nName\nperc_edu$value\n\n\nNumber of rows\n217\n\n\nNumber of columns\n1\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ndata\n59\n0.73\n0.05\n0.02\n0\n0.03\n0.04\n0.06\n0.14\n▂▇▃▁▁\n\n\n\n\n\nWe have 217 rows (because our unit of observation is a country, we can read this as 217 countries). We are missing education spending values for 59 of those countries (see n_missing), giving us a complete rate of 73% (see complete_rate).\nThe country that spent the least on education as a percent of its GDP in 2020 was Cuba, which spent 0.0% (see p0). The country that spent the most was the Marshall Islands, which spent 13.6% (see p100). The average percent of GDP spent on education in 2020 was 4.6% (see mean) and the median was 4.5% (see p50).\nThis description was a bit unwieldy. To get a better sense of our data, we can visualize it.\n\n\nBox plots\nBox plots (sometimes referred to as box and whisker plots) visualize the five number summary (with bonus features) nicely.\n\nggplot(perc_edu, aes(x = value)) + \n  geom_boxplot() + \n  theme_minimal() + \n  theme(\n    axis.text.y = element_blank()\n  ) + \n  labs(\n    x = \"Expenditure on education as a proportion of GDP\",\n    y = NULL\n  ) + \n  scale_x_continuous(labels = label_percent())\n\n\n\n\nNote that some values are displayed as dots. The box plot is providing you with a bit more information than the five number summary alone. The box itself displays the 25th percentile, the median, and the 75th percentile values. The tails show you all the data up to a range 1.5 times the interquartile range (IQR), or the 75th percentile minus the 25th percentile. If the smallest or largest values fall below or above (respectively) 1.5 times the IQR, the tail ends at that value. If, however, these values fall outside that range, they are displayed as dots. These are (very rule of thumb, take with a grain of salt, please rely on your theory and data generation process instead!) candidates for outliers.\n\n\nOutliers\nOutliers fall so far away from the majority of the other values that they should be examined closely and perhaps excluded from your analysis. Outliers can distort your mean. They do not, however, distort your median.\n\n\n\n\n\n\nNote\n\n\n\nWe will talk more about how to deal with outliers later in the course.\n\n\n\n\nMeasures of spread: range, variance, and standard deviation\nWe now have a good sense of some of the features of our data. Another useful thing to know is the shape of the distribution. Here, measures of spread are useful.\n\nRange\nThe range is the difference between the largest and smallest value.\n\\[\nrange = min - max\n\\]\n\nmax(perc_edu$value, na.rm = T) - min(perc_edu$value, na.rm = T)\n\n[1] 0.1362499\n\n\n\n\nVariance\nThe variance measures how spread out your values are. On average, how far are your observations from the mean? Take a look at these two plots. Both have the same center point (0) and number of observations (1,000,000). However, the data are much more spread out around that center point in the top graph.\n\nwide_dist &lt;- tibble(x = rnorm(1e6, sd = 2))\n\np1 &lt;- ggplot(wide_dist, aes(x = x)) + \n  geom_histogram() + \n  geom_vline(xintercept = 0) + \n  theme_minimal() + \n  scale_x_continuous(limits = c(-4, 4))\n\nnarrow_dist &lt;- tibble(x = rnorm(1e6, sd = 1))\n\np2 &lt;- ggplot(narrow_dist, aes(x = x)) + \n  geom_histogram() + \n  geom_vline(xintercept = 0) + \n  theme_minimal() + \n  scale_x_continuous(limits = c(-4, 4))\n\np1 / p2\n\n\n\n\nThe data in the top graph have higher variance (are more spread out) than those in the bottom graph. We measure this by calculating the average of the squares of the deviations of the observations from their mean.\n\\[\ns^2 = \\frac{\\Sigma(x_i - \\bar{x})^2}{n - 1}\n\\]\nLet’s step through this. We will first calculate the variance for wide_dist, or the top graph. To do this:\n\nCalculate the mean of your values.\nCalculate the difference between each individual value and that mean (how far from the mean is every value?).\nSquare those differences.\n\n\n\n\n\n\n\nTip\n\n\n\nWe do not care whether the value is higher or lower than the mean. We only care how far from the mean it is. Squaring a value removes its sign (positive or negative) allowing us to concentrate on this difference.\n\n\n\nAdd all of those squared differences to get a single number.\nDivide that single number by the number of observations you have minus 1.\n\nYou now have your variance!\nIn R:\n\nwide_dist_mean &lt;- mean(wide_dist$x)\n\nwide_var_calc &lt;- wide_dist |&gt; \n  mutate(\n    mean = wide_dist_mean,\n    diff = x - mean,\n    diff_2 = diff^2\n  )\n\nwide_var_calc\n\n# A tibble: 1,000,000 × 4\n         x    mean    diff   diff_2\n     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n 1  0.704  0.00161  0.703   0.494  \n 2 -0.438  0.00161 -0.439   0.193  \n 3  1.63   0.00161  1.62    2.64   \n 4  2.21   0.00161  2.21    4.88   \n 5  4.34   0.00161  4.34   18.8    \n 6  0.569  0.00161  0.567   0.322  \n 7 -2.10   0.00161 -2.10    4.43   \n 8 -0.737  0.00161 -0.739   0.546  \n 9 -0.840  0.00161 -0.841   0.708  \n10  0.0419 0.00161  0.0403  0.00162\n# ℹ 999,990 more rows\n\n\nWe the add those squared differences between each observation and the mean of our whole sample together. Finally, we divide that by one less than our number of observations.\n\nwide_var &lt;- sum(wide_var_calc$diff_2) / (nrow(wide_var_calc) - 1)\n\nwide_var\n\n[1] 3.984558\n\n\nWe can compare this to the variance for our narrower distribution.\n\nnarrow_var_calc &lt;- narrow_dist |&gt; \n  mutate(\n    mean = mean(narrow_dist$x),\n    diff = x - mean,\n    diff_2 = diff^2\n  )\n\nnarrow_var &lt;- sum(narrow_var_calc$diff_2) / (nrow(narrow_var_calc) - 1)\n\nnarrow_var\n\n[1] 1.000481\n\n\nIt is, in fact, smaller!\nThat was painful. Happily we can use var() to do this in one step:\n\nvar(wide_dist)\n\n         x\nx 3.984558\n\n\n\nvar(narrow_dist)\n\n         x\nx 1.000481\n\n\n\nvar(wide_dist) &gt; var(narrow_dist)\n\n     x\nx TRUE\n\n\n\n\nStandard deviation\nA simpler measure of spread is the standard deviation. It is simply the square root of the variance.\n\nsqrt(wide_var)\n\n[1] 1.996136\n\n\n\nsqrt(narrow_var)\n\n[1] 1.00024\n\n\nYou can get this directly using sd():\n\nsd(wide_dist$x)\n\n[1] 1.996136\n\n\n\nsd(narrow_dist$x)\n\n[1] 1.00024\n\n\nIf you look back to our graphs, you will see that I set the standard deviations explicitly when I generated the data: rnorm() takes an sd argument. This is great because we can confirm that the standard deviations for the wide and narrow distributions are 2 and 1 respectively (with a little bit of randomnoise).\n\ntibble(\n  n = rnorm(1e6, sd = 1),\n  w = rnorm(1e6, sd = 2)\n) |&gt; \n  ggplot() + \n  geom_density(aes(x = n), colour = \"green\", size = 2) + \n  geom_density(aes(x = w), colour = \"lightblue\", size = 2) + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nRemember that the standard deviation is a measure of how spread out our data are. Therefore, data with no spread (are all the exact same number) will have a standard deviation of 0.\n\n\n\n\n\nNormal distributions\nRemember that normal distributions share a mean and median. This has very cool and useful side effects.\nLet’s explore these with some simulated data that have a mean of 5 and a standard deviation of 2.\n\nnorm_5_2 &lt;- tibble(x = rnorm(n = 1e6, mean = 5, sd = 2))\n\nhead(norm_5_2)\n\n# A tibble: 6 × 1\n      x\n  &lt;dbl&gt;\n1  4.31\n2  8.61\n3  4.18\n4  6.49\n5  5.46\n6  3.62\n\n\n\n\n\n\n\n\nNote\n\n\n\nBecause we are taking random draws of numbers using rnorm(), you will get a different set of numbers to me if you run this command. They will still be centered around a mean of 5 and have a standard deviation of 2.\n\n\nLet’s plot these numbers:\n\nggplot(norm_5_2, aes(x = x)) + \n  stat_slab(\n    aes(fill_ramp = after_stat(cut_cdf_qi(cdf))), fill = met.brewer(\"Egypt\")[2]\n  ) + \n  scale_fill_ramp_discrete(range = c(1, 0.2), guide = \"none\") + \n  theme_minimal()\n\n\n\n\nBecause these numbers are normally distributed (share a mean and median), the following are true:\n\nApproximately 68% of the data fall within one standard deviation of the mean (the dark blue shaded area);\nApproximately 95% of the data fall within two standard deviations of the mean (the medium blue shaded area);\nApproximately 99.7% of the data fall within three standard deviations of the mean (the light blue shaded area).\n\n\n\n\n\n\n\nExercise\n\n\n\nRepeat this process with normally distributed data centered around different means and with different standard deviations.\n\n\n\n\nStandardization\nNotice how our description of each variable depends on its units of measurement. What do we do if we want to compare across different measurements that have different units?\n\nZ scores\nFor normal distributions, we can use the z score. This gives us a standard way of understanding how many standard deviations from the mean of a normally distributed variable a value is.\n\\[\nz_i = \\frac{x_i - \\mu_x}{\\sigma_x}\n\\]\nWe are just transforming our data. We want to center it around 0 and reshape it so that roughly 68% of the data fall within one standard deviation of the mean, 95% of the data fall within two standard deviations of the mean, and 99.7% of the data fall within three standard deviations of the mean.\nLet’s standardize our data from above.\n\nstandard_5_2 &lt;- norm_5_2 |&gt; \n  mutate(mean = mean(x),\n         sd = sd(x),\n         z_score = (x - mean) / sd)\n\nhead(standard_5_2)\n\n# A tibble: 6 × 4\n      x  mean    sd z_score\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1  4.31  5.00  2.00  -0.344\n2  8.61  5.00  2.00   1.80 \n3  4.18  5.00  2.00  -0.407\n4  6.49  5.00  2.00   0.745\n5  5.46  5.00  2.00   0.229\n6  3.62  5.00  2.00  -0.689\n\n\nWe can confirm this:\n\nggplot(standard_5_2, aes(x = z_score)) + \n  stat_slab(\n    aes(fill_ramp = after_stat(cut_cdf_qi(cdf))), fill = met.brewer(\"Egypt\")[2]\n  ) + \n  scale_fill_ramp_discrete(range = c(1, 0.2), guide = \"none\") + \n  theme_minimal() + \n  scale_x_continuous(breaks = seq(-5, 5, 1))\n\n\n\n\nWe can use the z-score of a value to determine what percentage of values of our normally distributed data are less than that value. To do this, we can either reference a z table, or use the following function in R:\npnorm()\nWhat percentage of the simulated data fall below 1.5 standard deviations from the mean? First, let’s visualize this question:\n\nggplot(standard_5_2, aes(x = z_score)) +\n  stat_halfeye(aes(fill = after_stat(x &lt; 1.5)), .width = c(0.95, 0.68)) + \n  theme_minimal() +\n  theme(legend.position = \"none\") + \n  scale_x_continuous(breaks = seq(-5, 5, 1)) + \n  labs(x = \"Z-Score\",\n       y = \"Density\",\n       caption = \"Median and mean shown with point. One and two standard deviations are shown by the black bars.\")\n\n\n\n\nNext, we can use the function above to work out the precise percentage of the data that fall below this value:\n\npnorm(1.5)\n\n[1] 0.9331928\n\n\n93.32% of the data fall below 1.5 standard deviations above the mean of our simulated data.\n\n\n\n\n\n\nExercise\n\n\n\nPick a different number of standard deviations from the mean and have a go at visualizing and calculating the percentage of the data that fall below that value.\n\n\n\n\n\n\n\n\nNote\n\n\n\nTo work out the percentage of the data that fall above that value, you simply subtract the proportion that falls below from 1 (or the percentage subtracted from 100%).\nWhat percentage of the data are greater than 1.5 standard deviations from the mean? 1 - pnorm(1.5) or 0.067.\n\n\n\n\n\n\nNext week\nA focus on techniques for examining relationships between variables."
  },
  {
    "objectID": "content/01-introduction.html",
    "href": "content/01-introduction.html",
    "title": "Course Introduction",
    "section": "",
    "text": "Quarto introductory tutorial\n Pollock & Edwards R Companion, Chapter 1"
  },
  {
    "objectID": "content/01-introduction.html#readings",
    "href": "content/01-introduction.html#readings",
    "title": "Course Introduction",
    "section": "",
    "text": "Quarto introductory tutorial\n Pollock & Edwards R Companion, Chapter 1"
  },
  {
    "objectID": "content/01-introduction.html#class-slides",
    "href": "content/01-introduction.html#class-slides",
    "title": "Course Introduction",
    "section": "Class slides",
    "text": "Class slides"
  },
  {
    "objectID": "content/01-introduction.html#section",
    "href": "content/01-introduction.html#section",
    "title": "Course Introduction",
    "section": "Section",
    "text": "Section\n\nIntroduction to Quarto and markdown\nQuarto provides you with a tool that allows you to create fully reproducible research outputs. It allows you to combine your code, results, and prose into one document that can output in many different format. All of the materials I produced for this class and Maths Camp were created using Quarto.\nYou can use Quarto from RStudio.1 Here is a screen shot of a Quarto document (extension .qmd) and its HTML output. You can render a Quarto document to many different types of formats, including PDF and MS Word.\n\n\n\nSource: Quarto\n\n\n\n\nA new Quarto document\nOpen up a new Quarto document in RStudio:\n\nFill in the relevant fields:\n\nIt will have a .qmd extension. This is the Quarto document extension.\nThe new document will have some introductory text and other things in it. Importantly, it will have a YAML section. This section (written in YAML) includes all of the metadata for your document. It includes the title, author, format in which it will rendered, and the default editor.\n\nThere are two ways to work with and view Quarto documents. The default editor is visual, which follows a more “what-you-see-is-what-you-get” style. Alternatively, you can edit in source, which looks more like a raw script. To switch between the two, you can use the Source and Visual icons in the top left hand side of the screen.\n\n\n\n\n\n\nNote\n\n\n\nI find myself switching between these two formats all the time. The visual editor is much easier to work in when writing, but it can be a bit buggy when it comes to formatting my work and writing code. I work in source when I am doing those two things.\n\n\n\n\nRendering your document\nTo render your document into your chosen format (in this case: HTML), you need to hit the Render icon in the document’s top bar. This will produce a HTML version of your Quarto document in the same folder in which you saved your Quarto document.\nYou can preview your document in RStudio by changing your settings to Preview in Viewer Pane.\n\nNow, whenever you render your document a preview of it will show up in the Viewer pane (which is in the same place as your Files, Plots, and Help panes).\n\n\n\n\n\n\n\nNote\n\n\n\nIf you toggle on the Render on Save option, your Quarto document will render and update your viewer every time you hit save. This can be helpful when you are formatting your document.\n\n\n\n\nWriting prose in Quarto\nYou can write prose as you would in any other text editor in Quarto. When you are in the Visual editor model, Quarto provides you with the shortcut keys for many of the formats you use in other text editors, including MS Word and Google Docs. You can also use your usual keyboard shortcuts.\nIn the Source editor mode, you will need to use markdown. Markdown is a lightweight markup language that allows you to format plain text. It gives you a lot of control over the format of your text documents (similar to Latex).\nHere is the link for a great Markdown tutorial. It takes about 10 minutes to complete, so we will play around in there now.\n\n\nRunning code in Quarto\nYou can also run code from within your Quarto document. You can do this through a code chunk or in-line code. I will step through both options now.\n\nCode chunks\nA code chunk starts with ```{r} and ends with ```. You can then write whole “chunks” of code that will output in your rendered document.\n```{r}\n#| echo: true\n\nlibrary(tidyverse)\n```\nYou can specify your chunk options using #| at the start of the line. For example, above I specified that I wanted the code in the code chunk to be shown when I render my document. You can hide the code by changing the chunk option echo to false. There are many different chunk options that you can control. A full list can be found here.\nYou can set the chunk options in the individual chunks, as show above. Alternatively, you can set them universally in the YAML section at the top of your Quarto document using the execute command. For example:\n```{yaml}\nexecute:\n  echo: true\n  message: false\n  warning: false\n```\nThis will apply to all code chunks unless you overwrite it by including chunk-specific options in a code chunk.\nCode chunks are useful for running large amounts of code. Commonly, I use them to include a plot, a regression table, or to read in my data or model results. For example, you can write the code to create a ggplot directly in your document.\n\n\n\nSource: Quarto\n\n\n\n\nIn-line code\nAlternatively, you often want to reference numbers or results in your prose. For example, I may be writing up the data section of a paper and want to specify that my data set includes 100 observations. If I were to write this in normally and then go away and collect more data, I would need to come back and update this number manually. I may do this several times (very tedious) or I may miss a time (we are all human). In-line coding allows you to include R code in your prose.\nYou include R code directly in your prose using the expression: `r `. For example:\n\nWill render as: There are 234 observations in our data. No need to go and update this reference if that number changes!\n\n\n\n\n\n\nNote\n\n\n\nscales is a great package for formatting numbers.\nFor example, R will output raw numbers such as 1000000000 and 8932348920. scales allows you to format these numbers so they are easier to read: scales::comma(1000000000) gives you 1,000,000,000 and scales::dollar(8932348920) gives you $8,932,348,920."
  },
  {
    "objectID": "content/01-introduction.html#footnotes",
    "href": "content/01-introduction.html#footnotes",
    "title": "Course Introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou can also use it from VS Code, Jupyter, Neovim, and Editor.↩︎"
  },
  {
    "objectID": "content/05-applications.html",
    "href": "content/05-applications.html",
    "title": "Applications & Midterm Exam Review",
    "section": "",
    "text": "Segal, Jeffrey A. & Albert D. Cover. 1989. “Ideological Values and the Votes of U.S. Supreme Court Justices.” American Political Science Review 83(2): 557-565.\n Sondheimer, Rachel Milstein & Donald P. Green. 2010. “Using Experiments to Estimate the Effects of Education on Voter Turnout.” American Journal of Political Science 54(1): 174-189."
  },
  {
    "objectID": "content/05-applications.html#readings",
    "href": "content/05-applications.html#readings",
    "title": "Applications & Midterm Exam Review",
    "section": "",
    "text": "Segal, Jeffrey A. & Albert D. Cover. 1989. “Ideological Values and the Votes of U.S. Supreme Court Justices.” American Political Science Review 83(2): 557-565.\n Sondheimer, Rachel Milstein & Donald P. Green. 2010. “Using Experiments to Estimate the Effects of Education on Voter Turnout.” American Journal of Political Science 54(1): 174-189."
  },
  {
    "objectID": "content/05-applications.html#class-slides",
    "href": "content/05-applications.html#class-slides",
    "title": "Applications & Midterm Exam Review",
    "section": "Class slides",
    "text": "Class slides"
  },
  {
    "objectID": "content/05-applications.html#section",
    "href": "content/05-applications.html#section",
    "title": "Applications & Midterm Exam Review",
    "section": "Section",
    "text": "Section"
  },
  {
    "objectID": "content/10-applications.html",
    "href": "content/10-applications.html",
    "title": "Applications & Midterm Exam Review II",
    "section": "",
    "text": "Brooks, Deborah Jordan. 2011. “Testing the Double Standard for Candidate Emotionality: Voter Reactions to the Tears and Anger of Male and Female Politicians.” The Journal of Politics 73: 597-615."
  },
  {
    "objectID": "content/10-applications.html#readings",
    "href": "content/10-applications.html#readings",
    "title": "Applications & Midterm Exam Review II",
    "section": "",
    "text": "Brooks, Deborah Jordan. 2011. “Testing the Double Standard for Candidate Emotionality: Voter Reactions to the Tears and Anger of Male and Female Politicians.” The Journal of Politics 73: 597-615."
  },
  {
    "objectID": "content/10-applications.html#class-slides",
    "href": "content/10-applications.html#class-slides",
    "title": "Applications & Midterm Exam Review II",
    "section": "Class slides",
    "text": "Class slides"
  },
  {
    "objectID": "content/10-applications.html#section",
    "href": "content/10-applications.html#section",
    "title": "Applications & Midterm Exam Review II",
    "section": "Section",
    "text": "Section"
  },
  {
    "objectID": "content/07-statistical_inference.html",
    "href": "content/07-statistical_inference.html",
    "title": "Hypothesis Testing I",
    "section": "",
    "text": "Pollock & Edwards, Chapter 6\n\n\n\n Pollock & Edwards R Companion, Chapter 6"
  },
  {
    "objectID": "content/07-statistical_inference.html#readings",
    "href": "content/07-statistical_inference.html#readings",
    "title": "Hypothesis Testing I",
    "section": "",
    "text": "Pollock & Edwards, Chapter 6\n\n\n\n Pollock & Edwards R Companion, Chapter 6"
  },
  {
    "objectID": "content/07-statistical_inference.html#class-slides",
    "href": "content/07-statistical_inference.html#class-slides",
    "title": "Hypothesis Testing I",
    "section": "Class slides",
    "text": "Class slides"
  },
  {
    "objectID": "content/07-statistical_inference.html#section",
    "href": "content/07-statistical_inference.html#section",
    "title": "Hypothesis Testing I",
    "section": "Section",
    "text": "Section\n\nPrerequisites\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(poliscidata)\nlibrary(ggdist)\nlibrary(MetBrewer)\nlibrary(DescTools)\n\nset.seed(1234)\n\n\n\nHow will what you learn this week help your research?\n\n\nPopulation and sample\nSay we are interested in the proportion of US voters who will vote for Joe Biden in the 2024 general election. We cannot ask all US voters of their intentions. Instead, we ask a sample of the US voting population and infer from that sample the population’s intentions.\n\nThe data point of interest among the population is referred to as the parameter. Here, it is the proportion of US voters who intend to vote for Joe Biden in the 2024 general election.\nThe data point of interest among the sample is referred to as the statistic. Here, it is the proportion of survey respondents who intend to vote for Joe Biden in the 2024 general election.\nWe aim to have a statistic that accurately represents the parameter.\n\nWhen we generalize from the sample statistic to the parameter we are engaging in statistical inference.\nHow can we be confident that our statistic represents the parameter? Generally speaking, the more our sample “looks like” our population, the more confident we can be that we have a good statistic. Drawing on probability theory, our sample is increasingly likely to resemble our population with its randomness and size.\nYou should strive for a pure random sample. This means that every individual within your population is equally likely to be drawn. This is really hard to achieve! Think about normal election surveys. Many are conducted over the phone. There are plenty of people who do not have a landline phone, or do not pick up calls from unknown numbers, or who keep their phones on do not disturb during the day. These people will be harder to contact than those who are sitting by the phone waiting eagerly for a call. Even if you have access to all US voters’ phone numbers (never mind that some voters do not have phone numbers) and you take a random sample of those phone numbers and start calling, you still will not get a hold of them all with equal probability.\nYou should also strive for as large a sample as you can possibly get. More is always better in terms of statistical inference (if not your research budget or time). Remember back to our coin flips last week. The more coin flips we did, the closer we got to the true probability distribution between heads and tails. This principle also holds here.\n\n\nSampling error\nImagine you have a large and representative sample. You are still going to have some error. This is because your sample varies in all the normal ways events with uncertainty vary. To illustrate, let’s return to our coin flips.\nWe state our possible outcomes:\n\npossible_outcomes &lt;- c(\"HEADS\", \"TAILS\")\npossible_outcomes\n\n[1] \"HEADS\" \"TAILS\"\n\n\nWe flip our coin 100 times:\n\nsample(possible_outcomes, 100, replace = T, prob = c(0.5, 0.5))\n\n  [1] \"TAILS\" \"HEADS\" \"HEADS\" \"HEADS\" \"HEADS\" \"HEADS\" \"TAILS\" \"TAILS\" \"HEADS\"\n [10] \"HEADS\" \"HEADS\" \"HEADS\" \"TAILS\" \"HEADS\" \"TAILS\" \"HEADS\" \"TAILS\" \"TAILS\"\n [19] \"TAILS\" \"TAILS\" \"TAILS\" \"TAILS\" \"TAILS\" \"TAILS\" \"TAILS\" \"HEADS\" \"HEADS\"\n [28] \"HEADS\" \"HEADS\" \"TAILS\" \"TAILS\" \"TAILS\" \"TAILS\" \"HEADS\" \"TAILS\" \"HEADS\"\n [37] \"TAILS\" \"TAILS\" \"HEADS\" \"HEADS\" \"HEADS\" \"HEADS\" \"TAILS\" \"HEADS\" \"TAILS\"\n [46] \"HEADS\" \"HEADS\" \"TAILS\" \"TAILS\" \"HEADS\" \"TAILS\" \"TAILS\" \"HEADS\" \"HEADS\"\n [55] \"TAILS\" \"HEADS\" \"TAILS\" \"HEADS\" \"TAILS\" \"HEADS\" \"HEADS\" \"TAILS\" \"TAILS\"\n [64] \"TAILS\" \"TAILS\" \"HEADS\" \"TAILS\" \"HEADS\" \"TAILS\" \"HEADS\" \"TAILS\" \"HEADS\"\n [73] \"TAILS\" \"HEADS\" \"TAILS\" \"HEADS\" \"TAILS\" \"TAILS\" \"TAILS\" \"HEADS\" \"HEADS\"\n [82] \"TAILS\" \"TAILS\" \"HEADS\" \"TAILS\" \"HEADS\" \"TAILS\" \"TAILS\" \"TAILS\" \"HEADS\"\n [91] \"TAILS\" \"HEADS\" \"TAILS\" \"TAILS\" \"TAILS\" \"HEADS\" \"TAILS\" \"TAILS\" \"TAILS\"\n[100] \"HEADS\"\n\n\nWe know that the true probability of the coin landing on heads is 0.5. If we flip a fair coin 100 times, we should get 50 heads. We also know that these random draws are a bit noisy: we can get proportions that do not reflect the underlying probability of 0.5. However, the more flips we do, the closer we will get to that true probability distribution.\nLet’s do 100,000 100-coin flip trials and record the number of heads we get each time:\n\ncoin_flip &lt;- function(possible_outcomes, n) {\n  \n  outcomes &lt;- sample(possible_outcomes, size = n, replace = T, prob = c(0.5, 0.5))\n  \n  return(table(outcomes)[\"HEADS\"])\n  \n}\n\nresults &lt;- tibble(trial = 1:100000) |&gt; \n  rowwise() |&gt; \n  mutate(n_heads = coin_flip(possible_outcomes, 100))\n\nresults\n\n# A tibble: 100,000 × 2\n# Rowwise: \n   trial n_heads\n   &lt;int&gt;   &lt;int&gt;\n 1     1      52\n 2     2      52\n 3     3      48\n 4     4      60\n 5     5      61\n 6     6      49\n 7     7      52\n 8     8      50\n 9     9      49\n10    10      45\n# ℹ 99,990 more rows\n\n\nWhat are the results of these repeated trials?\n\nggplot(results, aes(x = n_heads)) + \n  geom_histogram() + \n  geom_vline(xintercept = 50)\n\n\n\n\nSo we know that each time we flip that coin, there is a 50% chance that it will land on heads. We know this because we programmed it in to our sample (sample(c(\"HEADS\", \"TAILS\"), prob = c(0.5, 0.5))). We don’t usually have this luxury of knowing the parameter, so let’s take advantage of this to build up our confidence around good samples and their relationship to the population.\nEven though every time we flip the coin there is a 50% chance it lands on heads, we still get some trials in which we draw many more or far fewer than our expected 50 heads. We have some as low as 28 and some as large as 71. But notice how the number of heads recorded in most of our trials are clustered around our expected 50. The mean of our results is 49.9987 which is really, really close to our known parameter of 0.5 or 50%. Yay!\nSo, even with representative and large samples you will get some error. That’s okay. We can still use that sample to confidently infer what the parameter looks like.\nLet’s extend this a little further. What happens if we conduct less trials? Let’s try with only 100 trials.\n\nresults_100 &lt;- tibble(trial = 1:100) |&gt; \n  rowwise() |&gt; \n  mutate(n_heads = coin_flip(possible_outcomes, 100))\n\nggplot(results_100, aes(x = n_heads)) + \n  geom_histogram() + \n  geom_vline(xintercept = 50)\n\n\n\n\nNot as clean as we would like. The average number of heads drawn in each of these 100 trials is 50.31, which is 0.31 points away from the parameter (compared to -0.0013 for our 100,000 trials).\nWhat about if we decrease the number of draws we make in each trial? Let’s only take 10 draws in our original 100,000 trials.\n\nresults_10 &lt;- tibble(trial = 1:100000) |&gt; \n  rowwise() |&gt; \n  mutate(n_heads = coin_flip(possible_outcomes, 10))\n\nggplot(results_10, aes(x = n_heads)) + \n  geom_histogram() + \n  geom_vline(xintercept = 5)\n\n\n\n\nThe average number of heads drawn in each of these 100,000 trials is 5.0028231, which is 0.0028231 points away from the parameter (compared to -0.0013 for our 100,000 trials).\nThe lessons we can take from this is that more is better. The more times you flip that coin, the closer you will get to the true underlying probability of a fair coin landing on heads.\n\nYou will need to make important decisions in your own research regarding the number of samples with which you are comfortable. This will be constrained by your budget, time, and population. You will get a more accurate picture of the parameter with more data points. However, adding another 1,000,000 responses to your survey may result in a change so small it has no material impact what you infer from your analysis. If this is the case and you have a representative sample, you are well justified in not running yourself dry trying to get those extra observations.\nFor more information on working out the smallest acceptable sample size for an experiment, look up power analysis.\n\n\n\nSampling distributions\nLet’s move on from coin flips. Suppose that we want to know how many Americans identify as Democrats. We will return to the American National Election Survey to answer this question.\nThis survey asks respondents whether they identify as a Democrat (this binary variable takes on 0 if not and 1 if they do).\n\nnes |&gt; \n  select(caseid, dem) |&gt; \n  head()\n\n  caseid dem\n1    408   0\n2   3282   1\n3   1942   0\n4    118   1\n5   5533   0\n6   5880   0\n\n\nLet’s very cheekily pretend that this is a complete survey of the entire voting population of America. That way, we can pretend that we know the proportion of US voters who identify as Democrats (our parameter).\n\ntabyl(nes, dem)\n\n dem    n     percent valid_percent\n   0 3534 0.597363083     0.5997963\n   1 2358 0.398580122     0.4002037\n  NA   24 0.004056795            NA\n\n\nOkay, so let’s pretend that 40% of all US voters identify as Democrats.\nWe can’t survey all voters, so instead we take a representative and large sample from this population:\n\nnes_sample &lt;- nes |&gt; \n  select(caseid, dem) |&gt; \n  slice_sample(n = 3000)\n\nWe have taken a pure random sample of 3,000 (or 51% of our population of 5,916 voters). Each of those voters had an equal probability of being picked for this sample.\nWhat proportion of this sample identify as Democrats?\n\ntabyl(nes_sample, dem)\n\n dem    n     percent valid_percent\n   0 1795 0.598333333     0.6011386\n   1 1191 0.397000000     0.3988614\n  NA   14 0.004666667            NA\n\n\n39.70%. Nice! But what if we took a different sample of 3,000?\n\nnes_sample_2 &lt;- nes |&gt; \n  select(caseid, dem) |&gt; \n  slice_sample(n = 3000)\n\n\ntabyl(nes_sample_2, dem)\n\n dem    n percent valid_percent\n   0 1779   0.593     0.5947844\n   1 1212   0.404     0.4052156\n  NA    9   0.003            NA\n\n\nWe get a different answer: 40.40%. Of course! This is just like our different coin flip trials from last week. Each resulted in a different number of heads. The more flips we did, the closer we got to the true underlying probability distribution.\nLet’s take 1,000 different samples of 3,000 US voters and see what we get:\n\ndem_survey &lt;- function(df, n) {\n  \n  slice_sample(df, n = n) |&gt; \n    tabyl(dem) |&gt; \n    filter(dem == 1) |&gt; \n    pull(percent)\n  \n}\n\nnes_samples_1000 &lt;- tibble(survey = 1:1000) |&gt; \n  rowwise() |&gt; \n  mutate(prop_dem = dem_survey(select(nes, caseid, dem), 3000)) |&gt; \n  ungroup()\n\nnes_samples_1000\n\n# A tibble: 1,000 × 2\n   survey prop_dem\n    &lt;int&gt;    &lt;dbl&gt;\n 1      1    0.395\n 2      2    0.400\n 3      3    0.396\n 4      4    0.386\n 5      5    0.394\n 6      6    0.404\n 7      7    0.396\n 8      8    0.405\n 9      9    0.399\n10     10    0.404\n# ℹ 990 more rows\n\n\n\nggplot(nes_samples_1000, aes(x = prop_dem)) + \n  geom_histogram() + \n  geom_vline(xintercept = tabyl(nes, dem) |&gt; filter(dem == 1) |&gt; pull(percent))\n\n\n\n\nOn average, 39.84% of US voters in our 1,000 samples of 3,000 US voters identified as Democrats. Our (cheeky) population average is 39.86%. Yay! As long as our sample is large and representative, we should be able to infer from our sample what is going on in the population.\nYou’ll have noticed that these draws are always symmetrical or normally distributed around the sample mean (which is; hopefully, also the population mean). This distribution of your statistic is referred to as your sampling distribution. We lean very heavily on some important characteristics of this distribution when doing statistical inference.\nWhen your sample is large and representative, your sampling distribution will be near normally distributed. The center will be at (or very, very close to) the population mean. This is called the Central Limit Theorem. This theorem suggests that statistics (including means, proportions, counts) from large and randomly drawn samples are very good approximations of the underlying (and often unobservable) population parameter.\n\n\nInferring from a single “trial”\nIn a lot of (social) science is is not practical or, in some cases, possible to do many trials. For example, a lot of us study the onset, conduct, and termination of wars. Unlike a game of chess, you cannot reset and run a war many times in order to get your sampling distribution of your variable of interest.\nFurther, we often do not know the shape or size of our population. For example, the best guess we have of the demographics of the US population comes from the census. But this misses a lot of people. If you want to study houselessness, you might need to rely on surveys of samples of people that may or may not be representative of this difficult to reach population of people.\nA lot of the time; therefore, you will have one data point. This requires that we take some lessons learned from above and make some pretty important assumptions.\nLet’s return to our survey work above. We took 1,000 different samples of 3,000 US voters and asked each of them whether they identified as Democrats. We recorded the proportion of the 3,000 respondents who identified as Democrats in each of our 1,000 different samples. We then took the average of those 1,000 different proportions and compared it to our population average. In line with the Central Limit Theorem, we found that the average of our sample statistics was very, very close to our population parameter.\nOkay, now imagine that you could only run one of those trials. Let’s select one at random:\n\nnes_single &lt;- slice_sample(nes_samples_1000)\nnes_single\n\n# A tibble: 1 × 2\n  survey prop_dem\n   &lt;int&gt;    &lt;dbl&gt;\n1    821    0.396\n\n\nHow close is this single sample statistic to the population parameter of 39.86%? Pretty close! In fact, you are more likely to get a sample statistic close to the population parameter than not.\nRemember, when we ran multiple trials we got many sample statistics that were clustered around the population mean.\n\nggplot(nes_samples_1000, aes(x = prop_dem)) + \n  geom_histogram() + \n  geom_vline(xintercept = tabyl(nes, dem) |&gt; filter(dem == 1) |&gt; pull(percent))\n\n\n\n\nSo, if you were to pick one of these trials at random, you are more likely to pick one with a sample statistic that is close to the population parameter than not. Convenient!\n\n\nHow confident can we be in our statistic?\nThat being said, we could get unlucky and have drawn a large and representative sample that sits at one of those extreme values. How confident can we be that our single sample statistic is close to the population parameter?\nRemember back to our week on descriptive statistics. There are some super handy properties of normal distributions on which we will draw.\n\nnorm_5_2 &lt;- tibble(x = rnorm(n = 1e6, mean = 5, sd = 2))\n\nggplot(norm_5_2, aes(x = x)) + \n  stat_slab(\n    aes(fill_ramp = after_stat(cut_cdf_qi(cdf))), fill = met.brewer(\"Egypt\")[2]\n  ) + \n  scale_fill_ramp_discrete(range = c(1, 0.2), guide = \"none\") + \n  theme_minimal()\n\n\n\n\nFor normally distributed data:\n\nApproximately 68% of the data fall within one standard deviation of the mean (the dark blue).\nApproximately 95% of the data fall within two standard deviations of the mean (the medium blue).\nApproximately 99.7% of the data fall within three standard deviations of the mean (the light blue).\n\nSo, if we assume that the statistic we get from our large and representative sample is our “best guess” at the population parameter, we can center our theoretical sampling distribution around this point. We know that this distribution is normally distributed. So we can calculate the shape of that distribution using what we know about normal distributions.\n68 percent of the hypothetical sample statistics would fall within one standard deviation of the mean. We will build into our statistic an acknowledgment of this uncertainty.\nLet’s go back to our single sample of 3,000 respondents from the NES survey to illustrate how we do this.\n\nhead(nes_sample)\n\n  caseid dem\n1   5849   1\n2    595   0\n3   3988   1\n4   3822   0\n5   3157   0\n6   3145   1\n\n\nWe need to calculate the standard deviation of the mean. When we are looking at the standard deviation of the sampling distribution, we refer to it as the standard error.\nThe formula for working out the standard error is:\n\\[\nS_{\\bar{x}} = \\frac{S}{\\sqrt{n}}\n\\]\nThe standard error of the mean is equal to the standard deviation of the whole sample divided by the square root of the sample size.\n\nWe learnt how to calculate the standard deviation of a vector of data (in this case, the whole sample) in the Descriptive Statistics week.\n\nFirst, we need to find the standard deviation of the whole sample. We can use sd():\n\nsd_sample &lt;-sd(nes_sample$dem, na.rm = T)\n  \nsd_sample\n\n[1] 0.4897462\n\n\nThen we need to divide this by the square root of the size of the sample:\n\nse_mean &lt;- sd_sample / sqrt(nrow(nes_sample))\n\nse_mean\n\n[1] 0.008941501\n\n\nYou can use DescTools::MeanSE() to calculate this in one line:\n\nMeanSE(nes_sample$dem, na.rm = T)\n\n[1] 0.008962437\n\n\nThe mean is 0.3989 and the standard error of this mean is 0.0089. Great! So now we know that 68% of the hypothetical means of our hypothetical trials sit within plus or minus 0.0089 of 0.3989.\nTranslating this into our research question: based on our sample, we are 68 percent confident that the true percentage of US voters who identify as Democrats sits between 38.99% and 40.78%.\nOf course, if we took a different sample, we would get a different sample mean and a different confidence interval. The point is that as long as you have a large and representative sample, your sample is more likely than not to be close to the population parameter.\nWhat if you want to be more confident than 68 percent? Again, we can draw on our knowledge of normal distributions to help us out. Let’s work out the bounds within which we are 95 percent confident the proportion of US voters who identify as Democrats sit.\nI have to admit here that I cheated a little in calculating the 68 percent confidence interval. If you remember back to our week on descriptive statistics, you will recall that these ranges (68%, 95%, and 99.7%) are artifacts of our efforts to standardize our data using z-scores.\nA quick refresher that you can promptly forget again:\n\\[\nz_i = \\frac{x_i - \\mu_x}{\\sigma_x}\n\\]\nNow, it just so happens that doing this to your normally distributed data means that 68 percent of your data will land within one z-score of the center point.\nSo, when we calculated the 68 percent confidence interval as:\n\\[\nmean \\pm se\n\\]\nWhat we actually (sneakily) did was:\n\\[\nmean \\pm z * se\n\\]\nThe z-score just so happened to be one.\nWhat z-score captures 95 percent of our data?\n\nqnorm(p = 0.025, lower.tail = F)\n\n[1] 1.959964\n\n\n\nThe first argument in qnorm() is the vector of probabilities. We want to find the 95 percent confidence interval, so we need to find the boundaries beyond which the remaining 5 percent of data sit. Remember that a normal distribution is symmetrical. Data falls above and below our center point. So to get the upper boundary, we need to halve 5 percent (hence p = 0.025) and then ask qnorm() to only give us the upper boundary (lower.tail = F). You can, of course, take the lower boundary but then you have to deal with a negative number which is kind of annoying.\n\nSo, the z-score that will give us 95 percent of our data sitting around our center point is 1.96.\nTherefore, our lower bound is:\n\nse_mean - qnorm(p = 0.025, lower.tail = F) * se_mean\n\n[1] -0.008583519\n\n\nAnd our upper bound is:\n\nse_mean + qnorm(p = 0.025, lower.tail = F) * se_mean\n\n[1] 0.02646652\n\n\nTranslated, this means that we are 95 percent confident that the true percentage of US voters who identify as Democrats sits between 38.13% and 41.64%, based on our sample.\n\nEXERCISE: Calculate the 99.7 percent confidence interval around the mean.\n\nWe are putting a lot of stead in our single sample. That’s okay as long as your sample is large and representative. Over these past few weeks we have discussed in sometimes painful detail why we can make some of the assumptions on which we rely. But, at the end of the day, you are the expert. You have explored your data with a critical eye. You have read everything you possibly can about this topic. You might have even gone out in the field and gotten your hands dirty. The more you know about your subject matter, the better you will be able to detect whether something strange is going on with your sample and your findings. This is so important. The strength of your empirical analysis is built on these foundations.\n\n\nChoosing your sample size\nWe know that our confidence around our point estimate increases with the number of observations. Formally:\n\\[\nS_{\\bar{x}} = \\frac{S}{\\sqrt{n}}\n\\]\nSo, as you increase that \\(n\\), you decrease your standard error and you narrow the interval over which you have a given level of confidence.\nI have said before that more is always better. Technically, this is very, very true. But those marginal returns diminish. And those survey costs stack up.\nTo illustrate, let’s look at how much your standard error decreases as you increase your sample size.\n\nse_from_sample &lt;- function(n_samples) {\n  \n  nes |&gt; \n    slice_sample(n = n_samples) |&gt; \n    pull(dem) |&gt; \n    MeanSE(na.rm = T)\n  \n}\n\ntibble(sample_size = seq(from = 100, to = 3000, by = 100)) |&gt; \n  rowwise() |&gt; \n  mutate(se = se_from_sample(sample_size)) |&gt; \n  ggplot(aes(x = sample_size, y = se)) + \n  geom_line() + \n  theme_minimal() + \n  labs(x = \"Number of observations in the sample\",\n       y = \"Standard error\")\n\n\n\n\nMoving from 100 to 1,000 observations in your sample dramatically decreases your standard error. Moving the same distance from 1,000 to 1,900 makes a far smaller difference to your error.\n\n\nNext week\nWe will discuss the guts of quantitative analysis: hypothesis testing."
  },
  {
    "objectID": "content/12-multiple_regression.html",
    "href": "content/12-multiple_regression.html",
    "title": "Regression Analysis II",
    "section": "",
    "text": "Pollock & Edwards, Chapter 9\n\n\n\n Pollock & Edwards R Companion, Chapter 9"
  },
  {
    "objectID": "content/12-multiple_regression.html#readings",
    "href": "content/12-multiple_regression.html#readings",
    "title": "Regression Analysis II",
    "section": "",
    "text": "Pollock & Edwards, Chapter 9\n\n\n\n Pollock & Edwards R Companion, Chapter 9"
  },
  {
    "objectID": "content/12-multiple_regression.html#class-slides",
    "href": "content/12-multiple_regression.html#class-slides",
    "title": "Regression Analysis II",
    "section": "Class slides",
    "text": "Class slides"
  },
  {
    "objectID": "content/12-multiple_regression.html#section",
    "href": "content/12-multiple_regression.html#section",
    "title": "Regression Analysis II",
    "section": "Section",
    "text": "Section"
  },
  {
    "objectID": "content/09-hypothesis_testing.html",
    "href": "content/09-hypothesis_testing.html",
    "title": "Hypothesis Testing III",
    "section": "",
    "text": "Pollock & Edwards R Companion, Chapter 7"
  },
  {
    "objectID": "content/09-hypothesis_testing.html#readings",
    "href": "content/09-hypothesis_testing.html#readings",
    "title": "Hypothesis Testing III",
    "section": "",
    "text": "Pollock & Edwards R Companion, Chapter 7"
  },
  {
    "objectID": "content/09-hypothesis_testing.html#class-slides",
    "href": "content/09-hypothesis_testing.html#class-slides",
    "title": "Hypothesis Testing III",
    "section": "Class slides",
    "text": "Class slides"
  },
  {
    "objectID": "content/09-hypothesis_testing.html#section",
    "href": "content/09-hypothesis_testing.html#section",
    "title": "Hypothesis Testing III",
    "section": "Section",
    "text": "Section"
  },
  {
    "objectID": "content/13-conclusion.html#section",
    "href": "content/13-conclusion.html#section",
    "title": "Regression Analysis Extensions & Final Exam Review",
    "section": "Section",
    "text": "Section"
  },
  {
    "objectID": "content/04-research_design.html",
    "href": "content/04-research_design.html",
    "title": "Research Design",
    "section": "",
    "text": "Pollock & Edwards, Chapter 4\n\n\n\n Pollock & Edwards R Companion, Chapter 5"
  },
  {
    "objectID": "content/04-research_design.html#readings",
    "href": "content/04-research_design.html#readings",
    "title": "Research Design",
    "section": "",
    "text": "Pollock & Edwards, Chapter 4\n\n\n\n Pollock & Edwards R Companion, Chapter 5"
  },
  {
    "objectID": "content/04-research_design.html#section",
    "href": "content/04-research_design.html#section",
    "title": "Research Design",
    "section": "Section",
    "text": "Section\n\nPrerequisites\n\ninstall.packages(c(\"rio\", \"httr2\", \"rvest\"))\n\n\nlibrary(tidyverse)\nlibrary(rio)\nlibrary(httr2)\nlibrary(rvest)\n\n\n\nHow will what you learn this week help your research?\nCritical to our ability to understand the relationship between our outcome of interest and all of the variables that we think determine (or are, at least, associated) with that outcome is our ability to observe those variables moving with one another. To do this, we need quality data that accurately measure those variables. This week, we discuss how you can collect that quality data. The method by which you do this is very important to the quality of that data (i.e. how accurately it reflects the phenomena in which you are interested).\nWe have two broad methods at our disposal: observational research and experimental studies. Experimental studies are considered to be the gold standard for causal inference (your ability to confidently say that changes to your independent variable cause changes to your dependent variable).\nExperiments are not always feasible or appropriate. In fact, a lot of political science research (particularly international relations research) relies on observational studies. Unlike experiments, in which the researcher intervenes, observational research involves recording changes in your variables without influencing those variables.\nWhen you conduct your own research, you will need to make important decisions about what data you will use to uncover the relationships described by your theory. This week introduces you to some of the factors you will need to consider when making those decisions.\n\n\nData collection\nA lot of political science research uses observational data to make inferences about the relationship between some outcome of interest and various factors that are associated with that outcome of interest. For example, we collect data on the incidence of civil war. We also collect data on the regime type of the governments that have experienced civil wars and those that have not. We can then look at whether more civil wars occur in countries with particular regime types. This week, I will introduce some common techniques for collecting observational data used by political scientists.\n\nCommon data sets\nThere are many large-scale data sets that are commonly used in political science research. These include: the American National Election Survey, with which you are becoming very familiar; the Varieties of Democracy data set; and various Correlates of War data sets, include the Militarized Interstate Disputes data set.\nThe easiest way to access these data sets is via their associated websites. You can download the data file, store it in your R project, and use it in your analysis.\nHowever, these data sets are updated regularly. It can be tedious to download new data files every time they are updated.\n\n\nProgrammatically downloading data sets from the internet\nIt is very easy to download a file programmatically. The URL used to access the file can be thought of as a file path. Therefore, all you need to do is provide that URL (file path) to the appropriate R command and you will read in that data set directly from the internet.\nFor example, let’s read the UCDP/PRIO Armed Conflict Data Set into our current R session.\n\n\n\n\n\n\nNote\n\n\n\nFrom the UCDP website, the UCDP/PRIO Armed Conflict Data Set is:\nA conflict-year dataset with information on armed conflict where at least one party is the government of a state in the time period 1946-2022.\n\n\nWe can use the very flexible rio::import() function to do this:\n\nucdp_actor_df &lt;- import(\"https://ucdp.uu.se/downloads/ucdpprio/ucdp-prio-acd-231-csv.zip\") |&gt; \n  as_tibble()\n\nhead(ucdp_actor_df)\n\n# A tibble: 6 × 28\n  conflict_id location   side_a side_a_id side_a_2nd side_b side_b_id side_b_2nd\n        &lt;int&gt; &lt;chr&gt;      &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;     \n1       11342 India      Gover… 141       \"\"         GNLA   1163      \"\"        \n2       11342 India      Gover… 141       \"\"         GNLA   1163      \"\"        \n3       11343 Egypt, Is… Gover… 117       \"\"         Gover… 121       \"\"        \n4       11343 Egypt, Is… Gover… 117       \"\"         Gover… 121       \"\"        \n5       11343 Egypt, Is… Gover… 117       \"\"         Gover… 121       \"\"        \n6       11343 Egypt, Is… Gover… 117       \"\"         Gover… 121       \"\"        \n# ℹ 20 more variables: incompatibility &lt;int&gt;, territory_name &lt;chr&gt;, year &lt;int&gt;,\n#   intensity_level &lt;int&gt;, cumulative_intensity &lt;int&gt;, type_of_conflict &lt;int&gt;,\n#   start_date &lt;IDate&gt;, start_prec &lt;int&gt;, start_date2 &lt;IDate&gt;,\n#   start_prec2 &lt;int&gt;, ep_end &lt;int&gt;, ep_end_date &lt;IDate&gt;, ep_end_prec &lt;lgl&gt;,\n#   gwno_a &lt;chr&gt;, gwno_a_2nd &lt;chr&gt;, gwno_b &lt;chr&gt;, gwno_b_2nd &lt;chr&gt;,\n#   gwno_loc &lt;chr&gt;, region &lt;chr&gt;, version &lt;dbl&gt;\n\n\nInsofar as that URL points to the most up-to-date data file, you now have programmatic access to that data set.\n\n\nAPIs\nThis process is great because it is very straightforward and easy to implement. However, it can often not be very durable. Sometimes links break, or the authors of the data set change the structure of the data set stored in that data file, or they make a new link for more up-to-date data.\nPopular data sets often come with APIs, or Application Programming Interfaces. These can help us maintain durable, programmatic access to our data.\nAPIs can get a bit complicated. Today, I am going to briefly introduce you to them and some packages that can help you work with them from R. This is, by no means, a comprehensive tutorial on working with APIs, but it should give you the tools to get started with this very useful data collection process.\nThe best package in R for working with APIs is httr2. The documentation for this package is great and I would encourage you to have a look at it if you get stuck.\nUCDP provides an API to access their data sets. You can see the documentation for this API here: https://ucdp.uu.se/apidocs/.\nTo start, we need to load the httr2 package into our R session.\n\nlibrary(httr2)\n\nNext, we need to create our API request. For the UCDP API (and many other APIs), this essentially involves constructing a URL from which you will download the data.\nThe UCDP API request takes four different arguments:\n\nThe &lt;resource&gt;, or the data set you want to access. We will stick with the UCDP/PRIO Armed Conflict Data Set.\nThe &lt;version&gt;, or the version of the data set you want to access. As I said, this data set is regularly updated. At the time of writing, the latest version of the data set is 23.1.\nThe &lt;pagesize&gt; and page. This data set is (kind of) large: it has 2,626 rows. The API will only provide between 1 and 1,000 rows at a time. Therefore, if you want to return more than 1,000 rows (as you would need to if you wanted to access the whole data set), you need to set the page size to 1,000 and iterate over multiple pages (in this case, we need to go through 3 pages to get the full 2,626 rows.\n\nLet’s start simply by accessing the first 10 rows of the data set.\nFirst, we need to get the base URL. This is provided in the UCDP API documentation. You will see that they provide the following base URL:\n\nucdp_url &lt;- \"https://ucdpapi.pcr.uu.se/api/&lt;resource&gt;/&lt;version&gt;?&lt;pagesize=x&gt;&&lt;page=x&gt;\"\n\nYou can see where we need to insert our four arguments: &lt;resource&gt;, &lt;version&gt;, &lt;pagesize=x&gt;, and &lt;page=x&gt;. We can get the relevant values from the UCDP API documentation. These are:\n\n&lt;resource&gt;: ucdpprioconflict for the UCDP/PRIO Armed Conflict Data Set;\n&lt;version&gt;: 23.1.\n\nWe will start slowly by retrieving the first 10 rows of this data set. Therefore:\n\n&lt;pagesize=x&gt;: &lt;pagesize=10&gt;;\npage=x: page=1.\n\nAdding these into our base URL creates the following request URL:\n\nucdp_url &lt;- \"https://ucdpapi.pcr.uu.se/api/ucdpprioconflict/23.1?pagesize=10&page=1\"\n\nNow that we have our URL, we can make our request to the API using httr2::request():\n\nreq &lt;- request(ucdp_url)\nreq\n\nNext we need to perform the request using httr2::req_perform():\n\nresp &lt;- req |&gt; \n  req_perform()\n\nWe can check the status of our request:\n\nresp |&gt; \n  resp_status()\n\n[1] 200\n\n\n200 is good! It means that we have successfully performed our request. For a full list of HTTP status codes, look here.\nNow, let’s get our data! You can look at it using the following command:\n\nresp |&gt; \n  resp_body_json()\n\nThe response is an array of JSON objects. JSON is just a very light-weight way of sharing data. Lots of APIs will respond with this structure.\nThe API gave us lots of information. We want the Result object. This is where the data set is stored. First, we need to access the data from the response. We do this using the httr2::resp_body_json() function.\n\nresults_raw &lt;- resp |&gt; \n  resp_body_json()\n\nThe data set is stored in the Results object. You can have a look at it by running this:\n\nresults_raw$Result\n\nThis is a JSON object, which can be a bit difficult to work with. It is much easier to work with a tibble or data frame. To convert this JSON object to a tibble, we can use tibble::enframe() and tidyr::unnest_wider() (both of which are loaded with tidyverse.\n\nresults_df &lt;- enframe(results_raw$Result) |&gt; \n  unnest_wider(value)\n\nresults_df\n\n# A tibble: 10 × 29\n    name conflict_id location    side_a    side_a_Id side_a_2nd side_b side_b_Id\n   &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;  &lt;chr&gt;    \n 1     1 11345       South Sudan Governme… 113       \"Governme… SPLM/… 4226     \n 2     2 11345       South Sudan Governme… 113       \"Governme… SPLM/… 4226     \n 3     3 11345       South Sudan Governme… 113       \"\"         SPLM/… 4226     \n 4     4 11345       South Sudan Governme… 113       \"\"         SPLM/… 4226     \n 5     5 11345       South Sudan Governme… 113       \"\"         SPLM/… 4226     \n 6     6 11345       South Sudan Governme… 113       \"\"         NAS    6794     \n 7     7 11345       South Sudan Governme… 113       \"\"         NAS    6794     \n 8     8 11345       South Sudan Governme… 113       \"\"         NAS, … 6794, 82…\n 9     9 11346       Libya       Governme… 111       \" Governm… Force… 1126, 11…\n10    10 11346       Libya       Governme… 111       \"\"         ASL    7046     \n# ℹ 21 more variables: side_b_2nd &lt;chr&gt;, incompatibility &lt;chr&gt;,\n#   territory_name &lt;chr&gt;, year &lt;chr&gt;, intensity_level &lt;chr&gt;,\n#   cumulative_intensity &lt;chr&gt;, type_of_conflict &lt;chr&gt;, start_date &lt;chr&gt;,\n#   start_prec &lt;chr&gt;, start_date2 &lt;chr&gt;, start_prec2 &lt;chr&gt;, ep_end &lt;chr&gt;,\n#   ep_end_date &lt;chr&gt;, ep_end_prec &lt;chr&gt;, gwno_a &lt;chr&gt;, gwno_a_2nd &lt;chr&gt;,\n#   gwno_b &lt;chr&gt;, gwno_b_2nd &lt;chr&gt;, gwno_loc &lt;chr&gt;, region &lt;chr&gt;, version &lt;chr&gt;\n\n\nCool! We now have the first 10 rows of the UCDP/PRIO Armed Conflict Data Set.\nYou can do more precise requests with APIs. For example, you can request data for specific countries or time frames. This can be very useful when you are working with big data because it can allow you to work in memory by segmenting your analysis.\n\n\nWeb scraping\nAnother useful method for collecting data from the internet is web scraping. Web scraping allows you to extract data from a website. I am going to very quickly introduce you to this process and some useful R packages for performing web scraping. Again, this will be a very cursory introduction that aims to provide you with the tools to build up this skill if you need it.\nThe best package for web scraping in R is rvest. We can use this package to harvest data from the web.\nFirst, let’s load rvest into our current R session:\n\nlibrary(rvest)\n\nLet’s practice by creating a data set of all of the UNSC resolutions passed this year. The UNSC provides a table of these resolutions on its website:\n\nThis table includes some very useful information for anyone interested in looking at UNSC behaviour over time. It provides the unique resolution ID, the data of adoption, and a brief title or description of the resolution. We are going to use rvest and friends to read that information into R so that we can analyze it.\nThe first step you need to do is to read in the web page:\n\nunsc_res &lt;- read_html(\"https://www.un.org/securitycouncil/content/resolutions-adopted-security-council-2023\")\n\nWeb pages are (generally) HTML files. HTML is a markup language that is the standard format for documents displayed in web browsers.\nWe can have a look at this HTML file:\n\nunsc_res\n\n{html_document}\n&lt;html lang=\"en\" dir=\"ltr\"&gt;\n[1] &lt;head&gt;\\n&lt;meta charset=\"utf-8\"&gt;\\n&lt;meta name=\"viewport\" content=\"width=devi ...\n[2] &lt;body class=\"html not-front not-logged-in no-sidebars page-node page-node ...\n\n\nI am not about to teach you yet another scripting language. All you need to know is that HTML documents are highly structured. We can use this structure to point R to the specific part of the document that we want to scrape.\nHere is a basic example of HTML (from the rvest documentation):\n&lt;html&gt;\n&lt;head&gt;\n  &lt;title&gt;Page title&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n  &lt;h1 id='first'&gt;A heading&lt;/h1&gt;\n  &lt;p&gt;Some text &amp; &lt;b&gt;some bold text.&lt;/b&gt;&lt;/p&gt;\n  &lt;img src='myimg.png' width='100' height='100'&gt;\n&lt;/body&gt;\nHTML is hierarchical: all elements are nested within one another, providing them with a place within the documentation.\nAll HTML elements are contained within two tags: a start tag (&lt;tag&gt;) and corresponding end tag (&lt;/tag&gt;). These tags can have attributes that can group them together.\nFor example, the above HTML code includes a heading (&lt;h1&gt;) that includes an id attribute 'first'. The text of the heading - A heading - is contained within the heading tag. We know where the heading ends because it closes out with a &lt;/h1&gt; tag.\nHere, we want to scrape the table that contains all the useful information about the resolutions. We need to find the part of the HTML document that contains that table.\nHead back to the web page. We are looking at the rendered HTML. We need to find where in this document the table is located. To do this, right click anywhere in the table and select Inspect.\n\nA new window will pop up on the side of your web page that shows you the raw HTML language that is generating this web page.\n\nThis can look a bit weird if you have not come across HTML before, but it can be easy to work with. Scroll your mouse across different lines in that raw HTML window. You will see the corresponding parts of the rendered page light up.\n\nScroll until you have highlighted the whole table. You will be on a line that reads: &lt;table class=\"table table-striped table-sm\"&gt;.\n\nWithout getting too into the HTML weeds, this is the code used to generate the table. It includes a couple of unique identifiers that are useful for web scraping. Here, we are going to use the unique class of this table.\nThe class is table table-striped table-sm. When providing that to httr2::html_element() (which is function we use to select that part of the HTML document), we need to replace those spaces with full stops:\n\nunsc_res |&gt; \n  html_element(\"table.table-striped.table-sm\")\n\n{html_node}\n&lt;table class=\"table table-striped table-sm\"&gt;\n[1] &lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;td&gt;&lt;a href=\"http://undocs.org/en/S/RES/2697(2023)\"&gt;S/RES/ ...\n\n\nWe have filtered the whole HTML document to only the table we want to select. We don’t want to work with this table in HTML. Let’s convert it into a data frame. We can use the very helpful httr2::html_table() function to extract this tabular data and convert it:\n\nunsc_res |&gt; \n  html_element(\"table.table-striped.table-sm\") |&gt; \n  html_table()\n\n# A tibble: 26 × 3\n   X1                X2                X3                                       \n   &lt;chr&gt;             &lt;chr&gt;             &lt;chr&gt;                                    \n 1 S/RES/2697 (2023) 15 September 2023 Threats to international peace and secur…\n 2 S/RES/2696 (2023) 7 September 2023  The situation in Somalia                 \n 3 S/RES/2695 (2023) 31 August 2023    The situation in the Middle East (UNIFIL)\n 4 S/RES/2694 (2023) 2 August 2023     Identical letters dated 19 January 2016 …\n 5 S/RES/2693 (2023) 27 July 2023      The situation in the Central African Rep…\n 6 S/RES/2692 (2023) 14 July 2023      The question concerning Haiti (BINUH)    \n 7 S/RES/2691 (2023) 10 July 2023      The situation in the Middle East (UNMHA) \n 8 S/RES/2690 (2023) 30 June 2023      The situation in Mali (MINUSMA)          \n 9 S/RES/2689 (2023) 29 June 2023      The situation in the Middle East (UNDOF) \n10 S/RES/2688 (2023) 27 June 2023      The situation concerning the Democratic …\n# ℹ 16 more rows\n\n\nAwesome! We now have a table of all of the resolutions passed by the UNSC this year. Let’s clean it up a bit:\n\nunsc_res_df &lt;- unsc_res |&gt; \n  html_element(\"table.table-striped.table-sm\") |&gt; \n  html_table() |&gt; \n  rename(id = X1,\n         date_adoped = X2,\n         title = X3)\n\nunsc_res_df\n\n# A tibble: 26 × 3\n   id                date_adoped       title                                    \n   &lt;chr&gt;             &lt;chr&gt;             &lt;chr&gt;                                    \n 1 S/RES/2697 (2023) 15 September 2023 Threats to international peace and secur…\n 2 S/RES/2696 (2023) 7 September 2023  The situation in Somalia                 \n 3 S/RES/2695 (2023) 31 August 2023    The situation in the Middle East (UNIFIL)\n 4 S/RES/2694 (2023) 2 August 2023     Identical letters dated 19 January 2016 …\n 5 S/RES/2693 (2023) 27 July 2023      The situation in the Central African Rep…\n 6 S/RES/2692 (2023) 14 July 2023      The question concerning Haiti (BINUH)    \n 7 S/RES/2691 (2023) 10 July 2023      The situation in the Middle East (UNMHA) \n 8 S/RES/2690 (2023) 30 June 2023      The situation in Mali (MINUSMA)          \n 9 S/RES/2689 (2023) 29 June 2023      The situation in the Middle East (UNDOF) \n10 S/RES/2688 (2023) 27 June 2023      The situation concerning the Democratic …\n# ℹ 16 more rows\n\n\nFor more information on web-scraping, I recommend reading the Web scraping 101 article in the rvest documentation."
  },
  {
    "objectID": "content/03-bivariate_relationships.html",
    "href": "content/03-bivariate_relationships.html",
    "title": "Relationships Between Two Variables",
    "section": "",
    "text": "Pollock & Edwards, Chapter 3\n\n\n\n Pollock & Edwards R Companion, Chapters 4-5"
  },
  {
    "objectID": "content/03-bivariate_relationships.html#readings",
    "href": "content/03-bivariate_relationships.html#readings",
    "title": "Relationships Between Two Variables",
    "section": "",
    "text": "Pollock & Edwards, Chapter 3\n\n\n\n Pollock & Edwards R Companion, Chapters 4-5"
  },
  {
    "objectID": "content/03-bivariate_relationships.html#class-slides",
    "href": "content/03-bivariate_relationships.html#class-slides",
    "title": "Relationships Between Two Variables",
    "section": "Class slides",
    "text": "Class slides"
  },
  {
    "objectID": "content/03-bivariate_relationships.html#section",
    "href": "content/03-bivariate_relationships.html#section",
    "title": "Relationships Between Two Variables",
    "section": "Section",
    "text": "Section\n\nPrerequisites\n\ninstall.packages(c(\"broom\", \"janitor\", \"ggridges\", \"modelsummary\"))\n\n\nlibrary(tidyverse)\nlibrary(wbstats)\nlibrary(poliscidata)\nlibrary(countrycode)\nlibrary(broom)\nlibrary(janitor)\nlibrary(ggridges)\nlibrary(modelsummary)\n\n\n\nHow will what you learn this week help your research?\nAs usual, we start with an interesting question. You have some outcome of interest and you think that there is an important determinant of that outcome that no one has yet identified. Or perhaps the relationship between some heavily chewed over determinant and the outcome of interest is misunderstood. We are steadily building up your ability to determine empirically the relationship between that determinant and the outcome of interest.\nSimply put (and there really is no need to over-complicate this), we have two or more variables: an outcome of interest (the dependent variable) and a set of independent variables that we theorize are important determinants of that outcome. We can use empirical analysis to understand 1) how the dependent and independent variables change in relation to one another, and 2) whether this relationship is strong enough that we should declare (though 12,000-word journal articles or by shouting from rooftops) that whenever we want to change or understand that outcome, we must consider these important independent variables.\nLast week, we discussed various tools that you can use to explore your data. You can develop a very good understanding of each of the individual variables. This exploration is very important for building your intuition and, by extension, your expertise in the question at hand. These tools also allow you to identify unusual data points (or outliers).\nThis week, we will make the next step. We will explore how two variables relate to each other. How do they move with each other: when one goes up, does the other go down, up, or not really move? How strong is this association?\nThis exploration is particularly important for you to do with regard to the independent variable(s) that are the focal point of your theory and, therefore, your contribution to our understanding of the messy spaghetti bowl of things that determine your outcome of interest. Just as it is important for you to spend some time understanding the shape of your variables (using the tools we discussed last week), you must also start to understand the shape of the relationship between the outcome you are trying to understand or predict and the factors you think are important determinants of that outcome.\nLet’s begin!\n\n\nBivariate relationships\nHow do two variables move with one another? When when goes up, does the other go down, up, or not really move at all? How dramatic is this shift?\nThe type of variables we have determines how we can answer this question. To begin, we will explore the relationship between two continuous variables. Later in the class, we will look at how to explore the relationship between a continuous and categorical variable.\nTo start, we will explore the relationship between wealth and health. This question was made popular by Hans Rosling’s Gapminder project.\n\n\n\nCollecting our data\nFirst, we need to collect our data. Following Rosling, we will use each country’s average life expectancy to measure its health and the country’s GDP per capita to measure its wealth. We will use wbstats::wb_data() to pull these data directly from the World Bank.\n\ngapminder_df &lt;- wb_data(\n  indicator = c(\"SP.DYN.LE00.IN\", \"NY.GDP.PCAP.CD\"),\n  start_date = 2016,\n  end_date = 2016\n) |&gt; \n  rename(\n    life_exp = SP.DYN.LE00.IN,\n    gdp_per_cap = NY.GDP.PCAP.CD\n  ) |&gt; \n  mutate(\n    log_gdp_per_cap = log(gdp_per_cap),\n    region = countrycode(country, \"country.name\", \"region\", custom_match = c(\"Turkiye\" = \"Europe & Central Asia\"))\n  ) |&gt; \n  relocate(region, .after = country)\n\ngapminder_df\n\n# A tibble: 217 × 8\n   iso2c iso3c country         region  date gdp_per_cap life_exp log_gdp_per_cap\n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;           &lt;chr&gt;  &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;           &lt;dbl&gt;\n 1 AW    ABW   Aruba           Latin…  2016      28451.     75.6           10.3 \n 2 AF    AFG   Afghanistan     South…  2016        520.     63.1            6.25\n 3 AO    AGO   Angola          Sub-S…  2016       1710.     61.1            7.44\n 4 AL    ALB   Albania         Europ…  2016       4124.     78.9            8.32\n 5 AD    AND   Andorra         Europ…  2016      39931.     NA             10.6 \n 6 AE    ARE   United Arab Em… Middl…  2016      41055.     79.3           10.6 \n 7 AR    ARG   Argentina       Latin…  2016      12790.     76.3            9.46\n 8 AM    ARM   Armenia         Europ…  2016       3680.     74.7            8.21\n 9 AS    ASM   American Samoa  East …  2016      13301.     NA              9.50\n10 AG    ATG   Antigua and Ba… Latin…  2016      15863.     78.2            9.67\n# ℹ 207 more rows\n\n\n\n\nWhat is the relationship between two variables?\nWhat is the relationship between a country’s average life expectancy and its GDP per capita? The easiest way to determine this is to visualize these two variables.\n\nggplot(gapminder_df, aes(x = gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  theme_minimal() + \n  labs(x = \"GDP per capita (USD current)\",\n       y = \"Average life expectancy (years)\") + \n  scale_x_continuous(labels = label_dollar())\n\n\n\n\nThere seems to be a good case that there is a strong relationship between a country’s GDP per capita (wealth) and its average life expectancy (health).\nBecause we want to explore linear relationships at this stage of the course, we will look at the logged GDP per capita variable:\n\nggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  theme_minimal() + \n  labs(x = \"Logged GDP per capita\",\n       y = \"Average life expectancy (years)\") + \n  scale_x_continuous(labels = label_dollar())\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can transform your data to make it easier to work with. Just remember that you now need to talk in terms of logged GDP per capita instead of GDP per capita.\n\n\nI can imagine drawing a straight line among these points that summarises how they vary with each other. It appears that as a country’s logged GDP per capita increases, so too does the average life expectancy of its population. As wealth increases, so too does health.\nWell, that was easy! What is the relationship between health and wealth? They increase with each other.\n\n\nHow can we measure the strength of that relationship?\nNow we need some way of measuring the strength of the relationship. In other words, what amount of the variation in countries’ average life expectancy is associated with variation in their GDP per capita? We can measure the strength of this association using correlations. The correlation coefficient tells us how closely variables relate to one another. It tells us both the strength and direction of the association.\n\nStrength: how closely are these values tied to one another. Measured from 0 to |1|, with values closer to 0 indicating a very weak relationship and values closer to |1| indicating a very strong relationship.\nDirection: do both \\(X\\) and \\(Y\\) change in the same direction? Positive correlations show that when \\(X\\) increases (decreases), so does \\(Y\\). Negative correlations show that when \\(X\\) increases (decreases), \\(Y\\) decreases (increases). In other words, they move in different directions.\n\nWhat is the correlation between logged GDP per capita and life expectancy?\n\ncor(gapminder_df$log_gdp_per_cap, gapminder_df$life_exp, use = \"complete.obs\")\n\n[1] 0.8494337\n\n\nAs expected, the relationship is positive and strong.\n\n\nBuilding a generalizable description of this relationship\nWe have very quickly gained the skills to determine whether the relationship between two variables is positive, negative, or non-existent. We have also learnt how to describe the strength of that relationship. To that end, we are now able to describe the bivariate relationship between health and wealth as a positive and strong one.\nThis is useful, but we tend to need a more concrete way of describing the relationship between two variables. For example, what if a policy-maker comes up to you and asks what you think the effect of a $1,000 increase in a country’s GDP per capita will do to its average life expectancy? We can build simple models of this relationship to provide that policy-maker with a prediction of what we might expect to happen on average. Further, we can use the model to describe the relationship between these two variables in a generalized way. If a new country were to spring into existence, we can use our knowledge of its GDP per capita to determine how long we might expect its citizens to live.\n\n\nOLS and linear regression\nLooking back at our data, we can image a straight line running between each country’s plotted average life expectancy and GDP per capita. Let’s draw that line.\n\nggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  theme_minimal() + \n  labs(x = \"Logged GDP per capita\",\n       y = \"Average life expectancy (years)\") + \n  scale_x_continuous(labels = label_dollar())\n\n\n\n\nWe can, of course, draw many different lines through these points. Each of us has probably drawn a slightly different line in our heads. Which is the best line? Ordinary least squares (OLS) regression provides an answer. Simply put, OLS regression draws the line that minimizes the distance between itself and all of the data points. That line can take many shapes, including a straight line, an S, a frowney face, and smiley face, etc.\nLooking at our data above, it appears that a straight line is the best line to draw.\n\n\n\n\n\n\nNote\n\n\n\nOverfitting involves fitting a model (or drawing a line through our data) that misses the forest for the trees. You can draw all kinds of shapes through those data that perhaps result in a smaller distance between itself and each dot. In fact, if you draw a line that connects all of those dots there will be no difference between your line and the data points. However, this model will be too focused on the data we have at hand. Our model will have no idea what to do with any new data points we introduce. This is bad! Your aim here is to produce a generalizable model of the relationship between these two variables, not to draw a line that connects this particular constellation of dots.\n\n\nOkay, so a straight line is the best type of line to draw. But there are still many, many different straight lines that we can draw. Which straight line is best? Remember, OLS regression finds the line that minimizes the distance between itself and all of the data points. Let’s step through this. Look at the graph above.\n\nDraw a line through those dots. Pick a line, any line!\nCalculate the distance between each dot and the line.\nSum up the absolute values of those distances. Remember, we just care about the distance, so we don’t need to worry about whether or not the dots are above or below the line.\nRepeat steps 1 - 3 many, many, many times.\nPick the line with the smallest sum of distances (or results from step 3).\n\nPhew, this seems tedious. Happily, maths and R are to the rescue. Here is the line that minimizes those distances (all with the addition of one extra line of code).\n\nggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = F) + \n  theme_minimal() + \n  labs(x = \"Logged GDP per capita\",\n       y = \"Average life expectancy (years)\") + \n  scale_x_continuous(labels = label_dollar())\n\n\n\n\n\n\nEstimating a linear model in R\nHow did R do this? To answer this, we will first do some review.\nRemember the general equation for a line:\n\\[\ny = a + mx\n\\]\nRead this as: the value of \\(y\\) is the sum of some constant, \\(a\\), and some \\(x\\) variable that has been transformed by some slope value \\(m\\).\n\n\n\n\n\n\nNote\n\n\n\nRemember that the slope constant, \\(m\\), tells you how much \\(y\\) changes for every one unit increase in \\(x\\).\n\n\nSo, if:\n\\[\ny = 10 + 2x\n\\]\nThen, when \\(x = 20\\):\n\\[\ny = 10 + 2*20 = 50\n\\]\nFor many values of \\(x\\):\n\nggplot(tibble(x = 0:50, y = 10 + 2*x), aes(x = x, y = y)) + \n  geom_line(colour = \"lightgrey\", linewidth = 3) + \n  geom_point() + \n  theme_minimal()\n\n\n\n\nWell, let’s substitute in our variables of interest. Our \\(y\\) variable is a country’s average life expectancy and our \\(x\\) variable is that country’s logged GDP per capita.\n\\[\nlife Exp_x = \\beta_0 + \\beta_1 logGdpPerCap_x + \\epsilon\n\\]\nRead this as: a country’s average life expectancy is a function of some constant (\\(\\beta_0\\)) and its logged GDP per capita transformed by some value \\(\\beta_1\\) with some random error (\\(\\epsilon\\)), on average.\nLet’s imagine that this relationship is accurately described by the following formula:\n\\[\nlife Exp_x = 30 + 4 * logGdpPerCap_x\n\\]\n\n\n\n\n\n\nNote\n\n\n\nWe will get to that pesky error term in just a minute.\n\n\nThen, our model would predict the following average life expectancy for countries with log GDPs per capita between 0 and 20:\n\nggplot(\n  tibble(log_gdp_per_cap = 0:20, life_exp = 30 + 4*log_gdp_per_cap), \n  aes(x = log_gdp_per_cap, y = life_exp)\n) + \n  geom_line(colour = \"lightgrey\", linewidth = 3) + \n  geom_point() + \n  theme_minimal() + \n  labs(x = \"Logged GDP per capita\",\n       y = \"Average life expectancy (years)\") + \n  scale_x_continuous(labels = label_dollar())\n\n\n\n\nA country with a logged GDP per capita of 5 (the equivalent of a GDP per capita of $148.41) has a predicted average life expectancy of 50 years, or \\(30 + 4*5\\).\nA country with a logged GDP per capita of 10 (the equivalent of a GDP per capita of $22,026.47) has a predicted average life expectancy of 70 years, or \\(30 + 4*10\\).\nDoes this accurately describe what we see in our data? What is the average life expectancy for countries with roughly $22,000 GDP per capita?\n\ncountries_10 &lt;- filter(gapminder_df, gdp_per_cap &gt; 21000 & gdp_per_cap &lt; 23000)\n\ncountries_10\n\n# A tibble: 3 × 8\n  iso2c iso3c country          region  date gdp_per_cap life_exp log_gdp_per_cap\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;            &lt;chr&gt;  &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;           &lt;dbl&gt;\n1 BH    BHR   Bahrain          Middl…  2016      22867.     79.6           10.0 \n2 KN    KNA   St. Kitts and N… Latin…  2016      21095.     71.7            9.96\n3 SI    SVN   Slovenia         Europ…  2016      21678.     81.2            9.98\n\n\nWe predicted 70 years, but our data suggest that these countries have closer to an average of 77 years. Why do we have this difference?\nWell, we probably haven’t produced the best model we can (this isn’t the best line!). We just picked those numbers out of thin air. Let’s fit a linear OLS regression and see if we improve our ability to predict what we have seen in the wild.\n\nHow do we calculate the constant (\\(\\beta_0\\)) using OLS regression?\nRemember, OLS regression simply finds the line that minimizes the distance between itself and all the data points. The constant that minimizes this distance is the mean of \\(Y\\) minus \\(\\beta_1\\) times the mean of \\(X\\).\nSo, the constant that best predicts a country’s average life expectancy based on its logged GDP per capita is equal to the average life expectancy across our sample (72.3 years) minus the average logged GDP per capita ($8.80, or $6,633.36 GDP per capita) transformed by \\(\\beta_1\\).\nSo…\n\n\nHow do we calculate the coefficient \\(\\beta_1\\)?\nThe regression slope is the correlation coefficient between \\(X\\) and \\(Y\\) multiplied by the standard deviation of \\(Y\\) divided by the standard deviation of \\(X\\).\nEw… Let’s step through that.\nRemember, the correlation coefficient simply measures how \\(X\\) and \\(Y\\) change together. Does \\(Y\\) increase when \\(X\\) increases? How strong is this relationship?\nThe standard deviations of \\(X\\) and \\(Y\\) just measure how spread out they are.\nBringing these together, we are interested in how much \\(X\\) and \\(Y\\) change together moderated by how much they change independently of each other.\nFormally:\n\\[\n\\beta_1 = (\\frac{\\Sigma(\\frac{x_i - \\bar{x}}{s_X})(\\frac{y_i - \\bar{y}}{s_Y})}{n - 1})(\\frac{s_Y}{s_X}) = \\frac{\\Sigma(x_i - \\bar{x})(y_i - \\bar{y})}{\\Sigma(x_i - \\bar{x})^2}\n\\]\nHappily R does all of this for us.\n\n\nLet’s fit that model already!\n\nm &lt;- lm(life_exp ~ log_gdp_per_cap, data = gapminder_df)\n\nm\n\n\nCall:\nlm(formula = life_exp ~ log_gdp_per_cap, data = gapminder_df)\n\nCoefficients:\n    (Intercept)  log_gdp_per_cap  \n         32.807            4.521  \n\n\nOkay, so the line of best fit describing the relationship between life expectancy and logged GDP per capita is:\n\\[\nlife Exp_x = 32.9 + 4.5 * logGdpPerCap_x + \\epsilon\n\\]\nThat’s it! We now have a generalized model of the relationship between a country’s average life expectancy and its logged GDP per capita. This model is informed by what we actually observed in the world. It carefully balances our need to accurately describe what we have observed and to develop something that is generalizable.\nThe above model output is difficult to read. It will not be accepted by any journal or professor. Luckily, we can use modelsummary::modelsummary() to easily generate a professionally formatted table.\n\nmodelsummary(\n  m, \n  statistic = NULL,\n  coef_rename = c(\"log_gdp_per_cap\" = \"GDP per capita (logged)\"),\n  gof_map = \"nobs\"\n)\n\n\n\n\n\nModel 1\n\n\n\n\n(Intercept)\n32.807\n\n\nGDP per capita (logged)\n4.521\n\n\nNum.Obs.\n203\n\n\n\n\n\n\n\nNote that OLS regression, particularly linear regression, requires that you make a lot of important assumptions about the relationship between your two variables. These were discussed in detail in the lecture. For example, we assume that the best line to fit is straight. We also assume that the best way to generate and describe the relationship across all observations is to fit the line that minimizes the distance between itself and the observed values or dots.\n\n\n\n\n\n\nNote\n\n\n\n\n\nThere are other approaches to determining the “best” line. These include maximum likelihood estimation (discussed in detail in GVPT729) and Bayesian statistics. We won’t discuss these approaches in this class or in GVPT722. It’s worth noting here; however, that OLS regression requires a whole bunch of assumptions that may or may not be appropriate to your research question or theory. This class prepares you to grapple with those questions and appropriately use these tools in your own research.\n\n\n\n\n\n\nPrediction and performance\nOkay, so we now have a model that describes the relationship between our outcome of interest (health) and our independent variable of interest (wealth). We can use this to predict our outcome of interest for different values of our independent variable. For example, what do we predict to be the average life expectancy of a country with a GDP per capita of $20,000?\nbroom::tidy(m) makes this model object a lot easier (tidier) to work with.\n\ntidy(m)\n\n# A tibble: 2 × 5\n  term            estimate std.error statistic  p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)        32.8      1.76       18.7 7.55e-46\n2 log_gdp_per_cap     4.52     0.198      22.8 1.04e-57\n\n\nFirst, let’s pull out the estimated constant (or intercept or \\(\\beta_0\\)) for our calculations.\n\nm_res &lt;- tidy(m)\n\nbeta_0 &lt;- m_res |&gt; \n  filter(term == \"(Intercept)\") |&gt; \n  pull(estimate)\n\nbeta_0\n\n[1] 32.80653\n\n\nNext, let’s pull out the estimated coefficient for (logged) GDP per capita:\n\nbeta_1 &lt;- m_res |&gt; \n  filter(term == \"log_gdp_per_cap\") |&gt; \n  pull(estimate)\n\nbeta_1\n\n[1] 4.520775\n\n\nFinally, we can plug this in to our model:\n\\[\nlife Exp_x = \\beta_0 + \\beta_1 logGdpPerCap_x\n\\]\n\nlife_exp_20000 &lt;- beta_0 + beta_1 * log(20000)\nlife_exp_20000\n\n[1] 77.57797\n\n\nA country with a GDP per capita of $20,000 is predicted to have an average life expectancy of 78 years. Let’s take a look back at our data. Remember, these data describe what the World Bank actually observed for each country in 2016. How close is our predicted value to our observed values?\n\ncountries_10\n\n# A tibble: 3 × 8\n  iso2c iso3c country          region  date gdp_per_cap life_exp log_gdp_per_cap\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;            &lt;chr&gt;  &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;           &lt;dbl&gt;\n1 BH    BHR   Bahrain          Middl…  2016      22867.     79.6           10.0 \n2 KN    KNA   St. Kitts and N… Latin…  2016      21095.     71.7            9.96\n3 SI    SVN   Slovenia         Europ…  2016      21678.     81.2            9.98\n\n\nAs above, our data suggest that these countries have closer to an average of 77 years. Although our model predicted an average life expectancy closer to this than our guess above (which predicted 70 years), we still have a gap. Why?\nOur model is an attempt to formalize our understanding of the general relationship between a country’s wealth and health. Mapping our model against the observed values we used to generate it illustrates this point well.\n\nggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  geom_vline(xintercept = log(20000)) + \n  geom_hline(yintercept = life_exp_20000) + \n  geom_smooth(method = \"lm\", se = F) + \n  theme_minimal() + \n  labs(x = \"Logged GDP per capita\",\n       y = \"Average life expectancy (years)\") + \n  scale_x_continuous(labels = label_dollar())\n\n\n\n\nThe world is a complicated and messy place. There are many countries that have a GDP per capita of around $20,000 (those dots sitting around the vertical black line). They have a wide range of average life expectancy: look at their various placement along that vertical line. Some are higher than others.\nAlso, there are several countries with a wide range of logged GDP per capita that have an average life expectancy of 78 years (those sitting at or around the horizontal black line). These have a wide range of logged GDP per capita: some are further to the left than others.\nOur model is our best attempt at accounting for that diversity whilst still producing a useful summary of the relationship between health and wealth for those countries and all other countries with all observed values of GDP per capita.\nA bit of noise (error) is expected. How much error is okay? This is a complicated question that has contested answers. Let’s start with actually measuring that error. Then we can chat about whether or not it’s small enough to allow us to be confident in our model.\n\n\nMeasuring error in our model\nReturning to our question above, how close are our predicted values to our observed values? For example, how far from the observed average life expectancy of countries with a GDP per capita of or close to $20,000 is 78 years?\nStart by working out the average life expectancy predicted by our model for the logged GDP per capita of all of our countries. We can then compare this to the average life expectancy actually observed in all these countries. We can predict values from a model using broom::augment():\n\naugment(m)\n\n# A tibble: 203 × 9\n   .rownames life_exp log_gdp_per_cap .fitted .resid    .hat .sigma   .cooksd\n   &lt;chr&gt;        &lt;dbl&gt;           &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n 1 1             75.6           10.3     79.2 -3.55  0.0103    4.11 0.00393  \n 2 2             63.1            6.25    61.1  2.06  0.0193    4.12 0.00251  \n 3 3             61.1            7.44    66.5 -5.37  0.00883   4.10 0.00767  \n 4 4             78.9            8.32    70.4  8.42  0.00533   4.08 0.0113   \n 5 6             79.3           10.6     80.8 -1.49  0.0132    4.12 0.000895 \n 6 7             76.3            9.46    75.6  0.751 0.00612   4.12 0.000104 \n 7 8             74.7            8.21    69.9  4.74  0.00558   4.10 0.00375  \n 8 10            78.2            9.67    76.5  1.62  0.00695   4.12 0.000549 \n 9 11            82.4           10.8     81.7  0.740 0.0150    4.12 0.000250 \n10 12            81.6           10.7     81.3  0.367 0.0141    4.12 0.0000576\n# ℹ 193 more rows\n# ℹ 1 more variable: .std.resid &lt;dbl&gt;\n\n\nThis function is simply fitting our model (\\(life Exp_x = 32.9 + 4.5 * logGdpPerCap_x\\)) to each country’s logged GDP per capita. You can confirm this by running the model yourself:\n\ngapminder_df |&gt; \n  transmute(\n    country,\n    log_gdp_per_cap,\n    .fitted = beta_0 + beta_1*log_gdp_per_cap\n  )\n\n# A tibble: 217 × 3\n   country              log_gdp_per_cap .fitted\n   &lt;chr&gt;                          &lt;dbl&gt;   &lt;dbl&gt;\n 1 Aruba                          10.3     79.2\n 2 Afghanistan                     6.25    61.1\n 3 Angola                          7.44    66.5\n 4 Albania                         8.32    70.4\n 5 Andorra                        10.6     80.7\n 6 United Arab Emirates           10.6     80.8\n 7 Argentina                       9.46    75.6\n 8 Armenia                         8.21    69.9\n 9 American Samoa                  9.50    75.7\n10 Antigua and Barbuda             9.67    76.5\n# ℹ 207 more rows\n\n\nHow did the model do? What is the difference between what it predicted and the country’s observed average life expectancy? Compare .fitted (the predicted average life expectancy) to life_exp (the actual observed average life expectancy).\n\nm_eval &lt;- augment(m) |&gt; \n  transmute(\n    life_exp, \n    .fitted,\n    diff = life_exp - .fitted\n  )\n\nm_eval\n\n# A tibble: 203 × 3\n   life_exp .fitted   diff\n      &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1     75.6    79.2 -3.55 \n 2     63.1    61.1  2.06 \n 3     61.1    66.5 -5.37 \n 4     78.9    70.4  8.42 \n 5     79.3    80.8 -1.49 \n 6     76.3    75.6  0.751\n 7     74.7    69.9  4.74 \n 8     78.2    76.5  1.62 \n 9     82.4    81.7  0.740\n10     81.6    81.3  0.367\n# ℹ 193 more rows\n\n\nNote that broom::augment() already did this calculation and stored it in the .resid variable. The formal term for the difference between the predicted and observed values is the residual.\n\naugment(m) |&gt; \n  select(life_exp, .fitted, .resid)\n\n# A tibble: 203 × 3\n   life_exp .fitted .resid\n      &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1     75.6    79.2 -3.55 \n 2     63.1    61.1  2.06 \n 3     61.1    66.5 -5.37 \n 4     78.9    70.4  8.42 \n 5     79.3    80.8 -1.49 \n 6     76.3    75.6  0.751\n 7     74.7    69.9  4.74 \n 8     78.2    76.5  1.62 \n 9     82.4    81.7  0.740\n10     81.6    81.3  0.367\n# ℹ 193 more rows\n\n\nOkay, so there are some differences. Let’s look at those differences a bit more closely:\n\nggplot(augment(m), aes(x = .resid)) + \n  geom_density() + \n  geom_vline(xintercept = 0) + \n  theme_minimal()\n\n\n\n\nIf our model perfectly predicted each country’s life expectancy, we would see no difference between the predicted and observed values. There would just be a very tall straight line at 0 on the graph above.\nOur model hasn’t predicted life expectancy perfectly. Whilst most predictions are within a couple of years of the country’s true life expectancy, there are some that are very different (up to 10 or 15 years!). Where the model has got it wrong, it has tended to overestimate life expectancy (note that the peak of the density curve sits above 0).\nCan you see for which points these large differences exist?\n\nggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = F) + \n  theme_minimal() + \n  labs(x = \"Logged GDP per capita\",\n       y = \"Average life expectancy (years)\") + \n  scale_x_continuous(labels = label_dollar())\n\n\n\n\nWhat is causing these differences? A lot of your work as a political scientist is trying to answer this very question!\n\n(Random) error\nThe world is a messy and complicated place. Things often vary in random ways. That’s okay! It means that your observational data are going to move in funny and random ways. That’s okay too! As long as your model includes all of the systematic drivers of the thing you are interested in measuring (such as average life expectancy), we can accept a bit of random error.\nIn fact, we have already accounted for this. Remember that error term:\n\\[\nlife Exp_x = \\beta_0 + \\beta_1 logGdpPerCap_x + \\epsilon\n\\]\nWe run into issues when there are non-random things bundled up into the difference between what our model predicts and what we actually observe. We will discuss this more in later classes.\n\n\n\nA model-wide value for error\nWe often want to understand how the model has performed as a whole, rather than how well it predicts each individual observed data point. There are many different ways we can do this.\n\nSum of squared residuals (deviance)\nThe sum of squared residuals measures the total error in our model. Formally:\n\\[\n\\Sigma(y_i - \\hat{y_i})^2\n\\]\nWhere \\(y_i\\) is each observed value (the country’s actual average life expectancy) and \\(\\hat{y_i}\\) is each predicted value (the model’s estimate of country’s average life expectancy).\nWe just add those all up to get a single measure of the model’s overall performance.\n\n\n\n\n\n\nNote\n\n\n\nRemember that we tend to square things when we don’t care about the direction. We don’t care that the predicted value is less or more than the observed value, just about how far they are from each other.\n\n\nWe can do this ourselves:\n\naugment(m) |&gt; \n  summarise(sum(.resid^2))\n\n# A tibble: 1 × 1\n  `sum(.resid^2)`\n            &lt;dbl&gt;\n1           3392.\n\n\nOr we can use broom::glance():\n\nglance(m)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.722         0.720  4.11      521. 1.04e-57     1  -574. 1154. 1164.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\nglance(m) |&gt; \n  select(deviance)\n\n# A tibble: 1 × 1\n  deviance\n     &lt;dbl&gt;\n1    3392.\n\n\n\n\n\n\n\n\nTip\n\n\n\nWhere broom::tidy() gives us information about the coefficients of our model, broom::glance() gives us information on the overall model performance.\n\n\nThis is useful, but it is influenced by the units by which we measure our variables. If one model includes something like GDP which is measured in terms of billions of dollars, we will get a very large sum of squared residuals. If another model includes something like percentage of as state’s citizens who will vote for Donald Trump, we will get a relatively small sum of squared residuals. What if we want to compare model performance in a meaningful way?\n\n\n\\(R^2\\)\nThe \\(R^2\\) value measures the amount of variation in the dependent variable that is explained by the independent variable. In our example, it measures how much the changes in countries’ average life expectancy is explained by the changes in their (logged) GDP per capita.\n\\[\nR^2 = 1 - \\frac{Unexplained\\ variation}{Total\\ variation}\n\\]\nThe \\(R^2\\) value is useful because it does not reflect the units of measurement used in our variables. Therefore, we can compare how well different models perform.\nThe \\(R^2\\) value has three component parts.\n\nTotal Sum of Squares (TSS)\nTSS measures the squared sum of the differences between all predicted values of the dependent variable and the mean of the dependent variable.\n\n\nExplained Sum of Squares (ESS)\nESS measures the sum of the squares of the deviations of the predicted values from the mean value of the dependent variable.\n\n\nResidual Sum of Squares (RSS)\nRSS measures the difference between the TSS and ESS. In other words, the error not explained by the model.\nFormally, the \\(R^2\\) value is:\n\\[\nR^2 = 1 - \\frac{RSS}{TSS} = 1 - \\frac{\\Sigma(y_i - \\hat{y_i})^2}{\\Sigma(y_i - \\hat{y})^2}\n\\]\nOr:\n\\[\nR^2 = \\frac{ESS}{TSS} = \\frac{\\Sigma(\\hat{y}_i - \\bar{y})^2}{\\Sigma(y_i - \\bar{y})^2}\n\\]\nOur model’s \\(R^2\\) can be accessed using broom::glance():\n\nglance(m) |&gt; \n  select(r.squared)\n\n# A tibble: 1 × 1\n  r.squared\n      &lt;dbl&gt;\n1     0.722\n\n\nAn \\(R^2\\) of 1 means that all of the change in the dependent variable are completely explained by changes in the independent variable. Here, it would mean that all changes to a country’s average life expectancy are explained through changes to the country’s logged GDP per capita.\nAccording to our model, 72.2% of changes in a country’s average life expectancy are explained through changes to the country’s logged GDP per capita.\n\n\n\n\nModelling relationships among categorical variables\nSometimes we want to know whether our outcome of interest changes based on the category in which it sits. For example, do levels of support for abortion access differ between Democrats, Republicans, and Independents? Do the number of women elected to parliament change based on whether or not the country has a formal quota? Do the number of civilians targeted in war change based on whether the war is intra- or inter-state?\nLet’s return to the American National Election Survey we first explored last week. We will focus on that first question: do levels of support for abortion access differ between Democrats, Republicans, and Independents?\nWe can access the 2012 survey through R using the poliscidata package:\npoliscidata::nes\n\nCross tabs\nA simple cross tab can provide a nice summary of differences in your outcome of interest across your categories.\nFor example, let’s look at differences in the number of individuals who identified as Democrat, Republican, or Independent who do not support access to abortions, support access with some conditions, with more conditions, or always.\nWe can use modelsummary::datasummary_crosstab() to produce a nicely formatted cross tab of our variables:\n\ndatasummary_crosstab(abort4 ~ pid_3, data = nes)\n\n\n\n\nabort4\n\nDem\nInd\nRep\nAll\n\n\n\n\nNever\nN\n187\n229\n252\n672\n\n\n\n% row\n27.8\n34.1\n37.5\n100.0\n\n\nSome conds\nN\n499\n583\n519\n1607\n\n\n\n% row\n31.1\n36.3\n32.3\n100.0\n\n\nMore conds\nN\n332\n337\n227\n898\n\n\n\n% row\n37.0\n37.5\n25.3\n100.0\n\n\nAlways\nN\n1325\n964\n381\n2680\n\n\n\n% row\n49.4\n36.0\n14.2\n100.0\n\n\nAll\nN\n2358\n2149\n1385\n5916\n\n\n\n% row\n39.9\n36.3\n23.4\n100.0\n\n\n\n\n\n\n\nWe can also visualise this:\n\nnes |&gt; \n  count(pid_3, abort4) |&gt; \n  drop_na(pid_3) |&gt; \n  ggplot(aes(x = n, y = pid_3, fill = abort4)) + \n  geom_bar(position = \"dodge\", stat = \"identity\") + \n  theme_minimal() + \n  labs(x = \"N\",\n       y = NULL,\n       fill = \"Level of support\") + \n  scale_fill_manual(values = c(\"#EDE5CF\",\"#E0C2A2\",\"#D39C83\",\"#C1766F\"))\n\n\n\n\nOr this:\n\nnes |&gt; \n  count(pid_3, abort4) |&gt; \n  drop_na(pid_3) |&gt; \n  ggplot(aes(x = n, y = abort4, fill = pid_3)) + \n  geom_bar(position = \"dodge\", stat = \"identity\") + \n  theme_minimal() + \n  labs(x = \"N\",\n       y = NULL,\n       fill = \"Party\") + \n  scale_fill_manual(values = c(\"#1375B7\",\"lightgrey\",\"#C93135\"))\n\n\n\n\n\n\nMean comparison table\nWe can use mean comparison tables to, well, compare means (in a table). Let’s compare the average response to the feeling thermometer (scale from 0 to 100) for the Republican party across parties:\n\nnes |&gt;\n  group_by(pid_3) |&gt; \n  summarise(\n    mean = mean(ft_rep, na.rm = T),\n    sd = sd(ft_rep, na.rm = T),\n    freq = n()\n  )\n\n# A tibble: 4 × 4\n  pid_3  mean    sd  freq\n  &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 Dem    24.5  21.8  2358\n2 Ind    43.0  23.1  2149\n3 Rep    70.3  18.3  1385\n4 &lt;NA&gt;   50    14.7    24\n\n\nAre these counts or averages meaningfully different from one another? We need some additional tools to answer that question. We will discuss those in the coming weeks.\n\n\nLooking at the whole distribution\nWe can also visualize the whole distribution of a continuous variable of interest within our categories.\n\nnes |&gt;\n  drop_na(pid_3) |&gt; \n  ggplot(aes(x = ft_rep, fill = pid_3)) + \n  geom_density(alpha = 0.5) + \n  theme_minimal() + \n  scale_fill_manual(values = c(\"#1375B7\",\"lightgrey\",\"#C93135\")) + \n  labs(x = \"Feeling thermometer\",\n       y = \"Density\", \n       fill = \"Party\")\n\n\n\n\nAs expected, Democrats appear to respond least favourably to the Republican Party, followed by Independents, and Republicans. This is demonstrated by both the mean comparison table and density plots.\n\n\n\nNext week\nWe will discuss research design."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\n            Quantitative Methods for Political Science\n        ",
    "section": "",
    "text": "Quantitative Methods for Political Science\n        \n        \n            An introduction to research methods and quantitative research in political science.\n        \n        \n            Fall 2023Department of Government and PoliticsUniversity of Maryland, College Park\n        \n    \n    \n      \n        \n        \n        \n      \n    \n\n\n\n\n\n\nProfessor\n\n   Dr David Cunningham\n   dacunnin@umd.edu\n\n\n\nTeaching Assistant\n\n   Harriet Goers\n   Chincoteague Building\n   hgoers@umd.edu\n   hgoers\n\n\n\n\n\nCourse details\n\n   August 28 - 11 December\n   Monday, 12:30 - 3:15 PM\n   Tydings Building, Room 1111\n\n\n\nLab details\n\n   Friday, 3:00 - 5:00 PM\n   TYD1111\n\n\n\nOffice hours\n\n   Wednesday, 10:00 - 11:00 AM\n   Zoom\n\n\n\n\nContacting me\nE-mail is the best ways to get in contact with me. I will try to respond to all course-related e-mails within 24 hours."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "DATE\nTITLE\nCLASS NOTES\nSESSION SCRIPTS\nASSIGNMENTS\n\n\n\n\n2023-08-28\nCourse Introduction\n\n\n\n\n\n2023-09-04\nNo class\n\n\n\n\n\n2023-09-11\nDescriptive Statistics\n\n\n\n\n\n2023-09-18\nRelationships Between Two Variables\n\n\n\n\n\n2023-09-25\nResearch Design\n\n\n\n\n\n2023-10-02\nApplications & Midterm Exam Review\n\n\n\n\n\n2023-10-09\nProbability Theory\n\n\n\n\n\n2023-10-16\nHypothesis Testing I\n\n\n\n\n\n2023-10-23\nHypothesis Testing II\n\n\n\n\n\n2023-10-30\nHypothesis Testing III\n\n\n\n\n\n2023-11-06\nApplications & Midterm Exam Review\n\n\n\n\n\n2023-11-13\nRegression Analysis I\n\n\n\n\n\n2023-11-20\nNo class\n\n\n\n\n\n2023-11-27\nRegression Analysis II\n\n\n\n\n\n2023-12-04\nRegression Analysis Extensions & Final Exam Review"
  }
]