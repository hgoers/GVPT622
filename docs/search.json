[
  {
    "objectID": "content/06-probability_theory.html",
    "href": "content/06-probability_theory.html",
    "title": "Probability Theory",
    "section": "",
    "text": "Pollock & Edwards, Chapter 5\n\n\n\n Pollock & Edwards R Companion, Chapter 6"
  },
  {
    "objectID": "content/06-probability_theory.html#readings",
    "href": "content/06-probability_theory.html#readings",
    "title": "Probability Theory",
    "section": "",
    "text": "Pollock & Edwards, Chapter 5\n\n\n\n Pollock & Edwards R Companion, Chapter 6"
  },
  {
    "objectID": "content/06-probability_theory.html#class-slides",
    "href": "content/06-probability_theory.html#class-slides",
    "title": "Probability Theory",
    "section": "Class slides",
    "text": "Class slides"
  },
  {
    "objectID": "content/06-probability_theory.html#section",
    "href": "content/06-probability_theory.html#section",
    "title": "Probability Theory",
    "section": "Section",
    "text": "Section\n\nPrerequisites\n\nlibrary(tidyverse)\nlibrary(janitor)\n\nset.seed(1234)\n\n\nToday, we are working with randomness and chance. To make sure we are all working with the same randomness and chance, you need to set your seed! set.seed() sets the random number generator state in which R will operate for this session. The draws we make in the session are still random, but they will be the same random draw each time we take it. This is important for replication, so you will use it outside of class.\n\n\n\nRandomness and avoiding doing the dishes\nImagine you and a friend are trying to decide who will do the dishes after you have both cooked a very large and very messy meal. You agree to flip a coin. Should you pick heads or tails?\nYou really don’t want to do the dishes. Therefore, you want to maximize your chances of winning the coin flip. If you could predict the outcome of the coin flip with certainty, you would simply pick the winning side. Even if you don’t know for certain which side will land on top, you want to pick the side that has the highest chance of winning. How can you work this out?\nFirst, you need to work out all the possible outcomes. This task is simple for a coin flip: heads or tails.\n\npossible_outcomes &lt;- c(\"HEADS\", \"TAILS\")\npossible_outcomes\n\n[1] \"HEADS\" \"TAILS\"\n\n\nThen, you need to work out how likely each of those outcomes are to eventuate. How can we do this? One option available to us here is repeated trials. Flip your coin many times and record how many heads and tails you get. This provides you with a rough understanding of the chance that your coin will land on heads or tails for any given flip.\nFor example, you can flip the coin 10 times and record the results of each flip.\n\nrepeat_trials &lt;- sample(possible_outcomes, size = 10, replace = T, prob = c(0.5, 0.5))\nrepeat_trials\n\n [1] \"TAILS\" \"HEADS\" \"HEADS\" \"HEADS\" \"HEADS\" \"HEADS\" \"TAILS\" \"TAILS\" \"HEADS\"\n[10] \"HEADS\"\n\n\n\nWe want to sample with replacement, so we include the argument replace = T. This just means that we include all possible outcomes in every draw. If we sampled without replacement, we would remove each outcome from the sample after it has been selected in a previous draw.\nFor example, imagine you have 10 different colored marbles in a bag. You pull out a marble and record its color. If you want to sample with replacement, you put the marble you just pulled out back into the bag before you take your next draw. This means that you can draw that same marble out again in the subsequent draws.\n\nYou can then tally up those results to get your baseline understanding of the chances of heads vs tails.\n\ntable(repeat_trials)\n\nrepeat_trials\nHEADS TAILS \n    7     3 \n\n\nGiven the results of this trial, I would expect that for every 10 coin flips, I should get 7 heads and 3 tails. If I want to use this information to determine the chance that the coin will land on its head after one flip, I can convert this to percentages. There is a 70% chance that the coin will land on heads. Therefore, based solely on the 10 flip trial, you should pick heads.\nHmm… but aren’t fair coins meant to land on heads or tails with equal probability? In fact, if you look back to our sample (drawn using the sample() function) you will see that I explicitly set the probability of landing on heads and tails to be an even 0.5 and 0.5 each (using the prob argument). Why then are we getting 70% for heads and 30% for tails instead of 50% and 50%?\nTo answer this question, we need to build up some foundations in probability theory. Let’s start with independence.\n\n\nIndependence\nYou want to maximize your chances of not doing the dishes (i.e. of picking the winning side of the coin). To do this, you need to know all possible outcomes (heads and tails) and the probability that each of those outcomes will eventuate. To learn this, you ran a trial in which you flipped the coin 10 times and recorded the outcome of each flip. How can you trust that this trial is revealing the true underlying probabilities of heads vs. tails?\nEach time you flipped that coin, you undertook the very process you will eventually take to decide who has to do the dishes. You will only flip that deciding coin once, so you need to know what the chances are that the coin will land on heads or tails that one time. You can’t ever observe that. If you flip a coin once, you will either see heads or tails. But we know that if we flip it again we might get a different outcome. In other words, if you flip a coin once and it lands on heads, this does not necessarily mean that the probability of heads is 1 and the probability of tails is 0 (or that you will always get heads). We use trials to try to estimate the unobservable underlying probabilities of each possible outcome of a single coin flip.\nTo make sure that we can infer from our observed flips the underlying and unobservable probability of heads vs. tails of one coin flip, we need to make sure that our trials meet certain conditions. The first is independence: the outcome of any other flips cannot impact the outcome of the current flip.\nFor example, let’s go back to our bag of 10 different marbles. Say there are 2 red, 3 blue, and 5 green marbles in your bag. You want to know the probability of drawing out a green marble. You pull out a marble. We know that there is a 20% chance your marble will be red, a 30% chance it will be blue, and a 50% chance it will be green. It is green. You then do not replace the marble before your next draw. Now, there is a 22% chance that marble will be red, a 33% chance it is blue, and a 44% chance it will be green (there are now only nine marbles in your bag: 2 red, 3, blue, and 4 green). These draws are not independent of each other! Your first draw changed the underlying probability of drawing a green marble in your second draw.\nIf your draws are independent of one another, you can infer from the results of the trial the underlying probability of each outcome eventuating.\nBut hold on: we did that and we still got uneven results!\n\ntable(repeat_trials)\n\nrepeat_trials\nHEADS TAILS \n    7     3 \n\n\nWhy?\n\n\nThe law of large numbers\nIn short, our trial was too small.\nEven if our underlying probability is {0.5, 0.5} (which it is: remember that prob = c(0.5, 0.5) argument), we may observe a set of outcomes in our trial that do not reflect this true distribution.\nTo illustrate, think of the outcome you could observe from only one draw: heads or tails. If you draw heads and then use that trial to infer the underlying probability of drawing heads vs. tails, you will state that the underlying probability of drawing a head and tail is equal to {1, 0}. You will be very surprised if you subsequently flip a tail.\nNow, what if you run a trial of two flips?\n\nsample(possible_outcomes, 2, replace = T, prob = c(0.5, 0.5))\n\n[1] \"HEADS\" \"HEADS\"\n\n\nBoth heads!\nIn fact, if we flip a coin twice many times (say, 10 times), we will probably get a couple of trials in which we flip two heads or two tails:\n\ntrial_two_flips &lt;- tibble(trial = 1:10) |&gt; \n  rowwise() |&gt; \n  mutate(outcome = list(sample(possible_outcomes, 2, replace = T, prob = c(0.5, 0.5)))) |&gt; \n  unnest_wider(outcome) |&gt; \n  rename(\"flip_1\" = `...1`, \"flip_2\" = `...2`)\n\ntrial_two_flips\n\n# A tibble: 10 × 3\n   trial flip_1 flip_2\n   &lt;int&gt; &lt;chr&gt;  &lt;chr&gt; \n 1     1 TAILS  HEADS \n 2     2 TAILS  HEADS \n 3     3 TAILS  TAILS \n 4     4 TAILS  TAILS \n 5     5 TAILS  TAILS \n 6     6 TAILS  TAILS \n 7     7 TAILS  HEADS \n 8     8 HEADS  HEADS \n 9     9 HEADS  TAILS \n10    10 TAILS  TAILS \n\n\n6 or 60% of our 10 trials resulted in two of the same outcomes. How can we be confident that our trials are good reflections of the actual distribution of probabilities?\nThe law of large numbers suggests that when your population of independent observations has a finite mean, as the number of observations drawn increases, the mean of the observed values in the sample approaches the mean of the population.\nIn other words, the more flips you do, the closer you will get to the true underlying distribution of probabilities. Cool!\nLet’s try this out.\nFirst, let’s flip the coin 10 times:\n\nThe red dots are sitting at {0.5,0.5}. We are aiming for this true probability distribution.\n\n\ntibble(outcome = sample(possible_outcomes, 10, replace = T, prob = c(0.5, 0.5))) |&gt; \n  tabyl(outcome) |&gt; \n  ggplot(aes(y = outcome, x = percent)) + \n  geom_col() + \n  geom_point(aes(x = c(0.5, 0.5)), size = 3, colour = \"red\") + \n  theme_minimal() + \n  scale_x_continuous(limits = c(0, 1))\n\n\n\n\nNow, let’s flip it 100 times:\n\ntibble(outcome = sample(possible_outcomes, 100, replace = T, prob = c(0.5, 0.5))) |&gt; \n  tabyl(outcome) |&gt; \n  ggplot(aes(y = outcome, x = percent)) + \n  geom_col() + \n  geom_point(aes(x = c(0.5, 0.5)), size = 3, colour = \"red\") + \n  theme_minimal() + \n  scale_x_continuous(limits = c(0, 1))\n\n\n\n\nNow, let’s flip it 1,000 times:\n\ntibble(outcome = sample(possible_outcomes, 1000, replace = T, prob = c(0.5, 0.5))) |&gt; \n  tabyl(outcome) |&gt; \n  ggplot(aes(y = outcome, x = percent)) + \n  geom_col() + \n  geom_point(aes(x = c(0.5, 0.5)), size = 3, colour = \"red\") + \n  theme_minimal() + \n  scale_x_continuous(limits = c(0, 1))\n\n\n\n\nNow, let’s flip it 10,000 times:\n\ntibble(outcome = sample(possible_outcomes, 10000, replace = T, prob = c(0.5, 0.5))) |&gt; \n  tabyl(outcome) |&gt; \n  ggplot(aes(y = outcome, x = percent)) + \n  geom_col() + \n  geom_point(aes(x = c(0.5, 0.5)), size = 3, colour = \"red\") + \n  theme_minimal() + \n  scale_x_continuous(limits = c(0, 1))\n\n\n\n\nFinally, let’s flip it 100,000 times:\n\ntibble(outcome = sample(possible_outcomes, 100000, replace = T, prob = c(0.5, 0.5))) |&gt; \n  tabyl(outcome) |&gt; \n  ggplot(aes(y = outcome, x = percent)) + \n  geom_col() + \n  geom_point(aes(x = c(0.5, 0.5)), size = 3, colour = \"red\") + \n  theme_minimal() + \n  scale_x_continuous(limits = c(0, 1))\n\n\n\n\nEach time we increase the number of draws we make, we get closer to the true underlying distribution of probabilities (as coded in our sample() function).\nSo, after all of that how can you avoid doing the dishes? Sadly (and as expected) you cannot get an edge on your friend. All possible outcomes have the same probability of eventuating.\n\n\nWhat does this all have to do with political science?\nFair question.\nQuantitative social science involves statistical inference. We start with a parameter of interest. For example, what proportion of US voters approve of Joe Biden’s job as president? It would be very nice if we could go and ask all US voters what they think and if we could be confident that they are giving us their true opinions. However, this is simply not possible (even the census misses some people!). Instead, we rely on surveys.\nThese surveys (if done well) will take a representative sample of the population of US voters and ask their opinion of Biden. We can then use that sample to infer the overall level of support for Joe Biden among the population (of US voters). Think of a survey as a trial.\nWe can even use this sample to answer interesting questions about groups within the population. What do Republicans think about Joe Biden’s job as president? What about women? Or people of color?\nThis is all statistical inference. Probability theory undergrids our ability to observe or measure variables of interest and use these variables to strengthen our arguments in support of our theory. We will discuss this in more detail next week."
  },
  {
    "objectID": "content/08_hypothesis_testing.html",
    "href": "content/08_hypothesis_testing.html",
    "title": "Hypothesis Testing II",
    "section": "",
    "text": "Pollock & Edwards, Chapter 7\n\n\n\n Pollock & Edwards R Companion, Chapter 7"
  },
  {
    "objectID": "content/08_hypothesis_testing.html#readings",
    "href": "content/08_hypothesis_testing.html#readings",
    "title": "Hypothesis Testing II",
    "section": "",
    "text": "Pollock & Edwards, Chapter 7\n\n\n\n Pollock & Edwards R Companion, Chapter 7"
  },
  {
    "objectID": "content/08_hypothesis_testing.html#class-slides",
    "href": "content/08_hypothesis_testing.html#class-slides",
    "title": "Hypothesis Testing II",
    "section": "Class slides",
    "text": "Class slides"
  },
  {
    "objectID": "content/08_hypothesis_testing.html#section",
    "href": "content/08_hypothesis_testing.html#section",
    "title": "Hypothesis Testing II",
    "section": "Section",
    "text": "Section\n\nPrerequisites\n\nlibrary(tidyverse)\nlibrary(wbstats)\n\n\nregion_gdp_df &lt;- wb_data(\n  \"NY.GDP.MKTP.CD\",\n        start_date = 2021,\n        end_date = 2021,\n        return_wide = F\n) |&gt; \n  transmute(\n    country,\n            region = countrycode::countrycode(iso3c, \"iso3c\", \"region\"),\n            gdp = value\n  )\n\nregion_gdp_df\n\n# A tibble: 217 × 3\n   country             region                               gdp\n   &lt;chr&gt;               &lt;chr&gt;                              &lt;dbl&gt;\n 1 Afghanistan         South Asia                  14583135237.\n 2 Albania             Europe & Central Asia       17930565119.\n 3 Algeria             Middle East & North Africa 163472233246.\n 4 American Samoa      East Asia & Pacific           709000000 \n 5 Andorra             Europe & Central Asia        3325145407.\n 6 Angola              Sub-Saharan Africa          65685435100.\n 7 Antigua and Barbuda Latin America & Caribbean    1560518519.\n 8 Argentina           Latin America & Caribbean  487227125386.\n 9 Armenia             Europe & Central Asia       13861409969.\n10 Aruba               Latin America & Caribbean    3126019385.\n# ℹ 207 more rows\n\n\n\nggplot(region_gdp_df, aes(x = gdp, y = region, fill = region)) + \n  geom_boxplot()"
  },
  {
    "objectID": "content/11-regression.html",
    "href": "content/11-regression.html",
    "title": "Regression I",
    "section": "",
    "text": "Pollock & Edwards, Chapter 8\n\n\n\n Pollock & Edwards R Companion, Chapter 8"
  },
  {
    "objectID": "content/11-regression.html#readings",
    "href": "content/11-regression.html#readings",
    "title": "Regression I",
    "section": "",
    "text": "Pollock & Edwards, Chapter 8\n\n\n\n Pollock & Edwards R Companion, Chapter 8"
  },
  {
    "objectID": "content/11-regression.html#class-slides",
    "href": "content/11-regression.html#class-slides",
    "title": "Regression I",
    "section": "Class slides",
    "text": "Class slides"
  },
  {
    "objectID": "content/11-regression.html#section",
    "href": "content/11-regression.html#section",
    "title": "Regression I",
    "section": "Section",
    "text": "Section"
  },
  {
    "objectID": "content/02-descriptive_statistics.html",
    "href": "content/02-descriptive_statistics.html",
    "title": "Descriptive Statistics",
    "section": "",
    "text": "Pollock & Edwards, Chapters 1-2\n\n\n\n Pollock & Edwards R Companion, Chapters 2-3"
  },
  {
    "objectID": "content/02-descriptive_statistics.html#readings",
    "href": "content/02-descriptive_statistics.html#readings",
    "title": "Descriptive Statistics",
    "section": "",
    "text": "Pollock & Edwards, Chapters 1-2\n\n\n\n Pollock & Edwards R Companion, Chapters 2-3"
  },
  {
    "objectID": "content/02-descriptive_statistics.html#class-slides",
    "href": "content/02-descriptive_statistics.html#class-slides",
    "title": "Descriptive Statistics",
    "section": "Class slides",
    "text": "Class slides"
  },
  {
    "objectID": "content/02-descriptive_statistics.html#section",
    "href": "content/02-descriptive_statistics.html#section",
    "title": "Descriptive Statistics",
    "section": "Section",
    "text": "Section\n\nPrerequisites\n\nlibrary(tidyverse)\nlibrary(poliscidata)\nlibrary(wbstats)\nlibrary(janitor)\nlibrary(skimr)\nlibrary(countrycode)\nlibrary(ggridges)\nlibrary(ggdist)\nlibrary(MetBrewer)\nlibrary(patchwork)\n\n\n\nHow will what you learn this week help your research?\nYou have an interesting question that you want to explore. You have some data that relate to that question. Included in these data are information on your outcome of interest and information on the things that you think determine or shape that outcome. You think that one (or more) of the drivers is particularly important, but no one has yet written about it or proven its importance. Brilliant! What do you do now?\nThe first step in any empirical analysis is getting to know your data. I mean, really getting to know your data. You want to dig into it with a critical eye. You want to understand any patterns lurking beneath the surface.\nUltimately, you want to get a really good understanding of the data generation process. This process can be thought of in two different and important ways. First, you want to understand how, out there in the real world, your outcome and drivers come to be. For example, if you are interested in political voting patterns, you want to know the nitty gritty process of how people actually vote. Do they have to travel long distances, stand in long queues, fill out a lot of paperwork? Are there age restrictions on their ability to vote? Are there more insidious restrictions that might suppress voting for one particular group in the electorate?\nYou can use the skills we will discuss this week to help you answer these questions. For example, you can determine whether there are relatively few young voters compared to older voters. Then you can explore why this might be. In turn, your growing expertise in and understanding of the data generation process should inform your exploration of the data. You might note that people have to wait in long queues on a Tuesday to vote. Does this impact the number of workers vs. retirees who vote?\nNow, this is made slightly more tricky by the second part of your exploration. You need to understand how your variables are actually measured. How do we know who turns out to vote? Did you get access to the voter file, which records each individual who voted and some interesting and potentially relevant demographic information about them? Or are you relying on exit polls, that only include a portion of those who voted? Were the people included in the polls reflective of the total voting population? What or whom is missing from this survey? Of course, if your sample is not representative, you might find some patterns that appear to be very important to your outcome of interest but are, in fact, just an artifact of a poorly drawn sample.\nThis week you will be introduced to the first part of this process: data exploration. We use descriptive statistics to describe patterns in our data. These are incredibly powerful tools that will arm you with an intimate knowledge of the shape of your variables of interest. With this knowledge, you will be able to start to answer your important question and potentially identify new ones. You will also be able to sense-check your more complex models and pick up on odd or incorrect relationships that they may find.\nAs you make your frequency tables and histograms and very elaborate dot plots and box charts, keep in mind that these tools are useful for your interrogation of the data generation process. Be critical. Continue to ask whether your data allow you to detect true relationships between your variables of interest. Build your intuition for what is really going on and what factors are really driving your outcome of interest.\nLet’s get started.\n\n\nDescribing your data\nDescriptive statistics can be used to understand single variables and how any number of variables of interest relate to one another. The level of complexity involved in understanding these data and their relationships tend to increase with the number of variables you include. Today, we will look at single variables (for example, the age of respondents to a survey or how much each country spends on education in a year). Next week we will explore the relationship between two variables (for example, an individual’s party affiliation and their level of support for abortion access).\nYour variables are defined in terms of a unit of observation or analysis. These could include individuals, households, congressional districts, states, or countries. The unit of analysis you adopt should be relevant to your theory. For example, if you are interested in understanding which individuals are more susceptable to the different strategies rebel groups use to recruit individuals to fight, you probably want individual-level data. If you are interested in determining what drives countries to war with one another, you probably want country-level data (or leader-level data, or voter-level data?).\nThe important take-away here is that you should start with your theory. You should build a data set that reflects your theory.\n\n\nDifferent types of variables\nThe ways of describing your data depend on the type of variable. You can have categorical or continuous variables.\nCategorical variables are discrete. They can be unordered (nominal) - for example, the colour of cars - or ordered (ordinal) - for example, whether you strongly dislike, dislike, are neutral about, like, or strongly like Taylor Swift.\n\nDichotomous (or binary) variables are a special type of categorical variable. They take on one of two values. For example: yes or no; at war or not at war; is a Swifty, or is not a Swifty.\n\nContinuous variables are, well, continuous. For example, your height or weight, a country’s GDP or population, or the number of fatalities in a battle.\n\nContinuous variables can be made into (usually ordered) categorical variables. This process is called binning. For example, you can take individuals’ ages and reduce them to 0 - 18 years old, 18 - 45 years old, 45 - 65 years old, and 65+ years old.\nYou lose information in this process: you cannot go from 45 - 65 years old back to the individuals’ precise age. In other words, you cannot go from a categorical to continuous variable.\n\nLet’s take a look at how you can describe these different types of variables in turn, using real-world political science examples.\n\n\nDescribing categorical variables\nSimply put, for categorical variables we are interested in the count and/or percentage of cases that fall into each category.\n\nLater, we will ask interesting questions using these summaries. These include whether differences between the counts and/or percentages of cases that fall into each category are meaningfully (and/or statistically significantly) different from one another. This deceptively simple question serves as the foundation for a lot of political science (particularly comparative) research.\n\n\nCase study: American National Election Survey\nFor this section, we will be working with the American National Election Survey to explore how to produce useful descriptive statistics for categorical variables using R. The ANES polls annually individual Americans about their political beliefs and behavior.\nWe can access the 2012 survey using the poliscidata package:\n\npoliscidata::nes\n\n\nTake a look at the many different bits of information collected about each respondent using ?nes.\n\nLet’s look at how many individuals of different ages took part in the survey in 2012. The survey records the age of each respondent within six different brackets and reports that information in the dem_age6 variable.\n\n\nFrequency distribution\nWe can take advantage of janitor::tabyl() to quickly calculate the number and proportion of respondents in each age bracket.\n\ntabyl(nes, dem_age6)\n\n dem_age6    n    percent valid_percent\n    17-29  936 0.15821501     0.1598634\n    30-39  862 0.14570656     0.1472246\n    40-49  948 0.16024341     0.1619129\n    50-59 1312 0.22177147     0.2240820\n    60-69 1105 0.18678161     0.1887276\n 70-older  692 0.11697093     0.1181896\n     &lt;NA&gt;   61 0.01031102            NA\n\n\n\nNote: valid_percent provides the proportion of respondents in each age bracket with missing values removed from the denominator. For example, the NES survey had 5,916 respondents in 2012, but only 5,855 of them provided their age. 936 responded that they are 17 - 29 years old. Therefore, the 17-29 proportion (which is bounded by 0 and 1, whereas percents are bounded by 0 and 100) is 936 / 5,916 and its valid proportion is 936 / 5,855.\n\n\n\nVisualizing this frequency\nIt is a bit difficult to quickly determine relative counts. Which age bracket has the most respondents? Which has the least? Are these counts very different from each other.\nI highly recommend visualizing your data. You will get a much better sense of it. We can easily visualize this frequency table. I recommend using a bar chart to show clearly relative counts.\n\nnes |&gt; \n  tabyl(dem_age6) |&gt; \n  ggplot(aes(x = n, y = dem_age6)) + \n  geom_col() +\n  theme_minimal() + \n  labs(\n    x = \"Count of respondents\",\n    y = \"Age bracket\"\n  )\n\n\n\n\n\n\n\nDescribing continuous variables\nWe need to treat continuous variables differently from categorical ones because they cannot be meaningfully bound together and compared. For example, imagine making a frequency table or bar chart that counts the number of countries with each observed GDP. You would have 193 different counts of one. Not very helpful!\nWe can get a much better sense of our continuous variables by looking at characteristics of the distribution of these variables across the range of all possible values they could take on. Phew! Let’s make sense of this using some real-world data.\n\nCase study: Comparing countries’ spending on education\nFor this section, we will look at each country’s spending on education as a percent of their gross domestic product. We will use wbstats::wb_data() to collect these data.\n\nperc_edu &lt;- wb_data(\n  \"SE.XPD.TOTL.GD.ZS\", start_date = 2020, end_date = 2020, return_wide = F\n) |&gt; \n  transmute(\n    country, \n    region = countrycode(country, \"country.name\", \"region\"),\n    year = date, \n    value = value / 100\n  )\n\nperc_edu\n\n# A tibble: 217 × 4\n   country             region                      year   value\n   &lt;chr&gt;               &lt;chr&gt;                      &lt;dbl&gt;   &lt;dbl&gt;\n 1 Afghanistan         South Asia                  2020  0.0286\n 2 Albania             Europe & Central Asia       2020  0.0310\n 3 Algeria             Middle East & North Africa  2020  0.0704\n 4 American Samoa      East Asia & Pacific         2020 NA     \n 5 Andorra             Europe & Central Asia       2020 NA     \n 6 Angola              Sub-Saharan Africa          2020  0.0242\n 7 Antigua and Barbuda Latin America & Caribbean   2020  0.0345\n 8 Argentina           Latin America & Caribbean   2020  0.0502\n 9 Armenia             Europe & Central Asia       2020  0.0271\n10 Aruba               Latin America & Caribbean   2020 NA     \n# ℹ 207 more rows\n\n\nI have converted these percentages (0 - 100) to proportions (0 - 1) for ease of interpretation. I have also added each country’s region (using countrycode::countrycode()) so that we can explore regional trends in our data.\nWe can get a good sense of how expenditure varied by country by looking at the center, spread, and shape of the distribution.\n\n\nHistogram\n\nggplot(perc_edu, aes(x = value)) + \n  geom_histogram() + \n  theme_minimal() + \n  labs(\n    x = \"Expenditure on education as a proportion of GDP\",\n    y = \"Count\"\n  ) + \n  scale_x_continuous(labels = label_percent())\n\n\n\n\n\nTake a look at ?geom_histogram to find the arguments needed to change the bin width of your histograms.\n\n\n\nDensity curves\n\nggplot(perc_edu, aes(x = value)) + \n  geom_density() + \n  theme_minimal() + \n  labs(\n    x = \"Expenditure on education as a proportion of GDP\",\n    y = \"Count\"\n  ) + \n  scale_x_continuous(labels = label_percent())\n\n\n\n\n\n\nUnderstanding distributions\nBecause continuous variables are best described using their distribution, we can use the shape of that distribution to better understand our individual variables and compare them to others. Is the distribution symmetric or skewed? Where are the majority of observations clustered? Are there multiple distinct clusters, or high points, in the distribution?\n\nNormal distribution\n\ntibble(x = rnorm(n = 1e6)) |&gt; \n  ggplot(aes(x = x)) + \n  geom_histogram() + \n  theme_minimal()\n\n\n\n\n\n\nRight skewed distribution\n\ntibble(x = rbeta(1e6, 1, 10)) |&gt; \n  ggplot(aes(x = x)) + \n  geom_histogram() + \n  theme_minimal()\n\n\n\n\n\n\nLeft skewed distribution\n\ntibble(x = rbeta(1e6, 10, 1)) |&gt; \n  ggplot(aes(x = x)) + \n  geom_histogram() + \n  theme_minimal()\n\n\n\n\n\n\n\nMeasures of central tendency: mean, median, and mode\nWe can also use measures of central tendency to quickly describe and compare our variables.\n\nMean\nThe mean is the average of all values:\n\\[\n\\bar{x} = \\frac{\\Sigma x_i}{n}\n\\]\nIn other words, add all of your values together and then divide that total by the number of values you have.\nIn R:\n\nmean(perc_edu$value, na.rm = T)\n\n[1] 0.04639309\n\n\n\nNote: if you do not use the argument na.rm (read NA remove!), you will get an NA if any exist in your vector of values. This is a good default! You should be very aware of missing data points.\n\n\n\nMedian\nThe median is the mid-point of all values.\nTo calculate it, put all of your values in order from smallest to largest. Identify the value in the middle. That’s your median.\nIn R:\n\nmedian(perc_edu$value, na.rm = T)\n\n[1] 0.04461425\n\n\n\n\nMode\nThe mode is the most frequent of all values.\nTo calculate it, count how many times each value occurs in your data set. The one that occurs the most is your mode.\n\nThis is usually a more useful summary statistic for categorical variables than continuous ones. For example, which colour of car is most popular? Which political party has the most members?\n\nIn R:\n\nx &lt;- c(1, 1, 2, 4, 5, 32, 5, 1, 10, 3, 4, 6, 10)\n\ntable(x)\n\nx\n 1  2  3  4  5  6 10 32 \n 3  1  1  2  2  1  2  1 \n\n\n\n\nUsing central tendency to describe and understand distributions\nNormally distributed values have the same mean and median.\n\nnorm_dist &lt;- tibble(x = rnorm(n = 1e6))\n\nggplot(norm_dist, aes(x = x)) + \n  geom_histogram() + \n  geom_vline(xintercept = mean(norm_dist$x), colour = \"#FB8F86\", size = 2) + \n  geom_vline(xintercept = median(norm_dist$x), colour = \"#819FE3\", size = 2) + \n  theme_minimal()\n\n\n\n\nFor right skewed data, the mean is greater than the median.\n\nright_dist &lt;- tibble(x = rbeta(1e6, 2, 10))\n\nggplot(right_dist, aes(x = x)) + \n  geom_histogram() + \n  geom_vline(xintercept = mean(right_dist$x), colour = \"#FB8F86\", size = 2) + \n  geom_vline(xintercept = median(right_dist$x), colour = \"#819FE3\", size = 2) + \n  theme_minimal()\n\n\n\n\nFor left skewed data, the mean is smaller than the median.\n\nleft_dist &lt;- tibble(x = rbeta(1e6, 10, 2))\n\nggplot(left_dist, aes(x = x)) + \n  geom_histogram() + \n  geom_vline(xintercept = mean(left_dist$x), colour = \"#FB8F86\", size = 2) + \n  geom_vline(xintercept = median(left_dist$x), colour = \"#819FE3\", size = 2) + \n  theme_minimal()\n\n\n\n\n\n\n\nFive number summary\nAs you can see, we are attempting to summarise our continuous data to give us a meaningful but manageable sense of it. Means and medians are useful for continuous data.\nWe can provide more context to our understanding using more summary statistics. A common approach is the five number summary. This includes:\n\nThe smallest value;\nThe 25th percentile value, or the median of the lower half of the data;\nThe mean;\nThe 75th percentile value, or the median of the upper half of the data;\nThe largest value.\n\nWe can use skimr::skim() to quickly get useful information about our continuous variable.\n\nskim(perc_edu$value)\n\n\nData summary\n\n\nName\nperc_edu$value\n\n\nNumber of rows\n217\n\n\nNumber of columns\n1\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ndata\n59\n0.73\n0.05\n0.02\n0\n0.03\n0.04\n0.06\n0.14\n▂▇▃▁▁\n\n\n\n\n\nWe have 217 rows (because our unit of observation is a country, we can read this as 217 countries). We are missing education spending values for 59 of those countries (see n_missing), giving us a complete rate of 73% (see complete_rate).\nThe country that spent the least on education as a percent of its GDP in 2020 was Cuba, which spent 0.0% (see p0). The country that spent the most was the Marshall Islands, which spent 13.6% (see p100). The average percent of GDP spent on education in 2020 was 4.6% (see mean) and the median was 4.5% (see p50).\nThis description was a bit unwieldy. To get a better sense of our data, we can visualize it.\n\n\nBox plots\nBox plots (sometimes referred to as box and whisker plots) visualize the five number summary (with bonus features) nicely.\n\nggplot(perc_edu, aes(x = value)) + \n  geom_boxplot() + \n  theme_minimal() + \n  theme(\n    axis.text.y = element_blank()\n  ) + \n  labs(\n    x = \"Expenditure on education as a proportion of GDP\",\n    y = NULL\n  ) + \n  scale_x_continuous(labels = label_percent())\n\n\n\n\nNote that some values are displayed as dots. The box plot is providing you with a bit more information than the five number summary alone. The box itself displays the 25th percentile, the mean, and the 75th percentile values. The tails show you all the data up to a range 1.5 times the mean. If the smallest or largest values fall below or above (respectively) 1.5 times the mean, the tail ends at that value. If, however, these values fall outside that range, they are displayed as dots. These are (very rule of thumb, take with a grain of salt, please rely on your theory instead!) candidates for outliers.\n\n\nOutliers\nOutliers fall so far away from the majority of the other values that they should be examined closely and perhaps excluded from your analysis. Outliers can distort your mean. They do not, however, distort your median.\n\nWe will talk more about how to deal with outliers later in the course.\n\n\n\nMeasures of spread: range, variance, and standard deviation\nWe now have a good sense of some of the features of our data. Another useful thing to know is the shape of the distribution. Here, measures of spread are useful.\n\nRange\nThe range is the difference between the largest and smallest value.\n\\[\nrange = min - max\n\\]\n\nmax(perc_edu$value, na.rm = T) - min(perc_edu$value, na.rm = T)\n\n[1] 0.1362499\n\n\n\n\nVariance\nThe variance measures how spread out your values are. Take a look at these two plots. Both have the same center point (0) and number of observations (1,000,000). However, the data are much more spread out around that center point in the top graph.\n\nwide_dist &lt;- tibble(x = rnorm(1e6, sd = 2))\n\np1 &lt;- ggplot(wide_dist, aes(x = x)) + \n  geom_histogram() + \n  geom_vline(xintercept = 0) + \n  theme_minimal() + \n  scale_x_continuous(limits = c(-4, 4))\n\nnarrow_dist &lt;- tibble(x = rnorm(1e6, sd = 1))\n\np2 &lt;- ggplot(narrow_dist, aes(x = x)) + \n  geom_histogram() + \n  geom_vline(xintercept = 0) + \n  theme_minimal() + \n  scale_x_continuous(limits = c(-4, 4))\n\np1 / p2\n\n\n\n\nThe data in the top graph have higher variance (are more spread out) than those in the bottom graph. We measure this by calculating the average of the squares of the deviations of the observations from their mean.\n\\[\ns^2 = \\frac{\\Sigma(x_i - \\bar{x})^2}{n - 1}\n\\]\nLet’s step through this. We will first calculate the variance for wide_dist, or the top graph. To do this:\n\nCalculate the mean of your values.\nCalculate the difference between each individual value and that mean.\nSquare those differences.\n\nWe do not care whether the value is higher or lower than the mean. We only care how far from the mean it is. Squaring a value removes its sign (positive or negative) allowing us to concentrate on this difference.\n\nAdd all of those squared differences to get a single number.\nDivide that single number by the number of observations you have minus 1.\n\nYou now have your variance!\nIn R:\n\nwide_var_calc &lt;- wide_dist |&gt; \n  mutate(\n    mean = mean(wide_dist$x),\n    diff = x - mean,\n    diff_2 = diff^2\n  )\n\nwide_var_calc\n\n# A tibble: 1,000,000 × 4\n         x     mean    diff   diff_2\n     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n 1  2.65   -0.00102  2.66    7.05   \n 2 -0.120  -0.00102 -0.119   0.0143 \n 3  0.599  -0.00102  0.600   0.360  \n 4  0.731  -0.00102  0.732   0.536  \n 5  1.32   -0.00102  1.33    1.76   \n 6 -0.966  -0.00102 -0.965   0.932  \n 7  3.30   -0.00102  3.30   10.9    \n 8 -2.42   -0.00102 -2.42    5.87   \n 9  0.0948 -0.00102  0.0958  0.00917\n10  0.197  -0.00102  0.198   0.0392 \n# ℹ 999,990 more rows\n\n\nWe take the sum of square of the difference between each observation and the mean of our whole sample. We then divide that by one less than our number of observations.\n\nwide_var &lt;- sum(wide_var_calc$diff_2) / (nrow(wide_var_calc) - 1)\n\nwide_var\n\n[1] 3.992407\n\n\nWe can compare this to the variance for our narrower distribution.\n\nnarrow_var_calc &lt;- narrow_dist |&gt; \n  mutate(\n    mean = mean(narrow_dist$x),\n    diff = x - mean,\n    diff_2 = diff^2\n  )\n\nnarrow_var &lt;- sum(narrow_var_calc$diff_2) / (nrow(narrow_var_calc) - 1)\n\nnarrow_var\n\n[1] 0.9989804\n\n\nIt is, in fact, smaller!\nThat was painful. Happily we can use var() to do this in one step:\n\nvar(wide_dist)\n\n         x\nx 3.992407\n\n\n\nvar(narrow_dist)\n\n          x\nx 0.9989804\n\n\n\nvar(wide_dist) &gt; var(narrow_dist)\n\n     x\nx TRUE\n\n\n\n\nStandard deviation\nA simpler measure of spread is the standard deviation. It is simply the square root of the variance.\n\nsqrt(wide_var)\n\n[1] 1.998101\n\n\n\nsqrt(narrow_var)\n\n[1] 0.9994901\n\n\nYou can get this directly using sd():\n\nsd(wide_dist$x)\n\n[1] 1.998101\n\n\n\nsd(narrow_dist$x)\n\n[1] 0.9994901\n\n\nIf you look back to our graphs, you will see that I set the standard deviations explicitly when I generated the data: rnorm() takes an sd argument. This is great because we can confirm that the standard deviations for the wide and narrow distributions are 2 and 1 respectively (with a little bit of noise).\n\ntibble(\n  n = rnorm(1e6, sd = 1),\n  w = rnorm(1e6, sd = 2)\n) |&gt; \n  ggplot() + \n  geom_density(aes(x = n), colour = \"green\", size = 2) + \n  geom_density(aes(x = w), colour = \"lightblue\", size = 2) + \n  theme_minimal()\n\n\n\n\n\nRemember that the standard deviation is a measure of how spread out our data are. Therefore, data with no spread (are all the exact same number) will have a standard deviation of 0.\n\n\n\n\nNormal distributions\nRemember that normal distributions share a mean and median. This has very cool and useful side effects.\n\nnorm_5_2 &lt;- tibble(x = rnorm(n = 1e6, mean = 5, sd = 2))\n\nggplot(norm_5_2, aes(x = x)) + \n  stat_slab(\n    aes(fill_ramp = after_stat(cut_cdf_qi(cdf))), fill = met.brewer(\"Egypt\")[2]\n  ) + \n  scale_fill_ramp_discrete(range = c(1, 0.2), guide = \"none\") + \n  theme_minimal()\n\n\n\n\n\nApproximately 68% of the data fall within one standard deviation of the mean (the dark blue).\nApproximately 95% of the data fall within two standard deviations of the mean (the medium blue).\nApproximately 99.7% of the data fall within three standard deviations of the mean (the light blue).\n\n\n\nStandardization\nNotice how our description of each variable depends on its units of measurement. What do we do if we want to compare across different measurements that have different units?\n\nZ scores\nFor normal distributions, we can use the z score. This gives us a standard way of understanding how many standard deviations from the mean of a normally distributed variable a value is.\n\\[\nz_i = \\frac{x_i - \\mu_x}{\\sigma_x}\n\\]\nWe are just transforming our data. We want to center it around 0 and reshape it so that roughly 68% of the data fall within one standard deviation of the mean, 95% of the data fall within two standard deviations of the mean, and 99.7% of the data fall within three standard deviations of the mean.\nLet’s standardize our data from above.\n\nstandard_5_2 &lt;- norm_5_2 |&gt; \n  mutate(mean = mean(x),\n         sd = sd(x),\n         z_score = (x - mean) / sd)\n\nhead(standard_5_2)\n\n# A tibble: 6 × 4\n      x  mean    sd z_score\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1  6.65  5.00  2.00   0.824\n2  5.76  5.00  2.00   0.380\n3  6.30  5.00  2.00   0.649\n4  7.68  5.00  2.00   1.34 \n5  6.18  5.00  2.00   0.589\n6  6.49  5.00  2.00   0.745\n\n\nWe can confirm this:\n\nggplot(standard_5_2, aes(x = z_score)) + \n  stat_slab(\n    aes(fill_ramp = after_stat(cut_cdf_qi(cdf))), fill = met.brewer(\"Egypt\")[2]\n  ) + \n  scale_fill_ramp_discrete(range = c(1, 0.2), guide = \"none\") + \n  theme_minimal() + \n  scale_x_continuous(breaks = seq(-5, 5, 1))\n\n\n\n\n\n\n\n\nNext week\nA focus on techniques for examining relationships between variables."
  },
  {
    "objectID": "content/01-introduction.html",
    "href": "content/01-introduction.html",
    "title": "Course Introduction",
    "section": "",
    "text": "Pollock & Edwards R Companion, Chapter 1"
  },
  {
    "objectID": "content/01-introduction.html#readings",
    "href": "content/01-introduction.html#readings",
    "title": "Course Introduction",
    "section": "",
    "text": "Pollock & Edwards R Companion, Chapter 1"
  },
  {
    "objectID": "content/01-introduction.html#class-slides",
    "href": "content/01-introduction.html#class-slides",
    "title": "Course Introduction",
    "section": "Class slides",
    "text": "Class slides"
  },
  {
    "objectID": "content/05-applications.html",
    "href": "content/05-applications.html",
    "title": "Applications & Midterm Exam Review",
    "section": "",
    "text": "Segal, Jeffrey A. & Albert D. Cover. 1989. “Ideological Values and the Votes of U.S. Supreme Court Justices.” American Political Science Review 83(2): 557-565.\n Sondheimer, Rachel Milstein & Donald P. Green. 2010. “Using Experiments to Estimate the Effects of Education on Voter Turnout.” American Journal of Political Science 54(1): 174-189."
  },
  {
    "objectID": "content/05-applications.html#readings",
    "href": "content/05-applications.html#readings",
    "title": "Applications & Midterm Exam Review",
    "section": "",
    "text": "Segal, Jeffrey A. & Albert D. Cover. 1989. “Ideological Values and the Votes of U.S. Supreme Court Justices.” American Political Science Review 83(2): 557-565.\n Sondheimer, Rachel Milstein & Donald P. Green. 2010. “Using Experiments to Estimate the Effects of Education on Voter Turnout.” American Journal of Political Science 54(1): 174-189."
  },
  {
    "objectID": "content/05-applications.html#class-slides",
    "href": "content/05-applications.html#class-slides",
    "title": "Applications & Midterm Exam Review",
    "section": "Class slides",
    "text": "Class slides"
  },
  {
    "objectID": "content/05-applications.html#section",
    "href": "content/05-applications.html#section",
    "title": "Applications & Midterm Exam Review",
    "section": "Section",
    "text": "Section"
  },
  {
    "objectID": "content/10-applications.html",
    "href": "content/10-applications.html",
    "title": "Applications & Midterm Exam Review II",
    "section": "",
    "text": "Brooks, Deborah Jordan. 2011. “Testing the Double Standard for Candidate Emotionality: Voter Reactions to the Tears and Anger of Male and Female Politicians.” The Journal of Politics 73: 597-615."
  },
  {
    "objectID": "content/10-applications.html#readings",
    "href": "content/10-applications.html#readings",
    "title": "Applications & Midterm Exam Review II",
    "section": "",
    "text": "Brooks, Deborah Jordan. 2011. “Testing the Double Standard for Candidate Emotionality: Voter Reactions to the Tears and Anger of Male and Female Politicians.” The Journal of Politics 73: 597-615."
  },
  {
    "objectID": "content/10-applications.html#class-slides",
    "href": "content/10-applications.html#class-slides",
    "title": "Applications & Midterm Exam Review II",
    "section": "Class slides",
    "text": "Class slides"
  },
  {
    "objectID": "content/10-applications.html#section",
    "href": "content/10-applications.html#section",
    "title": "Applications & Midterm Exam Review II",
    "section": "Section",
    "text": "Section"
  },
  {
    "objectID": "content/07-statistical_inference.html",
    "href": "content/07-statistical_inference.html",
    "title": "Hypothesis Testing I",
    "section": "",
    "text": "Pollock & Edwards, Chapter 6\n\n\n\n Pollock & Edwards R Companion, Chapter 6"
  },
  {
    "objectID": "content/07-statistical_inference.html#readings",
    "href": "content/07-statistical_inference.html#readings",
    "title": "Hypothesis Testing I",
    "section": "",
    "text": "Pollock & Edwards, Chapter 6\n\n\n\n Pollock & Edwards R Companion, Chapter 6"
  },
  {
    "objectID": "content/07-statistical_inference.html#class-slides",
    "href": "content/07-statistical_inference.html#class-slides",
    "title": "Hypothesis Testing I",
    "section": "Class slides",
    "text": "Class slides"
  },
  {
    "objectID": "content/07-statistical_inference.html#section",
    "href": "content/07-statistical_inference.html#section",
    "title": "Hypothesis Testing I",
    "section": "Section",
    "text": "Section\n\nPrerequisites\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(poliscidata)\nlibrary(ggdist)\nlibrary(MetBrewer)\nlibrary(DescTools)\n\nset.seed(1234)\n\n\n\nHow will what you learn this week help your research?\n\n\nPopulation and sample\nSay we are interested in the proportion of US voters who will vote for Joe Biden in the 2024 general election. We cannot ask all US voters of their intentions. Instead, we ask a sample of the US voting population and infer from that sample the population’s intentions.\n\nThe data point of interest among the population is referred to as the parameter. Here, it is the proportion of US voters who intend to vote for Joe Biden in the 2024 general election.\nThe data point of interest among the sample is referred to as the statistic. Here, it is the proportion of survey respondents who intend to vote for Joe Biden in the 2024 general election.\nWe aim to have a statistic that accurately represents the parameter.\n\nWhen we generalize from the sample statistic to the parameter we are engaging in statistical inference.\nHow can we be confident that our statistic represents the parameter? Generally speaking, the more our sample “looks like” our population, the more confident we can be that we have a good statistic. Drawing on probability theory, our sample is increasingly likely to resemble our population with its randomness and size.\nYou should strive for a pure random sample. This means that every individual within your population is equally likely to be drawn. This is really hard to achieve! Think about normal election surveys. Many are conducted over the phone. There are plenty of people who do not have a landline phone, or do not pick up calls from unknown numbers, or who keep their phones on do not disturb during the day. These people will be harder to contact than those who are sitting by the phone waiting eagerly for a call. Even if you have access to all US voters’ phone numbers (never mind that some voters do not have phone numbers) and you take a random sample of those phone numbers and start calling, you still will not get a hold of them all with equal probability.\nYou should also strive for as large a sample as you can possibly get. More is always better in terms of statistical inference (if not your research budget or time). Remember back to our coin flips last week. The more coin flips we did, the closer we got to the true probability distribution between heads and tails. This principle also holds here.\n\n\nSampling error\nImagine you have a large and representative sample. You are still going to have some error. This is because your sample varies in all the normal ways events with uncertainty vary. To illustrate, let’s return to our coin flips.\nWe state our possible outcomes:\n\npossible_outcomes &lt;- c(\"HEADS\", \"TAILS\")\npossible_outcomes\n\n[1] \"HEADS\" \"TAILS\"\n\n\nWe flip our coin 100 times:\n\nsample(possible_outcomes, 100, replace = T, prob = c(0.5, 0.5))\n\n  [1] \"TAILS\" \"HEADS\" \"HEADS\" \"HEADS\" \"HEADS\" \"HEADS\" \"TAILS\" \"TAILS\" \"HEADS\"\n [10] \"HEADS\" \"HEADS\" \"HEADS\" \"TAILS\" \"HEADS\" \"TAILS\" \"HEADS\" \"TAILS\" \"TAILS\"\n [19] \"TAILS\" \"TAILS\" \"TAILS\" \"TAILS\" \"TAILS\" \"TAILS\" \"TAILS\" \"HEADS\" \"HEADS\"\n [28] \"HEADS\" \"HEADS\" \"TAILS\" \"TAILS\" \"TAILS\" \"TAILS\" \"HEADS\" \"TAILS\" \"HEADS\"\n [37] \"TAILS\" \"TAILS\" \"HEADS\" \"HEADS\" \"HEADS\" \"HEADS\" \"TAILS\" \"HEADS\" \"TAILS\"\n [46] \"HEADS\" \"HEADS\" \"TAILS\" \"TAILS\" \"HEADS\" \"TAILS\" \"TAILS\" \"HEADS\" \"HEADS\"\n [55] \"TAILS\" \"HEADS\" \"TAILS\" \"HEADS\" \"TAILS\" \"HEADS\" \"HEADS\" \"TAILS\" \"TAILS\"\n [64] \"TAILS\" \"TAILS\" \"HEADS\" \"TAILS\" \"HEADS\" \"TAILS\" \"HEADS\" \"TAILS\" \"HEADS\"\n [73] \"TAILS\" \"HEADS\" \"TAILS\" \"HEADS\" \"TAILS\" \"TAILS\" \"TAILS\" \"HEADS\" \"HEADS\"\n [82] \"TAILS\" \"TAILS\" \"HEADS\" \"TAILS\" \"HEADS\" \"TAILS\" \"TAILS\" \"TAILS\" \"HEADS\"\n [91] \"TAILS\" \"HEADS\" \"TAILS\" \"TAILS\" \"TAILS\" \"HEADS\" \"TAILS\" \"TAILS\" \"TAILS\"\n[100] \"HEADS\"\n\n\nWe know that the true probability of the coin landing on heads is 0.5. If we flip a fair coin 100 times, we should get 50 heads. We also know that these random draws are a bit noisy: we can get proportions that do not reflect the underlying probability of 0.5. However, the more flips we do, the closer we will get to that true probability distribution.\nLet’s do 100,000 100-coin flip trials and record the number of heads we get each time:\n\ncoin_flip &lt;- function(possible_outcomes, n) {\n  \n  outcomes &lt;- sample(possible_outcomes, size = n, replace = T, prob = c(0.5, 0.5))\n  \n  return(table(outcomes)[\"HEADS\"])\n  \n}\n\nresults &lt;- tibble(trial = 1:100000) |&gt; \n  rowwise() |&gt; \n  mutate(n_heads = coin_flip(possible_outcomes, 100))\n\nresults\n\n# A tibble: 100,000 × 2\n# Rowwise: \n   trial n_heads\n   &lt;int&gt;   &lt;int&gt;\n 1     1      52\n 2     2      52\n 3     3      48\n 4     4      60\n 5     5      61\n 6     6      49\n 7     7      52\n 8     8      50\n 9     9      49\n10    10      45\n# ℹ 99,990 more rows\n\n\nWhat are the results of these repeated trials?\n\nggplot(results, aes(x = n_heads)) + \n  geom_histogram() + \n  geom_vline(xintercept = 50)\n\n\n\n\nSo we know that each time we flip that coin, there is a 50% chance that it will land on heads. We know this because we programmed it in to our sample (sample(c(\"HEADS\", \"TAILS\"), prob = c(0.5, 0.5))). We don’t usually have this luxury of knowing the parameter, so let’s take advantage of this to build up our confidence around good samples and their relationship to the population.\nEven though every time we flip the coin there is a 50% chance it lands on heads, we still get some trials in which we draw many more or far fewer than our expected 50 heads. We have some as low as 28 and some as large as 71. But notice how the number of heads recorded in most of our trials are clustered around our expected 50. The mean of our results is 49.9987 which is really, really close to our known parameter of 0.5 or 50%. Yay!\nSo, even with representative and large samples you will get some error. That’s okay. We can still use that sample to confidently infer what the parameter looks like.\nLet’s extend this a little further. What happens if we conduct less trials? Let’s try with only 100 trials.\n\nresults_100 &lt;- tibble(trial = 1:100) |&gt; \n  rowwise() |&gt; \n  mutate(n_heads = coin_flip(possible_outcomes, 100))\n\nggplot(results_100, aes(x = n_heads)) + \n  geom_histogram() + \n  geom_vline(xintercept = 50)\n\n\n\n\nNot as clean as we would like. The average number of heads drawn in each of these 100 trials is 50.31, which is 0.31 points away from the parameter (compared to -0.0013 for our 100,000 trials).\nWhat about if we decrease the number of draws we make in each trial? Let’s only take 10 draws in our original 100,000 trials.\n\nresults_10 &lt;- tibble(trial = 1:100000) |&gt; \n  rowwise() |&gt; \n  mutate(n_heads = coin_flip(possible_outcomes, 10))\n\nggplot(results_10, aes(x = n_heads)) + \n  geom_histogram() + \n  geom_vline(xintercept = 5)\n\n\n\n\nThe average number of heads drawn in each of these 100,000 trials is 5.0028231, which is 0.0028231 points away from the parameter (compared to -0.0013 for our 100,000 trials).\nThe lessons we can take from this is that more is better. The more times you flip that coin, the closer you will get to the true underlying probability of a fair coin landing on heads.\n\nYou will need to make important decisions in your own research regarding the number of samples with which you are comfortable. This will be constrained by your budget, time, and population. You will get a more accurate picture of the parameter with more data points. However, adding another 1,000,000 responses to your survey may result in a change so small it has no material impact what you infer from your analysis. If this is the case and you have a representative sample, you are well justified in not running yourself dry trying to get those extra observations.\nFor more information on working out the smallest acceptable sample size for an experiment, look up power analysis.\n\n\n\nSampling distributions\nLet’s move on from coin flips. Suppose that we want to know how many Americans identify as Democrats. We will return to the American National Election Survey to answer this question.\nThis survey asks respondents whether they identify as a Democrat (this binary variable takes on 0 if not and 1 if they do).\n\nnes |&gt; \n  select(caseid, dem) |&gt; \n  head()\n\n  caseid dem\n1    408   0\n2   3282   1\n3   1942   0\n4    118   1\n5   5533   0\n6   5880   0\n\n\nLet’s very cheekily pretend that this is a complete survey of the entire voting population of America. That way, we can pretend that we know the proportion of US voters who identify as Democrats (our parameter).\n\ntabyl(nes, dem)\n\n dem    n     percent valid_percent\n   0 3534 0.597363083     0.5997963\n   1 2358 0.398580122     0.4002037\n  NA   24 0.004056795            NA\n\n\nOkay, so let’s pretend that 40% of all US voters identify as Democrats.\nWe can’t survey all voters, so instead we take a representative and large sample from this population:\n\nnes_sample &lt;- nes |&gt; \n  select(caseid, dem) |&gt; \n  slice_sample(n = 3000)\n\nWe have taken a pure random sample of 3,000 (or 51% of our population of 5,916 voters). Each of those voters had an equal probability of being picked for this sample.\nWhat proportion of this sample identify as Democrats?\n\ntabyl(nes_sample, dem)\n\n dem    n     percent valid_percent\n   0 1795 0.598333333     0.6011386\n   1 1191 0.397000000     0.3988614\n  NA   14 0.004666667            NA\n\n\n39.70%. Nice! But what if we took a different sample of 3,000?\n\nnes_sample_2 &lt;- nes |&gt; \n  select(caseid, dem) |&gt; \n  slice_sample(n = 3000)\n\n\ntabyl(nes_sample_2, dem)\n\n dem    n percent valid_percent\n   0 1779   0.593     0.5947844\n   1 1212   0.404     0.4052156\n  NA    9   0.003            NA\n\n\nWe get a different answer: 40.40%. Of course! This is just like our different coin flip trials from last week. Each resulted in a different number of heads. The more flips we did, the closer we got to the true underlying probability distribution.\nLet’s take 1,000 different samples of 3,000 US voters and see what we get:\n\ndem_survey &lt;- function(df, n) {\n  \n  slice_sample(df, n = n) |&gt; \n    tabyl(dem) |&gt; \n    filter(dem == 1) |&gt; \n    pull(percent)\n  \n}\n\nnes_samples_1000 &lt;- tibble(survey = 1:1000) |&gt; \n  rowwise() |&gt; \n  mutate(prop_dem = dem_survey(select(nes, caseid, dem), 3000)) |&gt; \n  ungroup()\n\nnes_samples_1000\n\n# A tibble: 1,000 × 2\n   survey prop_dem\n    &lt;int&gt;    &lt;dbl&gt;\n 1      1    0.395\n 2      2    0.400\n 3      3    0.396\n 4      4    0.386\n 5      5    0.394\n 6      6    0.404\n 7      7    0.396\n 8      8    0.405\n 9      9    0.399\n10     10    0.404\n# ℹ 990 more rows\n\n\n\nggplot(nes_samples_1000, aes(x = prop_dem)) + \n  geom_histogram() + \n  geom_vline(xintercept = tabyl(nes, dem) |&gt; filter(dem == 1) |&gt; pull(percent))\n\n\n\n\nOn average, 39.84% of US voters in our 1,000 samples of 3,000 US voters identified as Democrats. Our (cheeky) population average is 39.86%. Yay! As long as our sample is large and representative, we should be able to infer from our sample what is going on in the population.\nYou’ll have noticed that these draws are always symmetrical or normally distributed around the sample mean (which is; hopefully, also the population mean). This distribution of your statistic is referred to as your sampling distribution. We lean very heavily on some important characteristics of this distribution when doing statistical inference.\nWhen your sample is large and representative, your sampling distribution will be near normally distributed. The center will be at (or very, very close to) the population mean. This is called the Central Limit Theorem. This theorem suggests that statistics (including means, proportions, counts) from large and randomly drawn samples are very good approximations of the underlying (and often unobservable) population parameter.\n\n\nInferring from a single “trial”\nIn a lot of (social) science is is not practical or, in some cases, possible to do many trials. For example, a lot of us study the onset, conduct, and termination of wars. Unlike a game of chess, you cannot reset and run a war many times in order to get your sampling distribution of your variable of interest.\nFurther, we often do not know the shape or size of our population. For example, the best guess we have of the demographics of the US population comes from the census. But this misses a lot of people. If you want to study houselessness, you might need to rely on surveys of samples of people that may or may not be representative of this difficult to reach population of people.\nA lot of the time; therefore, you will have one data point. This requires that we take some lessons learned from above and make some pretty important assumptions.\nLet’s return to our survey work above. We took 1,000 different samples of 3,000 US voters and asked each of them whether they identified as Democrats. We recorded the proportion of the 3,000 respondents who identified as Democrats in each of our 1,000 different samples. We then took the average of those 1,000 different proportions and compared it to our population average. In line with the Central Limit Theorem, we found that the average of our sample statistics was very, very close to our population parameter.\nOkay, now imagine that you could only run one of those trials. Let’s select one at random:\n\nnes_single &lt;- slice_sample(nes_samples_1000)\nnes_single\n\n# A tibble: 1 × 2\n  survey prop_dem\n   &lt;int&gt;    &lt;dbl&gt;\n1    821    0.396\n\n\nHow close is this single sample statistic to the population parameter of 39.86%? Pretty close! In fact, you are more likely to get a sample statistic close to the population parameter than not.\nRemember, when we ran multiple trials we got many sample statistics that were clustered around the population mean.\n\nggplot(nes_samples_1000, aes(x = prop_dem)) + \n  geom_histogram() + \n  geom_vline(xintercept = tabyl(nes, dem) |&gt; filter(dem == 1) |&gt; pull(percent))\n\n\n\n\nSo, if you were to pick one of these trials at random, you are more likely to pick one with a sample statistic that is close to the population parameter than not. Convenient!\n\n\nHow confident can we be in our statistic?\nThat being said, we could get unlucky and have drawn a large and representative sample that sits at one of those extreme values. How confident can we be that our single sample statistic is close to the population parameter?\nRemember back to our week on descriptive statistics. There are some super handy properties of normal distributions on which we will draw.\n\nnorm_5_2 &lt;- tibble(x = rnorm(n = 1e6, mean = 5, sd = 2))\n\nggplot(norm_5_2, aes(x = x)) + \n  stat_slab(\n    aes(fill_ramp = after_stat(cut_cdf_qi(cdf))), fill = met.brewer(\"Egypt\")[2]\n  ) + \n  scale_fill_ramp_discrete(range = c(1, 0.2), guide = \"none\") + \n  theme_minimal()\n\n\n\n\nFor normally distributed data:\n\nApproximately 68% of the data fall within one standard deviation of the mean (the dark blue).\nApproximately 95% of the data fall within two standard deviations of the mean (the medium blue).\nApproximately 99.7% of the data fall within three standard deviations of the mean (the light blue).\n\nSo, if we assume that the statistic we get from our large and representative sample is our “best guess” at the population parameter, we can center our theoretical sampling distribution around this point. We know that this distribution is normally distributed. So we can calculate the shape of that distribution using what we know about normal distributions.\n68 percent of the hypothetical sample statistics would fall within one standard deviation of the mean. We will build into our statistic an acknowledgment of this uncertainty.\nLet’s go back to our single sample of 3,000 respondents from the NES survey to illustrate how we do this.\n\nhead(nes_sample)\n\n  caseid dem\n1   5849   1\n2    595   0\n3   3988   1\n4   3822   0\n5   3157   0\n6   3145   1\n\n\nWe need to calculate the standard deviation of the mean. When we are looking at the standard deviation of the sampling distribution, we refer to it as the standard error.\nThe formula for working out the standard error is:\n\\[\nS_{\\bar{x}} = \\frac{S}{\\sqrt{n}}\n\\]\nThe standard error of the mean is equal to the standard deviation of the whole sample divided by the square root of the sample size.\n\nWe learnt how to calculate the standard deviation of a vector of data (in this case, the whole sample) in the Descriptive Statistics week.\n\nFirst, we need to find the standard deviation of the whole sample. We can use sd():\n\nsd_sample &lt;-sd(nes_sample$dem, na.rm = T)\n  \nsd_sample\n\n[1] 0.4897462\n\n\nThen we need to divide this by the square root of the size of the sample:\n\nse_mean &lt;- sd_sample / sqrt(nrow(nes_sample))\n\nse_mean\n\n[1] 0.008941501\n\n\nYou can use DescTools::MeanSE() to calculate this in one line:\n\nMeanSE(nes_sample$dem, na.rm = T)\n\n[1] 0.008962437\n\n\nThe mean is 0.3989 and the standard error of this mean is 0.0089. Great! So now we know that 68% of the hypothetical means of our hypothetical trials sit within plus or minus 0.0089 of 0.3989.\nTranslating this into our research question: based on our sample, we are 68 percent confident that the true percentage of US voters who identify as Democrats sits between 38.99% and 40.78%.\nOf course, if we took a different sample, we would get a different sample mean and a different confidence interval. The point is that as long as you have a large and representative sample, your sample is more likely than not to be close to the population parameter.\nWhat if you want to be more confident than 68 percent? Again, we can draw on our knowledge of normal distributions to help us out. Let’s work out the bounds within which we are 95 percent confident the proportion of US voters who identify as Democrats sit.\nI have to admit here that I cheated a little in calculating the 68 percent confidence interval. If you remember back to our week on descriptive statistics, you will recall that these ranges (68%, 95%, and 99.7%) are artifacts of our efforts to standardize our data using z-scores.\nA quick refresher that you can promptly forget again:\n\\[\nz_i = \\frac{x_i - \\mu_x}{\\sigma_x}\n\\]\nNow, it just so happens that doing this to your normally distributed data means that 68 percent of your data will land within one z-score of the center point.\nSo, when we calculated the 68 percent confidence interval as:\n\\[\nmean \\pm se\n\\]\nWhat we actually (sneakily) did was:\n\\[\nmean \\pm z * se\n\\]\nThe z-score just so happened to be one.\nWhat z-score captures 95 percent of our data?\n\nqnorm(p = 0.025, lower.tail = F)\n\n[1] 1.959964\n\n\n\nThe first argument in qnorm() is the vector of probabilities. We want to find the 95 percent confidence interval, so we need to find the boundaries beyond which the remaining 5 percent of data sit. Remember that a normal distribution is symmetrical. Data falls above and below our center point. So to get the upper boundary, we need to halve 5 percent (hence p = 0.025) and then ask qnorm() to only give us the upper boundary (lower.tail = F). You can, of course, take the lower boundary but then you have to deal with a negative number which is kind of annoying.\n\nSo, the z-score that will give us 95 percent of our data sitting around our center point is 1.96.\nTherefore, our lower bound is:\n\nse_mean - qnorm(p = 0.025, lower.tail = F) * se_mean\n\n[1] -0.008583519\n\n\nAnd our upper bound is:\n\nse_mean + qnorm(p = 0.025, lower.tail = F) * se_mean\n\n[1] 0.02646652\n\n\nTranslated, this means that we are 95 percent confident that the true percentage of US voters who identify as Democrats sits between 38.13% and 41.64%, based on our sample.\n\nEXERCISE: Calculate the 99.7 percent confidence interval around the mean.\n\nWe are putting a lot of stead in our single sample. That’s okay as long as your sample is large and representative. Over these past few weeks we have discussed in sometimes painful detail why we can make some of the assumptions on which we rely. But, at the end of the day, you are the expert. You have explored your data with a critical eye. You have read everything you possibly can about this topic. You might have even gone out in the field and gotten your hands dirty. The more you know about your subject matter, the better you will be able to detect whether something strange is going on with your sample and your findings. This is so important. The strength of your empirical analysis is built on these foundations.\n\n\nChoosing your sample size\nWe know that our confidence around our point estimate increases with the number of observations. Formally:\n\\[\nS_{\\bar{x}} = \\frac{S}{\\sqrt{n}}\n\\]\nSo, as you increase that \\(n\\), you decrease your standard error and you narrow the interval over which you have a given level of confidence.\nI have said before that more is always better. Technically, this is very, very true. But those marginal returns diminish. And those survey costs stack up.\nTo illustrate, let’s look at how much your standard error decreases as you increase your sample size.\n\nse_from_sample &lt;- function(n_samples) {\n  \n  nes |&gt; \n    slice_sample(n = n_samples) |&gt; \n    pull(dem) |&gt; \n    MeanSE(na.rm = T)\n  \n}\n\ntibble(sample_size = seq(from = 100, to = 3000, by = 100)) |&gt; \n  rowwise() |&gt; \n  mutate(se = se_from_sample(sample_size)) |&gt; \n  ggplot(aes(x = sample_size, y = se)) + \n  geom_line() + \n  theme_minimal() + \n  labs(x = \"Number of observations in the sample\",\n       y = \"Standard error\")\n\n\n\n\nMoving from 100 to 1,000 observations in your sample dramatically decreases your standard error. Moving the same distance from 1,000 to 1,900 makes a far smaller difference to your error.\n\n\nNext week\nWe will discuss the guts of quantitative analysis: hypothesis testing."
  },
  {
    "objectID": "content/12-multiple_regression.html",
    "href": "content/12-multiple_regression.html",
    "title": "Regression Analysis II",
    "section": "",
    "text": "Pollock & Edwards, Chapter 9\n\n\n\n Pollock & Edwards R Companion, Chapter 9"
  },
  {
    "objectID": "content/12-multiple_regression.html#readings",
    "href": "content/12-multiple_regression.html#readings",
    "title": "Regression Analysis II",
    "section": "",
    "text": "Pollock & Edwards, Chapter 9\n\n\n\n Pollock & Edwards R Companion, Chapter 9"
  },
  {
    "objectID": "content/12-multiple_regression.html#class-slides",
    "href": "content/12-multiple_regression.html#class-slides",
    "title": "Regression Analysis II",
    "section": "Class slides",
    "text": "Class slides"
  },
  {
    "objectID": "content/12-multiple_regression.html#section",
    "href": "content/12-multiple_regression.html#section",
    "title": "Regression Analysis II",
    "section": "Section",
    "text": "Section"
  },
  {
    "objectID": "content/09-hypothesis_testing.html",
    "href": "content/09-hypothesis_testing.html",
    "title": "Hypothesis Testing III",
    "section": "",
    "text": "Pollock & Edwards R Companion, Chapter 7"
  },
  {
    "objectID": "content/09-hypothesis_testing.html#readings",
    "href": "content/09-hypothesis_testing.html#readings",
    "title": "Hypothesis Testing III",
    "section": "",
    "text": "Pollock & Edwards R Companion, Chapter 7"
  },
  {
    "objectID": "content/09-hypothesis_testing.html#class-slides",
    "href": "content/09-hypothesis_testing.html#class-slides",
    "title": "Hypothesis Testing III",
    "section": "Class slides",
    "text": "Class slides"
  },
  {
    "objectID": "content/09-hypothesis_testing.html#section",
    "href": "content/09-hypothesis_testing.html#section",
    "title": "Hypothesis Testing III",
    "section": "Section",
    "text": "Section"
  },
  {
    "objectID": "content/13-conclusion.html#section",
    "href": "content/13-conclusion.html#section",
    "title": "Regression Analysis Extensions & Final Exam Review",
    "section": "Section",
    "text": "Section"
  },
  {
    "objectID": "content/04-research_design.html",
    "href": "content/04-research_design.html",
    "title": "Research Design",
    "section": "",
    "text": "Pollock & Edwards, Chapter 4\n\n\n\n Pollock & Edwards R Companion, Chapter 5"
  },
  {
    "objectID": "content/04-research_design.html#readings",
    "href": "content/04-research_design.html#readings",
    "title": "Research Design",
    "section": "",
    "text": "Pollock & Edwards, Chapter 4\n\n\n\n Pollock & Edwards R Companion, Chapter 5"
  },
  {
    "objectID": "content/04-research_design.html#class-slides",
    "href": "content/04-research_design.html#class-slides",
    "title": "Research Design",
    "section": "Class slides",
    "text": "Class slides"
  },
  {
    "objectID": "content/04-research_design.html#section",
    "href": "content/04-research_design.html#section",
    "title": "Research Design",
    "section": "Section",
    "text": "Section"
  },
  {
    "objectID": "content/03-bivariate_relationships.html",
    "href": "content/03-bivariate_relationships.html",
    "title": "Relationships Between Two Variables",
    "section": "",
    "text": "Pollock & Edwards, Chapter 3\n\n\n\n Pollock & Edwards R Companion, Chapters 4-5"
  },
  {
    "objectID": "content/03-bivariate_relationships.html#readings",
    "href": "content/03-bivariate_relationships.html#readings",
    "title": "Relationships Between Two Variables",
    "section": "",
    "text": "Pollock & Edwards, Chapter 3\n\n\n\n Pollock & Edwards R Companion, Chapters 4-5"
  },
  {
    "objectID": "content/03-bivariate_relationships.html#class-slides",
    "href": "content/03-bivariate_relationships.html#class-slides",
    "title": "Relationships Between Two Variables",
    "section": "Class slides",
    "text": "Class slides"
  },
  {
    "objectID": "content/03-bivariate_relationships.html#section",
    "href": "content/03-bivariate_relationships.html#section",
    "title": "Relationships Between Two Variables",
    "section": "Section",
    "text": "Section\n\nPrerequisites\n\nlibrary(tidyverse)\nlibrary(wbstats)\nlibrary(poliscidata)\nlibrary(countrycode)\nlibrary(broom)\nlibrary(janitor)\nlibrary(ggridges)\nlibrary(modelsummary)\n\n\n\nHow will what you learn this week help your research?\nAs usual, we start with an interesting question. You have some outcome of interest and you think that there is an important determinant of that outcome that no has yet identified. Or perhaps the relationship between some heavily chewed over determinant and the outcome of interest is misunderstood. We are steadily building up your ability to determine empirically the relationship between that determinant and the outcome of interest.\nSimply put (and there really is no need to over-complicate this), we have two or more variables: an outcome of interest (the dependent variable) and a set of independent variables that we theorize are important determinants of that outcome. We can use empirical analysis to understand 1) how the dependent and independent variables change in relation to one another, and 2) whether this relationship is strong enough that we should declare (though 12,000-word journal articles or by shouting from rooftops) that whenever we want to change or understand that outcome, we must consider these important independent variables.\nLast week, we discussed various tools that you can use to explore your data. You can develop a very good understanding of each of the individual variables. This exploration is very important for building your intuition and, by extension, your expertise in the question at hand. These tools also allow you to identify unusual data points (or outliers).\nThis week, we will make the next step. We will explore how two variables relate to each other. How do they move with each other: when one goes up, does the other go down, up, or not really move? How strong is this association?\nThis exploration is particularly important for you to do with regard to the independent variable(s) that are the focal point of your theory and, therefore, your contribution to our understanding of the messy spaghetti bowl of things that determine your outcome of interest. Just as it is important for you to spend some time understanding the shape of your variables (using the tools we discussed last week), you must also start to understand the shape of the relationship between the outcome you are trying to understand or predict and the factors you think are important determinants of that outcome.\nLet’s begin!\n\n\nBivariate relationships\nHow do two variables move with one another? When when goes up, does the other go down, up, or not really move at all? How dramatic is this shift?\nThe type of variables we have determines how we can answer this question. To begin, we will explore the relationship between two continuous variables. Later in the class, we will look at how to explore the relationship between a continuous and categorical variable.\nTo start, we will explore the relationship between wealth and health. This question was made popular by Hans Rosling’s Gapminder project.\n\n\n\nCollecting our data\nFirst, we need to collect our data. Following Rosling, we will use each country’s average life expectancy to measure its health and the country’s GDP per capita to measure its wealth. We will use wbstats::wb_data() to pull these data directly from the World Bank.\n\ngapminder_df &lt;- wb_data(\n  indicator = c(\"SP.DYN.LE00.IN\", \"NY.GDP.PCAP.CD\"),\n  start_date = 2016,\n  end_date = 2016\n) |&gt; \n  rename(\n    life_exp = SP.DYN.LE00.IN,\n    gdp_per_cap = NY.GDP.PCAP.CD\n  ) |&gt; \n  mutate(\n    log_gdp_per_cap = log(gdp_per_cap),\n    region = countrycode(country, \"country.name\", \"region\", custom_match = c(\"Turkiye\" = \"Europe & Central Asia\"))\n  ) |&gt; \n  relocate(region, .after = country)\n\ngapminder_df\n\n# A tibble: 217 × 8\n   iso2c iso3c country         region  date gdp_per_cap life_exp log_gdp_per_cap\n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;           &lt;chr&gt;  &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;           &lt;dbl&gt;\n 1 AW    ABW   Aruba           Latin…  2016      28451.     75.6           10.3 \n 2 AF    AFG   Afghanistan     South…  2016        520.     63.1            6.25\n 3 AO    AGO   Angola          Sub-S…  2016       1710.     61.1            7.44\n 4 AL    ALB   Albania         Europ…  2016       4124.     78.9            8.32\n 5 AD    AND   Andorra         Europ…  2016      39931.     NA             10.6 \n 6 AE    ARE   United Arab Em… Middl…  2016      41055.     79.3           10.6 \n 7 AR    ARG   Argentina       Latin…  2016      12790.     76.3            9.46\n 8 AM    ARM   Armenia         Europ…  2016       3680.     74.7            8.21\n 9 AS    ASM   American Samoa  East …  2016      13301.     NA              9.50\n10 AG    ATG   Antigua and Ba… Latin…  2016      15863.     78.2            9.67\n# ℹ 207 more rows\n\n\n\n\nWhat is the relationship between two variables?\nWhat is the relationship between a country’s average life expectancy and its GDP per capita? The easiest way to determine this is to visualize these two variables.\n\nggplot(gapminder_df, aes(x = gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  theme_minimal()\n\n\n\n\nThere seems to be a good case that there is a strong relationship between a country’s GDP per capita (wealth) and its average life expectancy (health).\nBecause we want to explore linear relationships at this stage of the course, we will look at the logged GDP per capita variable:\n\nggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  theme_minimal()\n\n\n\n\n\nYou can transform your data to make it easier to work with. Just remember that you now need to talk in terms of logged GDP per capita instead of GDP per capita.\n\nI can imagine drawing a straight line among these points that summarises how they vary with each other. It appears that as a country’s logged GDP per capita increases, so too does the average life expectancy of its population. As wealth increases, so too does health.\nWell, that was easy! What is the relationship between health and wealth? They increase with each other.\n\n\nHow can we measure the strength of that relationship?\nNow we need some way of measuring the strength of the relationship. In other words, what amount of the variation in countries’ average life expectancy is associated with variation in their GDP per capita? We can measure the strength of this association using correlations. The correlation coefficient tells us how closely variables relate to one another. It tells us both the strength and direction of the association.\n\nStrength: how closely are these values tied to one another. Measured from 0 to |1|, with values closer to 0 indicating a very weak relationship and values closer to |1| indicating a very strong relationship.\nDirection: do both \\(X\\) and \\(Y\\) change in the same direction? Positive correlations show that when \\(X\\) increases (decreases), so too does \\(Y\\). Negative correlations show that when \\(X\\) increases (decreases), \\(Y\\) decreases (increases). In other words, the move in different directions.\n\nWhat is the correlation between logged GDP per capita and life expectancy?\n\ncor(gapminder_df$log_gdp_per_cap, gapminder_df$life_exp, use = \"complete.obs\")\n\n[1] 0.8494337\n\n\nAs expected, the relationship is positive and strong.\n\n\nBuilding a generalizable description of this relationship\nWe have very quickly gained the skills to determine whether the relationship between two variables is positive, negative, or non-existent. We have also learnt how to describe the strength of that relationship. To that end, we are now able to describe the bivariate relationship between health and wealth as a positive and strong one.\nThis is useful, but we tend to need a more concrete way of describing the relationship between two variables. For example, what if a policy-maker comes up to you and asks what you think the effect of a $1,000 increase in a country’s GDP per capita will do to its average life expectancy? We can build simple models of this relationship to provide that policy-maker with a prediction of what we might expect to happen. Further, we can use the model to describe the relationship between these two variables in a generalized way. If a new country were to spring into existence, we can use our knowledge of its GDP per capita to determine how long we might expect its citizens to live.\n\n\nOLS and linear regression\nLooking back at our data, we can image a straight line running between each country’s plotted average life expectancy and GDP per capita. Let’s draw that line.\n\nggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  theme_minimal()\n\n\n\n\nWe can, of course, draw many different lines through these points. Each of us has probably drawn a slightly different line in our heads. Which is the best line? Ordinary least squares (OLS) regression provides an answer. Simply put, OLS regression draws the line that minimizes the distance between itself and all of the data points. That line can take many shapes, including a straight line, an S, a frowney face, and smiley face, etc.\nLooking at our data above, it appears that a straight line is the best line to draw.\n\nOverfitting involves fitting a model (or drawing a line through our data) that misses the forest for the trees. You can draw all kinds of shapes through those data that perhaps result in a smaller distance between itself and each dot. In fact, if you draw a line that connects all of those dots there will be no difference between your line and the data points. However, this model will be too focused on the data we have at hand. Our model will have no idea what to do with any new data points we introduce. This is bad! Your aim here is to produce a generalizable model of the relationship between these two variables, not to draw a line that connects this particular constellation of dots.\n\nOkay, so a straight line in the best type of line to draw. But there are still many, many different straight lines that we can draw. Which straight line is best? Remember, OLS regression finds the line that minimizes the distance between itself and all of the data points. Let’s step through this. Look at the graph above.\n\nDraw a line through those dots. Pick a line, any line!\nCalculate the distance between each dot and the line.\nSum up the absolute values of those distances. Remember, we just care about the distance, so we don’t need to worry about whether or not the dots are above or below the line.\nRepeat steps 1 - 3 many, many, many times.\nPick the line with the smallest sum of distances (or results from step 3).\n\nPhew, this seems tedious and I still might not draw the correct line. Happily, maths and R are to the rescue. Here is the line that minimizes those distances (all with the addition of one extra line of code).\n\nggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = F) + \n  theme_minimal()\n\n\n\n\n\n\nEstimating a linear model in R\nHow can we find this line? To answer this, we will first do some review.\nRemember the general equation for a line:\n\\[\ny = a + mx\n\\]\nRead this as: the value of \\(y\\) is the sum of some constant, \\(a\\), and some \\(x\\) variable that has been transformed by some slope value \\(m\\).\n\nRemember that the slope constant, \\(m\\), tells you how much \\(y\\) changes for every one unit increase in \\(x\\).\n\nSo, if:\n\\[\ny = 10 + 2x\n\\]\nThen, when \\(x = 20\\):\n\\[\ny = 10 + 2*20 = 50\n\\]\nFor many values of \\(x\\):\n\nggplot(tibble(x = 1:50, y = 10 + 2*x), aes(x = x, y = y)) + \n  geom_line(colour = \"lightgrey\", linewidth = 3) + \n  geom_point() + \n  theme_minimal()\n\n\n\n\nWell, let’s substitute in our variables of interest. Our \\(y\\) variable is a country’s average life expectancy and our \\(x\\) variable is that country’s logged GDP per capita.\n\\[\nlife Exp_x = \\beta_0 + \\beta_1 logGdpPerCap_x + \\epsilon\n\\]\nRead this as: a country’s average life expectancy is a function of some constant (\\(\\beta_0\\)) and its logged GDP per capita transformed by some value \\(\\beta_1\\) with some random error (\\(\\epsilon\\)), on average.\nLet’s imagine that this relationship is accurately described by the following formula:\n\\[\nlife Exp_x = 30 + 4 * logGdpPerCap_x\n\\]\n\nWe will get to that pesky error term in just a minute.\n\nThen, our model would predict the following average life expectancy for countries with log GDPs per capita between 0 and 20:\n\nggplot(\n  tibble(log_gdp_per_cap = 1:20, life_exp = 30 + 4*log_gdp_per_cap), \n  aes(x = log_gdp_per_cap, y = life_exp)\n) + \n  geom_line(colour = \"lightgrey\", linewidth = 3) + \n  geom_point() + \n  theme_minimal()\n\n\n\n\nA country with a logged GDP per capita of 5 (the equivalent of a GDP per capita of $148.41) has a predicted average life expectancy of 50 years, or \\(30 + 4*5\\).\nA country with a logged GDP per capita of 10 (the equivalent of a GDP per capita of $22,026.47) has a predicted average life expectancy of 70 years, or \\(30 + 4*10\\).\nDoes this accurately describe what we see in our data? What is the average life expectancy for countries with roughly $22,000 GDP per capita?\n\ncountries_10 &lt;- filter(gapminder_df, gdp_per_cap &gt; 21000 & gdp_per_cap &lt; 23000)\n\ncountries_10\n\n# A tibble: 3 × 8\n  iso2c iso3c country          region  date gdp_per_cap life_exp log_gdp_per_cap\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;            &lt;chr&gt;  &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;           &lt;dbl&gt;\n1 BH    BHR   Bahrain          Middl…  2016      22867.     79.6           10.0 \n2 KN    KNA   St. Kitts and N… Latin…  2016      21095.     71.7            9.96\n3 SI    SVN   Slovenia         Europ…  2016      21678.     81.2            9.98\n\n\nWe predicted 70 years, but our data suggest that these countries have closer to an average of 77 years. Why do we have this difference?\nWell, we probably haven’t produced the best model we can (this isn’t the best line!). We just picked those numbers out of thin air. Let’s fit a linear OLS regression and see if we improve our ability to predict what we have seen in the wild.\n\nHow do we calculate the constant (\\(\\beta_0\\)) using OLS regression?\nRemember, OLS regression simply finds the line that minimizes the distance between itself and all the data points. The constant that minimizes this distance is the mean of \\(Y\\) minus \\(\\beta_1\\) times the mean of \\(X\\).\nSo, the constant that best predicts a country’s average life expectancy based on its logged GDP per capita is equal to the average life expectancy across our sample (72.3 years) minus the average logged GDP per capita ($8.80, or $6,633.36 GDP per capita) transformed by \\(\\beta_1\\).\nSo…\n\n\nHow do we calculate the coefficient \\(\\beta_1\\)?\nThe regression slope is the correlation coefficient between \\(X\\) and \\(Y\\) multiplied by the standard deviation of \\(Y\\) divided by the standard deviation of \\(X\\).\nEw… Let’s step through that.\nRemember, the correlation coefficient simply measures how \\(X\\) and \\(Y\\) change together. Does \\(Y\\) increase when \\(X\\) increases? How strong is this relationship?\nThe standard deviations of \\(X\\) and \\(Y\\) just measure how spread out they are.\nBringing these together, we are interested in how much \\(X\\) and \\(Y\\) change together moderated by how much they change independently of each other.\nFormally:\n\\[\n\\beta_1 = (\\frac{\\Sigma(\\frac{x_i - \\bar{x}}{s_X})(\\frac{y_i - \\bar{y}}{s_Y})}{n - 1})(\\frac{s_Y}{s_X}) = \\frac{\\Sigma(x_i - \\bar{x})(y_i - \\bar{y})}{\\Sigma(x_i - \\bar{x})^2}\n\\]\nHappily R does all of this for us.\n\n\nLet’s fit that model already!\n\nm &lt;- lm(life_exp ~ log_gdp_per_cap, data = gapminder_df)\n\nm\n\n\nCall:\nlm(formula = life_exp ~ log_gdp_per_cap, data = gapminder_df)\n\nCoefficients:\n    (Intercept)  log_gdp_per_cap  \n         32.807            4.521  \n\n\nOkay, so the line of best fit describing the relationship between life expectancy and logged GDP per capita is:\n\\[\nlife Exp_x = 32.9 + 4.5 * logGdpPerCap_x + \\epsilon\n\\]\nThat’s it! We now have a generalized model of the relationship between a country’s average life expectancy and its logged GDP per capita. This model is informed by what we actually observed in the world. It carefully balances our need to accurately describe what we have observed and to develop something that is generalizable.\nThe above model output is difficult to read. It will not be accepted by any journal or professor. Luckily, we can use modelsummary::modelsummary() to easily generate a professionally formatted table.\n\nmodelsummary(\n  m, \n  statistic = NULL,\n  coef_rename = c(\"log_gdp_per_cap\" = \"GDP per capita (logged)\"),\n  gof_map = \"nobs\"\n)\n\n\n\n\n\nModel 1\n\n\n\n\n(Intercept)\n32.807\n\n\nGDP per capita (logged)\n4.521\n\n\nNum.Obs.\n203\n\n\n\n\n\n\n\nNote that OLS regression, particularly linear regression, requires that you make a lot of important assumptions about the relationship between your two variables. These were discussed in detail in the lecture. For example, we assume that the best line to fit is straight. We also assume that the best way to generate and describe the relationship across all observations is to fit the line that minimizes the distance between itself and the observed values or dots.\n\nThere are other approaches to determining the “best” line. These include maximum likelihood estimation (discussed in detail in GVPT729) and Bayesian statistics. We won’t discuss these approaches in this class or in GVPT722. It’s worth noting here; however, that OLS regression requires a whole bunch of assumptions that may or may not be appropriate to your research question or theory. This class prepares you to grapple with those questions and appropriately use these tools in your own research.\n\n\n\n\nPrediction and performance\nOkay, so we now have a model that describes the relationship between our outcome of interest (health) and our independent variable of interest (wealth). What can we do with this?\nFirst, we can use this model to predict a country’s average life expectancy given its GDP per capita.\nbroom::tidy(m) makes this model object a lot easier (tidier) to work with.\n\ntidy(m)\n\n# A tibble: 2 × 5\n  term            estimate std.error statistic  p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)        32.8      1.76       18.7 7.55e-46\n2 log_gdp_per_cap     4.52     0.198      22.8 1.04e-57\n\n\nWhat do we predict to be the average life expectancy of a country with a GDP per capita of $20,000? First, let’s pull out the estimated constant (or intercept or \\(\\beta_0\\)) for our calculations.\n\nm_res &lt;- tidy(m)\n\nbeta_0 &lt;- m_res |&gt; \n  filter(term == \"(Intercept)\") |&gt; \n  pull(estimate)\n\nbeta_0\n\n[1] 32.80653\n\n\nNext, let’s pull out the estimated coefficient for (logged) GDP per capita:\n\nbeta_1 &lt;- m_res |&gt; \n  filter(term == \"log_gdp_per_cap\") |&gt; \n  pull(estimate)\n\nbeta_1\n\n[1] 4.520775\n\n\nFinally, we can plug this in to our model:\n\\[\nlife Exp_x = \\beta_0 + \\beta_1 logGdpPerCap_x\n\\]\n\nlife_exp_20000 &lt;- beta_0 + beta_1 * log(20000)\nlife_exp_20000\n\n[1] 77.57797\n\n\nA country with a GDP per capita of $20,000 is predicted to have an average life expectancy of 78 years. Let’s take a look back at our data. Remember, these data describe what the World Bank actually observed for each country in 2016. How close is our predicted value to our observed values?\n\ncountries_10\n\n# A tibble: 3 × 8\n  iso2c iso3c country          region  date gdp_per_cap life_exp log_gdp_per_cap\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;            &lt;chr&gt;  &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;           &lt;dbl&gt;\n1 BH    BHR   Bahrain          Middl…  2016      22867.     79.6           10.0 \n2 KN    KNA   St. Kitts and N… Latin…  2016      21095.     71.7            9.96\n3 SI    SVN   Slovenia         Europ…  2016      21678.     81.2            9.98\n\n\nAs above, our data suggest that these countries have closer to an average of 77 years. Although our model predicted an average life expectancy closer to this than our guess above (which predicted 70 years), we still have a gap. Why?\nOur model is an attempt to formalize our understanding of the general relationship between a country’s wealth and health. Mapping our model against the observed values we used to generate it illustrates this point well.\n\nggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  geom_vline(xintercept = log(20000)) + \n  geom_hline(yintercept = life_exp_20000) + \n  geom_smooth(method = \"lm\", se = F) + \n  theme_minimal()\n\n\n\n\nThe world is a complicated and messy place. There are many countries that have a GDP per capita of around $20,000 (those dots sitting around the vertical black line). They have a wide range of average life expectancy: look at their various placement along that vertical line. Some are higher than others.\nAlso, there are several countries with a wide range of logged GDP per capita that have an average life expectancy of 78 years (those sitting at or around the horizontal black line). These have a wide range of logged GDP per capita: some are further to the left than others.\nOur model is our best attempt at accounting for that diversity whilst still producing a useful summary of the relationship between health and wealth for those countries and all other countries with all observed values of GDP per capita.\nA bit of noise (error) is expected. How much error is okay? This is a complicated question that has contested answers. Let’s start with actually measuring that error. Then we can chat about whether or not it’s small enough to allow us to be confident in our model.\n\n\nMeasuring error in our model\nReturning to our question above, how close are our predicted values to our observed values? For example, how far from the observed average life expectancy of countries with a GDP per capita of or close to $20,000 is 78 years?\nStart by working out the average life expectancy predicted by our model for the logged GDP per capita of all of our countries. We can then compare this to the average life expectancy actually observed in all these countries. We can predict values from a model using broom::augment():\n\naugment(m)\n\n# A tibble: 203 × 9\n   .rownames life_exp log_gdp_per_cap .fitted .resid    .hat .sigma   .cooksd\n   &lt;chr&gt;        &lt;dbl&gt;           &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n 1 1             75.6           10.3     79.2 -3.55  0.0103    4.11 0.00393  \n 2 2             63.1            6.25    61.1  2.06  0.0193    4.12 0.00251  \n 3 3             61.1            7.44    66.5 -5.37  0.00883   4.10 0.00767  \n 4 4             78.9            8.32    70.4  8.42  0.00533   4.08 0.0113   \n 5 6             79.3           10.6     80.8 -1.49  0.0132    4.12 0.000895 \n 6 7             76.3            9.46    75.6  0.751 0.00612   4.12 0.000104 \n 7 8             74.7            8.21    69.9  4.74  0.00558   4.10 0.00375  \n 8 10            78.2            9.67    76.5  1.62  0.00695   4.12 0.000549 \n 9 11            82.4           10.8     81.7  0.740 0.0150    4.12 0.000250 \n10 12            81.6           10.7     81.3  0.367 0.0141    4.12 0.0000576\n# ℹ 193 more rows\n# ℹ 1 more variable: .std.resid &lt;dbl&gt;\n\n\nThis function is simply fitting our model (\\(life Exp_x = 32.9 + 4.5 * logGdpPerCap_x\\)) to each country’s logged GDP per capita. You can confirm this by running the model yourself:\n\ngapminder_df |&gt; \n  transmute(\n    country,\n    log_gdp_per_cap,\n    .fitted = beta_0 + beta_1*log_gdp_per_cap\n  )\n\n# A tibble: 217 × 3\n   country              log_gdp_per_cap .fitted\n   &lt;chr&gt;                          &lt;dbl&gt;   &lt;dbl&gt;\n 1 Aruba                          10.3     79.2\n 2 Afghanistan                     6.25    61.1\n 3 Angola                          7.44    66.5\n 4 Albania                         8.32    70.4\n 5 Andorra                        10.6     80.7\n 6 United Arab Emirates           10.6     80.8\n 7 Argentina                       9.46    75.6\n 8 Armenia                         8.21    69.9\n 9 American Samoa                  9.50    75.7\n10 Antigua and Barbuda             9.67    76.5\n# ℹ 207 more rows\n\n\nHow did the model do? What is the difference between what it predicted and the country’s observed average life expectancy? Compare .fitted (the predicted average life expectancy) to life_exp (the actual observed average life expectancy).\n\nm_eval &lt;- augment(m) |&gt; \n  transmute(\n    life_exp, \n    .fitted,\n    diff = life_exp - .fitted\n  )\n\nm_eval\n\n# A tibble: 203 × 3\n   life_exp .fitted   diff\n      &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1     75.6    79.2 -3.55 \n 2     63.1    61.1  2.06 \n 3     61.1    66.5 -5.37 \n 4     78.9    70.4  8.42 \n 5     79.3    80.8 -1.49 \n 6     76.3    75.6  0.751\n 7     74.7    69.9  4.74 \n 8     78.2    76.5  1.62 \n 9     82.4    81.7  0.740\n10     81.6    81.3  0.367\n# ℹ 193 more rows\n\n\nNote that broom::augment() already did this calculation and stored it in the .resid variable. The formal term for the difference between the predicted and observed values is the residual.\n\naugment(m) |&gt; \n  select(life_exp, .fitted, .resid)\n\n# A tibble: 203 × 3\n   life_exp .fitted .resid\n      &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1     75.6    79.2 -3.55 \n 2     63.1    61.1  2.06 \n 3     61.1    66.5 -5.37 \n 4     78.9    70.4  8.42 \n 5     79.3    80.8 -1.49 \n 6     76.3    75.6  0.751\n 7     74.7    69.9  4.74 \n 8     78.2    76.5  1.62 \n 9     82.4    81.7  0.740\n10     81.6    81.3  0.367\n# ℹ 193 more rows\n\n\nOkay, so there are some differences. Let’s look at those differences a bit more closely:\n\nggplot(augment(m), aes(x = .resid)) + \n  geom_density() + \n  geom_vline(xintercept = 0) + \n  theme_minimal()\n\n\n\n\nIf our model perfectly predicted each country’s life expectancy, we would see no difference between the predicted and observed values. There would just be a very tall straight line at 0 on the graph above.\nOur model hasn’t predicted life expectancy perfectly. Whilst most predictions are within a couple of years of the country’s true life expectancy, there are some that are very different (up to 10 or 15 years!). Where the model has got it wrong, it has tended to overestimate life expectancy (note that the peak of the density curve sits above 0).\nCan you see for which points these large differences exist?\n\nggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = F) + \n  theme_minimal()\n\n\n\n\nWhat is causing these differences? A lot of your work as a political scientist is trying to answer this very question!\n\n(Random) error\nThe world is a messy and complicated place. Things often vary in random ways. That’s okay! It means that your observational data are going to move in funny and random ways. That’s okay too! As long as your model includes all of the systematic drivers of the thing you are interested in measuring (such as average life expectancy), we can accept a bit of random error.\nIn fact, we have already accounted for this. Remember that error term:\n\\[\nlife Exp_x = \\beta_0 + \\beta_1 logGdpPerCap_x + \\epsilon\n\\]\nWe run into issues when there are non-random things bundled up into the difference between what our model predicts and what we actually observe. We will discuss this more in later classes.\n\n\n\nA model-wide value for error\nWe often want to understand how the model has performed as a whole, rather than how well it predicts each individual observed data point. There are many different ways we can do this.\n\nSum of squared residuals (deviance)\nThe sum of squared residuals measures the total error in our model. Formally:\n\\[\n\\Sigma(y_i - \\hat{y_i})^2\n\\]\nWhere \\(y_i\\) is each predicted value (the model’s estimate of country’s average life expectancy) and \\(\\hat{y_i}\\) is each observed value (the country’s actual average life expectancy).\nWe just add those all up to get a single measure of the model’s overall performance.\n\nRemember that we tend to square things when we don’t care about the direction. We don’t care that the predicted value is less or more than the observed value, just about how far they are from each other.\n\nWe can do this ourselves:\n\naugment(m) |&gt; \n  summarise(sum(.resid^2))\n\n# A tibble: 1 × 1\n  `sum(.resid^2)`\n            &lt;dbl&gt;\n1           3392.\n\n\nOr we can use broom::glance():\n\nglance(m)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.722         0.720  4.11      521. 1.04e-57     1  -574. 1154. 1164.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\nglance(m) |&gt; \n  select(deviance)\n\n# A tibble: 1 × 1\n  deviance\n     &lt;dbl&gt;\n1    3392.\n\n\n\nWhere broom::tidy() gives us information about the coefficients of our model, broom::glance() gives us information on the overall model performance.\n\nThis is useful, but it is influenced by the units by which we measure our variables. If one model includes something like GDP which is measured in terms of billions of dollars, we will get a very large sum of squared residuals. If another model includes something like percentage of as state’s citizens who will vote for Donald Trump, we will get a relatively small sum of squared residuals. What if we want to compare model performance in a meaningful way?\n\n\n\\(R^2\\)\nThe \\(R^2\\) value measures the amount of variation in the dependent variable that is explained by the independent variable. In our example, it measures how much the changes in countries’ average life expectancy is explained by the changes in their (logged) GDP per capita. The \\(R^2\\) value is useful because it does not reflect the units of measurement used in our variables. Therefore, we can compare how well different models perform.\nThe \\(R^2\\) value has three component parts.\n\nTotal Sum of Squares (TSS)\nTSS measures the squared sum of the differences between all predicted values of the dependent variable and the mean of the dependent variable.\n\n\nExplained Sum of Squares (ESS)\nESS measures the sum of the squares of the deviations of the predicted values from the mean value of the dependent variable.\n\n\nResidual Sum of Squares (RSS)\nRSS measures the difference between the TSS and ESS. In other words, the error not explained by the model.\nFormally, the \\(R^2\\) value is:\n\\[\nR^2 = 1 - \\frac{RSS}{TSS} = 1 - \\frac{\\Sigma(y_i - \\hat{y_i})^2}{\\Sigma(y_i - \\hat{y})^2}\n\\]\nOr:\n\\[\nR^2 = \\frac{ESS}{TSS} = \\frac{\\Sigma(\\hat{y}_i - \\bar{y})^2}{\\Sigma(y_i - \\bar{y})^2}\n\\]\nOur model’s \\(R^2\\) can be accessed using broom::glance():\n\nglance(m) |&gt; \n  select(r.squared)\n\n# A tibble: 1 × 1\n  r.squared\n      &lt;dbl&gt;\n1     0.722\n\n\n\n\n\n\nModelling relationships among categorical variables\nSometimes we want to know whether our outcome of interest changes based on the category in which it sits. For example, do levels of support for abortion access differ between Democrats, Republicans, and Independents? Do the number of women elected to parliament change based on whether or not the country has a formal quota? Do the number of civilians targeted in war change based on whether the war is intra- or inter-state?\nLet’s return to the American National Election Survey first explored last week. We will focus on that first question: do levels of support for abortion access differ between Democrats, Republicans, and Independents?\nWe can access the 2012 survey through R using the poliscidata package:\npoliscidata::nes\n\nCross tabs\nA simple cross tab can provide a nice summary of differences in your outcome of interest across your categories.\nFor example, let’s look at differences in the number of individuals who identified as Democrat, Republican, or Independent who do not support access to abortions, support access with some conditions, with more conditions, or always.\nWe can use modelsummary::datasummary_crosstab() to produce a nicely formatted cross tab of our variables:\n\ndatasummary_crosstab(abort4 ~ pid_3, data = nes)\n\n\n\n\nabort4\n\nDem\nInd\nRep\nAll\n\n\n\n\nNever\nN\n187\n229\n252\n672\n\n\n\n% row\n27.8\n34.1\n37.5\n100.0\n\n\nSome conds\nN\n499\n583\n519\n1607\n\n\n\n% row\n31.1\n36.3\n32.3\n100.0\n\n\nMore conds\nN\n332\n337\n227\n898\n\n\n\n% row\n37.0\n37.5\n25.3\n100.0\n\n\nAlways\nN\n1325\n964\n381\n2680\n\n\n\n% row\n49.4\n36.0\n14.2\n100.0\n\n\nAll\nN\n2358\n2149\n1385\n5916\n\n\n\n% row\n39.9\n36.3\n23.4\n100.0\n\n\n\n\n\n\n\n\n\nMean comparison table\nWe can use mean comparison tables to, well, compare means (in a table). Let’s compare the average response to the feeling thermometer (scale from 0 to 100) for the Republican party across parties:\n\nnes |&gt;\n  group_by(pid_3) |&gt; \n  summarise(\n    mean = mean(ft_rep, na.rm = T),\n    sd = sd(ft_rep, na.rm = T),\n    freq = n()\n  )\n\n# A tibble: 4 × 4\n  pid_3  mean    sd  freq\n  &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 Dem    24.5  21.8  2358\n2 Ind    43.0  23.1  2149\n3 Rep    70.3  18.3  1385\n4 &lt;NA&gt;   50    14.7    24\n\n\nAre these counts or averages meaningfully different from one another? We need some additional tools to answer that question. We will discuss those in the coming weeks.\n\n\nLooking at the whole distribution\nWe can also visualize the whole distribution of a continuous variable of interest within our categories.\n\nnes |&gt;\n  drop_na(pid_3) |&gt; \n  ggplot(aes(x = ft_rep, fill = pid_3)) + \n  geom_density(alpha = 0.5) + \n  theme_minimal()\n\n\n\n\nAs expected, Democrats appear to respond least favourably to the Republican Party, followed by Independents, and Republicans. This is demonstrated by both the mean comparison table and density plots.\n\n\n\nNext week\nWe will discuss research design."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\n            Quantitative Methods for Political Science\n        ",
    "section": "",
    "text": "Quantitative Methods for Political Science\n        \n        \n            An introduction to research methods and quantitative research in political science.\n        \n        \n            Fall 2023Department of Government and PoliticsUniversity of Maryland, College Park\n        \n    \n    \n      \n        \n        \n        \n      \n    \n\n\n\n\n\n\nProfessor\n\n   Dr David Cunningham\n   dacunnin@umd.edu\n\n\n\nTeaching Assistant\n\n   Harriet Goers\n   Chincoteague Building\n   hgoers@umd.edu\n   hgoers\n\n\n\n\n\nCourse details\n\n   August 28 - 11 December\n   Monday, 12:30 - 3:15 PM\n   Tydings Building, Room 1111\n\n\n\nLab details\n\n   Friday, 3:00 - 5:00 PM\n   LeFrak Hall, Room 0227\n\n\n\n\nContacting me\nE-mail is the best ways to get in contact with me. I will try to respond to all course-related e-mails within 24 hours."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "DATE\nTITLE\nCLASS NOTES\nSESSION SCRIPTS\nASSIGNMENTS\n\n\n\n\n2023-08-28\nCourse Introduction\n\n\n\n\n\n2023-09-04\nNo class\n\n\n\n\n\n2023-09-11\nDescriptive Statistics\n\n\n\n\n\n2023-09-18\nRelationships Between Two Variables\n\n\n\n\n\n2023-09-25\nResearch Design\n\n\n\n\n\n2023-10-02\nApplications & Midterm Exam Review\n\n\n\n\n\n2023-10-09\nProbability Theory\n\n\n\n\n\n2023-10-16\nHypothesis Testing I\n\n\n\n\n\n2023-10-23\nHypothesis Testing II\n\n\n\n\n\n2023-10-30\nHypothesis Testing III\n\n\n\n\n\n2023-11-06\nApplications & Midterm Exam Review\n\n\n\n\n\n2023-11-13\nRegression Analysis I\n\n\n\n\n\n2023-11-20\nNo class\n\n\n\n\n\n2023-11-27\nRegression Analysis II\n\n\n\n\n\n2023-12-04\nRegression Analysis Extensions & Final Exam Review"
  }
]