---
title: "Bivariate Relationships"
execute: 
  warning: false
  message: false
  echo: true
  fig-width: 10
---

## Readings

## Class slides

```{=html}
<iframe class="slide-deck" src="../files/slides/Lecture 3- Relationships between Variables.pdf" width = "100%" height = 600px></iframe>
```
## Section

### Prerequisites

```{r}
#| echo: false

library(scales)
```

```{r}
library(tidyverse)
library(wbstats)
library(poliscidata)
library(countrycode)
library(broom)
library(janitor)
library(ggridges)
library(modelsummary)
```

### How will what you learn this week help your research?

As usual, we start with an interesting question. You have some outcome of interest and you think that there is an important determinant of that outcome that no has yet identified. Or perhaps the relationship between some heavily chewed over determinant and the outcome of interest is misunderstood. We are steadily building up your ability to determine empirically the relationship between that determinant and the outcome of interest.

Simply put (and there really is no need to over-complicate this), we have two or more variables: an outcome of interest (the dependent variable) and a set of independent variables that we theorize are important determinants of that outcome. We can use empirical analysis to understand 1) how the dependent and independent variables change in relation to one another, and 2) whether this relationship is strong enough that we should declare (though 12,000-word journal articles or by shouting from rooftops) that whenever we want to change or understand that outcome, we *must* consider these important independent variables.

Last week, we discussed various tools that you can use to explore your data. You can develop a very good understanding of each of the individual variables. This exploration is very important for building your intuition and, by extension, your expertise in the question at hand. These tools also allow you to identify unusual data points (or outliers).

This week, we will make the next step. We will explore how two variables relate to each other. How do they move with each other: when one goes up, does the other go down, up, or not really move? How strong is this association?

This exploration is particularly important for you to do with regard to the independent variable(s) that are the focal point of your theory and, therefore, your contribution to our understanding of the messy spaghetti bowl of things that determine your outcome of interest. Just as it is important for you to spend some time understanding the shape of your variables (using the tools we discussed last week), you must start to understand the shape of the relationship between the outcome you are trying to understand or predict and the factors you think are important determinants of that outcome.

Let's begin!

### Bivariate relationships

How do two variables move with one another? When when goes up, does the other go down, up, or not really move at all? How dramatic is this shift?

The type of variables we have determines how we can answer this question. To begin, we will explore the relationship between two continuous variables. Later in the class, we will look at how to explore the relationship between a continuous and categorical variable.

To start, we will explore the relationship between wealth and health. This question was made popular by Hans Rosling's Gapminder project.

{{< video https://www.youtube.com/embed/jbkSRLYSojo >}}

### Collecting our data

First, we need to collect our data. Following Rosling, we will use each country's average life expectancy to measure its health and the country's GDP per capita to measure its wealth. We will use `wbstats::wb_data()` to pull these data directly from the World Bank.

```{r}

gapminder_df <- wb_data(
  indicator = c("SP.DYN.LE00.IN", "NY.GDP.PCAP.CD"),
  start_date = 2016,
  end_date = 2016
) |> 
  rename(
    life_exp = SP.DYN.LE00.IN,
    gdp_per_cap = NY.GDP.PCAP.CD
  ) |> 
  mutate(
    log_gdp_per_cap = log(gdp_per_cap),
    region = countrycode(country, "country.name", "region", custom_match = c("Turkiye" = "Europe & Central Asia"))
  ) |> 
  relocate(region, .after = country)

gapminder_df
```

### What is the relationship between two variables?

What is the relationship between a country's average life expectancy and its GDP per capita? The easiest way to determine this is to visualize these two variables.

```{r}
ggplot(gapminder_df, aes(x = gdp_per_cap, y = life_exp)) + 
  geom_point() + 
  theme_minimal()
```

There seems to be a good case that there is a strong relationship between a country's GDP per capita (wealth) and its average life expectancy (health).

Because we want to explore linear relationships at this stage of the course, we will look at the logged GDP per capita variable:

```{r}
ggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp)) + 
  geom_point() + 
  theme_minimal()
```

> You can transform your data to make it easier to work with. Just remember that you now need to talk in terms of logged GDP per capita instead of GDP per capita.

I can imagine drawing a straight line among these points that summarises how they vary with each other. It appears that as a country's logged GDP per capita increases, so too does the average life expectancy of its population. As wealth increases, so too does health.

### How can we measure the strength of that relationship?

Well, that was easy! What is the relationship between health and wealth? They increase with each other.

Now we need some way of measuring the strength of the relationship. In other words, what amount of the variation in countries' average life expectancy are associated with variation in their GDP per capita? We can measure the strength of this association using correlations. The **correlation coefficient** tells us how closely variables relate to one another. It tells us both the strength and direction of the association.

-   Strength: how closely are these values tied to one another. Measured from 0 to \|1\|, with values closer to 0 indicating a very weak relationship and values closer to \|1\| indicating a very strong relationship.

-   Direction: do both $X$ and $Y$ change in the same direction? Positive correlations show that when $X$ increases (decreases), so too does $Y$. Negative correlations show that when $X$ increases (decreases), $Y$ decreases (increases). In other words, the move in different directions.

What is the correlation between logged GDP per capita and life expectancy?

```{r}
cor(gapminder_df$log_gdp_per_cap, gapminder_df$life_exp, use = "complete.obs")
```

As expected, the relationship is positive and strong.

### Building a generalizable description of this relationship

We have very quickly gained the skills to determine whether the relationship between two variables is positive, negative, or non-existent. We have also learnt how to describe the strength of that relationship. To that end, we are now able to describe the bivariate relationship between health and wealth as a positive and strong one.

This is useful, but we tend to need a more concrete way of describing the relationship between two variables. For example, what if a policy-maker comes up to you and asks what you think the effect of a \$1,000 increase in a country's GDP per capita will do to its average life expectancy? We can build simple models of this relationship to provide that policy-maker with a *prediction* of what we might *expect* to happen. Further, we can use the model to describe the relationship between these two variables in a generalized way. If a new country were to spring into existence, we can use our knowledge of its GDP per capita to determine how long we might expect its citizens to live.

### OLS and linear regression

Looking back at our data, we can image a straight line running between each country's plotted average life expectancy and GDP per capita. Let's draw that line.

```{r}
ggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp)) + 
  geom_point() + 
  theme_minimal()
```

We can, of course, draw many different lines through these points. Each of us has probably drawn a slightly different line in our heads. Which is the best line? **Ordinary least squares (OLS) regression** provides an answer. Simply put, OLS regression draws the line that minimizes the distance between itself and all of the data points. That line can take many shapes, including a straight line, an S, a frowney face, and smiley face, etc.

Looking at our data above, it appears that a straight line is the best line to draw.

> **Overfitting** involves fitting a model (or drawing a line through our data) that misses the forest for the trees. You can draw all kinds of shapes through those data that perhaps result in a smaller distance between itself and each dot. In fact, if you draw a line that connects all of those dots there will be no difference between your line and the data points. However, this model will be too focused on the data we have at hand. Our model will have no idea what to do with any new data points we introduce. This is bad! Your aim here is to produce a generalizable model of the relationship between these two variables, not to draw a line that connects this particular constellation of dots.

Okay, so a straight line in the best type of line to draw. But there are still many, many different straight lines that we can draw. Which straight line is best? Remember, OLS regression finds the line that minimizes the distance between itself and all of the data points. Let's step through this. Look at the graph above.

1.  Draw a line through those dots. Pick a line, any line!

2.  Calculate the distance between each dot and the line.

3.  Sum up the absolute values of those distances. Remember, we just care about the distance, so we don't need to worry about whether or not the dots are above or below the line.

4.  Repeat steps 1 - 3 many, many, many times.

5.  Pick the line with the **smallest** sum of distances (or results from step 3).

Phew, this seems tedious and I still might not draw the correct line. Happily, maths and R are to the rescue. Here is the line that minimizes those distances (all with the addition of one extra line of code).

```{r}
ggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = F) + 
  theme_minimal()
```

### Estimating a linear model in R

How can we find this line? To answer this, we will first do some review.

Remember the general equation for a line:

$$
y = a + mx
$$

Read this as: the value of $y$ is the sum of some constant, $a$, and some $x$ variable that has been transformed by some slope value $m$.

> Remember that the slope constant, $m$, tells you how much $y$ changes for every one unit increase in $x$.

So, if:

$$
y = 10 + 2x
$$

Then, when $x = 20$:

$$
y = 10 + 2*20 = 50
$$

For many values of $x$:

```{r}
ggplot(tibble(x = 1:50, y = 10 + 2*x), aes(x = x, y = y)) + 
  geom_line(colour = "lightgrey", size = 3) + 
  geom_point() + 
  theme_minimal()
```

Well, let's substitute in our variables of interest. Our $y$ variable is a country's average life expectancy and our $x$ variable is that country's logged GDP per capita.

$$
life Exp_x = \beta_0 + \beta_1 logGdpPerCap_x + \epsilon
$$

Read this as: a country's average life expectancy is a function of some constant ($\beta_0$) and its logged GDP per capita transformed by some value $\beta_1$ with some random error ($\epsilon$).

Let's imagine that this relationship is accurately described by the following formula:

$$
life Exp_x = 30 + 4 * logGdpPerCap_x
$$

> We will get to that pesky error term in just a minute.

Then, our model would predict the following average life expectancy for countries with log GDPs per capita between 0 and 20:

```{r}
ggplot(
  tibble(log_gdp_per_cap = 1:20, life_exp = 30 + 4*log_gdp_per_cap), 
  aes(x = log_gdp_per_cap, y = life_exp)
) + 
  geom_point() + 
  theme_minimal()
```

A country with a logged GDP per capita of 5 (the equivalent of a GDP per capita of `r exp(5) |> scales::dollar()`) has a predicted average life expectancy of 50 years, or $30 + 4*5$.

A country with a logged GDP per capita of 10 (the equivalent of a GDP per capita of `r exp(10) |> scales::dollar()`) has a predicted average life expectancy of 70 years, or $30 + 4*10$.

Does this accurately describe what we see in our data? What is the average life expectancy for countries with roughly \$22,000 GDP per capita?

```{r}
countries_10 <- filter(gapminder_df, gdp_per_cap > 21000 & gdp_per_cap < 23000)

countries_10
```

We predicted 70 years, but our data suggest that these countries have closer to an average of `r round(mean(countries_10$life_exp))` years. Why do we have this difference?

Well, we probably haven't produced the best model we can (this isn't the best line!). We just picked those numbers out of thin air. Let's fit a linear OLS regression and see if we improve our ability to predict what we have seen in the wild.

#### How do we calculate the constant ($\beta_0$) using OLS regression?

Remember, OLS regression simply finds the line that minimizes the distance between itself and all the data points. The constant that minimizes this distance is the mean of $Y$ minus $\beta_1$ times the mean of $X$.

So, the constant that best predicts a country's average life expectancy based on its logged GDP per capita is equal to the average life expectancy across our sample (`r mean(gapminder_df$life_exp, na.rm = T) |> round(2)` years) minus the average logged GDP per capita (`r mean(gapminder_df$log_gdp_per_cap, na.rm = T) |> dollar()`, or `r mean(gapminder_df$log_gdp_per_cap, na.rm = T) |> exp() |> dollar()` GDP per capita) transformed by $\beta_1$.

So...

#### How do we calculate the coefficient $\beta_1$?

The regression slope is the correlation coefficient between $X$ and $Y$ multiplied by the standard deviation of $Y$ divided by the standard deviation of $X$.

Ew... Let's step through that.

Remember, the correlation coefficient simply measures how $X$ and $Y$ change together. Does $Y$ increase when $X$ increases? How strong is this relationship?

The standard deviations of $X$ and $Y$ just measure how spread out they are.

Bringing these together, we are interested in how much $X$ and $Y$ change together moderated by how much they change independently of each other.

Formally:

$$
\beta_1 = (\frac{\Sigma(\frac{x_i - \bar{x}}{s_X})(\frac{y_i - \bar{y}}{s_Y})}{n - 1})(\frac{s_Y}{s_X}) = \frac{\Sigma(x_i - \bar{x})(y_i - \bar{y})}{\Sigma(x_i - \bar{x})^2}
$$

Happily R does all of this for us.

#### Let's fit that model already!

```{r}
m <- lm(life_exp ~ log_gdp_per_cap, data = gapminder_df)

m
```

Okay, so the line of best fit describing the relationship between life expectancy and logged GDP per capita is:

$$
life Exp_x = 32.9 + 4.5 * logGdpPerCap_x + \epsilon
$$

That's it! We now have a generalized model of the relationship between a country's average life expectancy and its logged GDP per capita. This model is informed by what we actually observed in the world. It carefully balances our need to accurately describe what we have observed and the need to develop something that is generalizable.

The above model output is difficult to read. It will not be accepted by any journal or professor. Luckily, we can use `modelsummary::modelsummary()` to easily generate a professionally formatted table.

```{r}
modelsummary(
  m, 
  statistic = NULL,
  coef_rename = c("log_gdp_per_cap" = "GDP per capita (logged)")
)
```

Note that OLS regression, particularly linear regression, involves a lot of important assumptions. These were discussed in detail in the lecture. For example, we assume that the best line to fit is straight. We also assume that the best way to generate and describe the relationship across all observations is to fit the line that minimizes the distance between itself and the observed values or dots.

> There are other approaches to determining the "best" line. These include maximum likelihood estimation (discussed in detail in GVPT729) and Bayesian statistics. We won't discuss these approaches in this class or in GVPT722. It's worth noting here; however, that OLS regression requires a whole bunch of assumptions that may or may not be appropriate to your research question or theory. This class prepares you to grapple with those questions and appropriately use these tools in your own research.

### Prediction and performance

Okay, so we now have a model that describes the relationship between our outcome of interest (health) and our independent variable of interest (wealth). What can we do with this?

First, we can use this model to predict a country's average life expectancy given its GDP per capita.

`broom::tidy(m)` makes this model object a lot easier to work with.

```{r}
tidy(m)
```

What do we predict to be the average life expectancy of a country with a GDP per capita of \$10,000? First, let's pull out the estimated constant (or intercept or $\beta_0$) for our calculations.

```{r}
m_res <- tidy(m)

beta_0 <- m_res |> 
  filter(term == "(Intercept)") |> 
  pull(estimate)

beta_0
```

Next, let's pull out the estimated coefficient for (logged) GDP per capita:

```{r}
beta_1 <- m_res |> 
  filter(term == "log_gdp_per_cap") |> 
  pull(estimate)

beta_1
```

Finally, we can plug this in to our model:

$$
life Exp_x = \beta_0 + \beta_1 logGdpPerCap_x
$$

```{r}
life_exp_10000 <- beta_0 + beta_1 * log(10000)
life_exp_10000
```

A country with a GDP per capita of \$10,000 is predicted to have an average life expectancy of `r round(life_exp_10000)` years. Let's take a look back at our data. Remember, these data describe what the World Bank actually observed for each country in 2016. How close is our predicted value to our observed values?

```{r}

countries_10
```

As above, our data suggest that these countries have closer to an average of `r round(mean(countries_10$life_exp))` years. Although our model predicted an average life expectancy closer to this than our guess above (which predicted 70 years), we still have a gap. Why?

```{r}
ggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp)) + 
  geom_point() + 
  geom_vline(xintercept = log(10000)) + 
  geom_hline(yintercept = life_exp_10000) + 
  geom_smooth(method = "lm", se = F) + 
  theme_minimal()
```

Our model is an attempt to solidify our understanding of the general relationship between a country's wealth and health. Mapping our model against the observed values we used to generate it illustrates this point well.

The world is a complicated and messy place. There are many countries that have a GDP per capita of around \$10,000 (those dots sitting around the vertical black line). They have a wide range of average life expectancy. Also, there are several countries with a wide range of logged GDP per capita that have an average life expectancy of `r round(life_exp_10000)` years (those sitting at or around the horizontal black line).

Our model is our best attempt at accounting for that diversity whilst still producing a useful summary of the relationship between health and wealth for those countries and all other countries with all observed values of GDP per capita.

A bit of noise (error) is expected. How much error is okay? This is a complicated question that has contested answers. Let's start with actually measuring that error. Then we can chat about whether or not it's small enough to allow us to be confident in our model.

### Measuring error in our model

Returning to our question above, how close are our predicted values to our observed values? For example, how far from the observed average life expectancy of countries with a GDP per capita of or close to \$10,000 is `r round(life_exp_10000)` years?

Start by working out the average life expectancy predicted by our model for the logged GDP per capita of all of our countries. We can then compare this to the average life expectancy actually observed in all these countries. We can predict values from a model using `broom::augment()`:

```{r}
augment(m)
```

This function is simply fitting our model ($life Exp_x = 32.9 + 4.5 * logGdpPerCap_x$) to each country's logged GDP per capita.

How did the model do? What is the difference between what it predicted and the country's observed average life expectancy? Compare `.fitted` (the predicted average life expectancy) to `life_exp` (the actual observed average life expectancy).

```{r}
m_eval <- augment(m) |> 
  transmute(
    life_exp, 
    .fitted,
    diff = life_exp - .fitted
  )

m_eval
```

Note that `broom::augment()` already did this calculation and stored it in the `.resid` variable. The formal term for the difference between the predicted and observed values is the **residual**.

```{r}
augment(m) |> 
  transmute(
    life_exp, 
    .fitted,
    diff = life_exp - .fitted,
    .resid
  )
```

Okay, so there are some differences. Let's look at those difference a bit more closely:

```{r}
ggplot(augment(m), aes(x = .resid)) + 
  geom_density() + 
  geom_vline(xintercept = 0) + 
  theme_minimal()
```

If our model perfectly predicted each country's life expectancy, we would see no difference between the predicted and observed values. There would just be a very tall straight line at 0 on the graph above.

Our model hasn't predicted life expectancy perfectly. Whilst most predictions are within a couple of years of the country's true life expectancy, there are some that are very different (up to 10 or 15 years!). Where the model has got it wrong, it has tended to overestimate life expectancy (note that the peak of the density curve sits above 0).

Can you see for which points these large differences exist?

```{r}
ggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = F) + 
  theme_minimal()
```

What is causing these differences? A lot of your work as a political scientist is trying to answer this very question!

#### (Random) error

The world is a messy and complicated place. Things often vary in random ways. That's okay! It means that your observational data are going to move in funny and random ways. That's okay too! As long as your model includes all those systematic drivers of the thing you are interested in measuring (such as average life expectancy), we can accept a bit of random error.

In fact, we have already accounted for this. Remember that error term:

$$
life Exp_x = \beta_0 + \beta_1 logGdpPerCap_x + \epsilon
$$

We run into issues when these are non-random things bundled up into the difference between what our model predicts and what we actually observe. We will discuss this more in later classes.

### A model-wide value for error

We often want to understand how the model has performed as a whole, rather than how well it predicts each individual observed data point. There are many different ways we can do this.

#### Sum of squared residuals

The sum of squared residuals measures the total error in our model. Formally:

$$
\Sigma(y_i - \hat{y_i})^2
$$

Where $y_i$ is each predicted value (the model's estimate of country's average life expectancy) and $\hat{y_i}$ is each observed value (the country's actual average life expectancy).

We just add those all up to get a single measure of the model's overall performance.

> Remember that we tend to square things when we don't care about the direction. We don't care that the predicted value is less or more than the observed value, just about how far they are from each other.

This is useful, but influenced by the units by which we measure our variables. If one model includes something like GDP which is measured in terms of billions of dollars, we will get a very large sum of squared residuals. If another model includes something like percentage of as state's citizens who will vote for Donald Trump, we will get a relatively small sum of squared residuals. What if we want to compare model performance in a meaningful way?

#### $R^2$

The $R^2$ value measures the amount of variation in the dependent variable that is explained by the independent variable. In our example, it measures how much the changes in countries' average life expectancy is explained by the changes in their (logged) GDP per capita. The $R^2$ value is useful because it does not reflect the units of measurement used in our variables. Therefore, we can compare how well different models perform.

The $R^2$ value has three component parts.

##### Total Sum of Squares (TSS)

TSS measures the squared sum of the differences between all predicted values of the dependent variable and the mean of the dependent variable.

##### Explained Sum of Squares (ESS)

ESS measures the sum of the squares of the deviations of the predicted values from the mean value of the dependent variable.

##### Residual Sum of Squares (RSS)

RSS measures the difference between the TSS and ESS. In other words, the error not explained by the model.

Formally, the $R^2$ value is:

$$
R^2 = 1 - \frac{RSS}{TSS} = 1 - \frac{\Sigma(y_i - \hat{y_i})^2}{\Sigma(y_i - \hat{y})^2}
$$

Or:

$$
R^2 = \frac{ESS}{TSS} = \frac{\Sigma(\hat{y}_i - \bar{y})^2}{\Sigma(y_i - \bar{y})^2}
$$

Our model's $R^2$ was one of the many values provided at the bottom of our nicely formatted regression table above. We can access it (and other general model statistics) directly using `broom::glance()`:

```{r}

glance(m)
```

### Modelling relationships among categorical variables

Sometimes we want to know whether our outcome of interest changes based on the category in which it sits. For example, do levels of support for abortion access differ between Democrats, Republicans, and Independents? Do the number of women elected to parliament change based on whether or not the country has a formal quota? Do the number of civilians targeted in war change based on whether the war is intra- or inter-state?

Let's return to the [American National Election Survey](https://electionstudies.org/) first explored last week. We will focus on that first question: do levels of support for abortion access differ between Democrats, Republicans, and Independents?

We can easily access the 2012 survey through R using the `poliscidata` package.

```         
poliscidata::nes
```

#### Cross tabs

A simple cross tab can provide a nice summary of differences in your outcome of interest across your categories.

For example, let's look at differences in the number of individuals who identified as Democrat, Republican, or Independent who do not support access to abortions, support access with some conditions, with more conditions, or always.

We can use `modelsummary::datasummary_crosstab()` to produce a nicely formatted cross tab of our variables:

```{r}
datasummary_crosstab(abort4 ~ pid_3, data = nes)
```

#### Mean comparison table

We can use mean comparison tables to, well, compare means (in a table). Let's compare the average response to the feeling thermometer (scale from 0 to 100) for the Republican party across parties:

```{r}

nes |>
  group_by(pid_3) |> 
  summarise(
    mean = mean(ft_rep, na.rm = T),
    sd = sd(ft_rep, na.rm = T),
    freq = n()
  )
```

Are these counts or averages meaningfully different from one another? We need some additional tools to answer that question. We will discuss those in the coming weeks.

#### Looking at the whole distribution

We can also visualize the whole distribution of a continuous variable of interest within our categories. Returning to the `gapminder` dataset:

```{r}
ggplot(gapminder::gapminder, aes(x = lifeExp, fill = continent)) + 
  geom_density(alpha = 0.5) + 
  theme_minimal()
```
