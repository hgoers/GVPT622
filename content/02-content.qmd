---
title: "Descriptive Statistics"
toc-depth: 4
execute: 
  warning: false
  message: false
  echo: true
  fig-width: 10
---

## Readings

## Class slides

## Section

### Prerequisites

```{r}
library(tidyverse)
library(poliscidata)
library(wbstats)
library(janitor)
library(skimr)
library(countrycode)
library(ggridges)
library(ggdist)
library(MetBrewer)
library(patchwork)
```

### Describing categorical variables

For this section, we will be working with the [American National Election Survey](https://electionstudies.org/) to explore how to produce useful descriptive statistics for categorical variables using R. The ANES polls annually individual Americans about their political beliefs and behavior. 

We can access the 2012 survey using the `poliscidata` package:

```
poliscidata::nes
```

> Take a look at the many different pieces of information collected via the survey at `?nes`. 

We are interested in understanding how many individuals of different ages took part in the survey. The survey records the age of each respondent within six different brackets in the `dem_age6` variable. 

#### Frequency distribution

We can take advantage of `janitor::tabyl()` to quickly calculate the number and proportion of respondents in each age bracket.

```{r}
tabyl(nes, dem_age6)
```

#### Visualizing this frequency

It is a bit difficult to quickly determine relative counts. Which age bracket has the most respondents? Which has the least? Are these counts very different from each other. 

I highly recommend visualizing your data. You will get a much better sense of it. We can easily visualize this frequency table. I recommend using a bar chart to clearly show relative counts.

```{r}
nes |> 
  tabyl(dem_age6) |> 
  ggplot(aes(x = n, y = dem_age6)) + 
  geom_col() +
  theme_minimal() + 
  labs(
    x = "Count of respondents",
    y = "Age bracket"
  )
```

### Describing continuous variables

For this section, we will be looking at each country's expenditure on education, as a percent of their gross domestic product. We will use the `wbstats::wb_data()` function to collect these data. 

```{r}
perc_edu <- wb_data(
  "SE.XPD.TOTL.GD.ZS", start_date = 2020, end_date = 2020, return_wide = F
) |> 
  mutate(
    value = value / 100,
    region = countrycode(country, "country.name", "region", 
                         custom_match = c("Turkiye" = "Europe & Central Asia"))
  )
```

I have converted these percentages (0 - 100) to proportions (0 - 1) for ease of interpretation. I have also added each country's region (using `countrycode::countrycode()`) so that we can explore regional trends in our data. 

We can get a good sense of how expenditure varied by country looking at the **center**, **spread**, and **shape** of the distribution.

#### Five number summary

We can use `skimr::skim()` to quickly get useful information on our continuous variable.

```{r}
skim(perc_edu$value)
```

The country that spent the least on education as a percent of its GDP in 2020 was `r perc_edu |> slice_min(value) |> pull(country)`, which spent `r perc_edu |> slice_min(value) |> pull(value) |> scales::percent(accuracy = 0.001)`. The country that spent the most in 2020 was the `r perc_edu |> slice_max(value) |> pull(country)`, which spent `r perc_edu |> slice_max(value) |> pull(value) |> scales::percent()`. The average percent of GDP spent on education in 2020 was `r perc_edu |> summarise(avg = mean(value, na.rm = T)) |> pull(avg) |> scales::percent()`. 

This description was a bit unwieldy. To get a better sense of our data, we can visualize it. 

#### Histogram

```{r}
ggplot(perc_edu, aes(x = value)) + 
  geom_histogram() + 
  theme_minimal()
```

> Take a look at `?geom_histogram` to find the arguments needed to change the bin width of your histograms. 

#### Density curves

```{r}
ggplot(perc_edu, aes(x = value)) + 
  geom_density() + 
  theme_minimal()
```

#### Box and whisker plots

```{r}
ggplot(perc_edu, aes(x = value)) + 
  geom_boxplot() + 
  theme_minimal()
```

#### Looking for patterns in our groups

```{r}
ggplot(perc_edu, aes(x = value, y = region)) + 
  geom_boxplot() + 
  theme_minimal()
```

Let's make a very cool grouped density plot using `ggridges::geom_density_ridges()`. 

```{r}
ggplot(perc_edu, aes(x = value, y = region, fill = region)) + 
  geom_density_ridges(jittered_points = T, point_shape = "|", position = position_points_jitter(height = 0)) + 
  scale_fill_brewer(palette = 4) + 
  theme_minimal() + 
  theme(legend.position = "none")
```

#### Understanding distributions

##### Normal distribution

```{r}
tibble(z = rnorm(n = 1000)) |> 
  ggplot(aes(x = z)) + 
  geom_histogram() + 
  theme_minimal()
```

```{r}
tibble(z = rnorm(n = 1e6)) |> 
  ggplot(aes(x = z)) + 
  geom_histogram() + 
  theme_minimal()
```

##### Right skewed distribution

```{r}
tibble(z = rbeta(10000, 2, 10)) |> 
  ggplot(aes(x = z)) + 
  geom_histogram() + 
  theme_minimal()
```

##### Left skewed distribution

```{r}
tibble(z = rbeta(10000, 10, 2)) |> 
  ggplot(aes(x = z)) + 
  geom_histogram() + 
  theme_minimal()
```

#### Measures of central tendency: mean, median, and mode

##### Mean

The **mean** is the average of all values. 

##### Median

The **median** is the mid-point of all values. 

##### Mode

The **mode** is the most frequent of all values. 

##### Using central tendency to describe and understand distributions

Normally distributed vectors share their mean and medians. 

```{r}
norm_dist <- tibble(z = rnorm(n = 1000))

ggplot(norm_dist, aes(x = z)) + 
  geom_histogram() + 
  geom_vline(xintercept = mean(norm_dist$z), colour = "red") + 
  geom_vline(xintercept = median(norm_dist$z), colour = "blue") + 
  theme_minimal()
```

For right skewed data, the mean is greater than the median. 

```{r}
right_dist <- tibble(z = rbeta(10000, 2, 10))

ggplot(right_dist, aes(x = z)) + 
  geom_histogram() + 
  geom_vline(xintercept = mean(right_dist$z), colour = "red") + 
  geom_vline(xintercept = median(right_dist$z), colour = "blue") + 
  theme_minimal()
```

For left skewed data, the mean is smaller than the median. 

```{r}
left_dist <- tibble(z = rbeta(10000, 10, 2))

ggplot(left_dist, aes(x = z)) + 
  geom_histogram() + 
  geom_vline(xintercept = mean(left_dist$z), colour = "red") + 
  geom_vline(xintercept = median(left_dist$z), colour = "blue") + 
  theme_minimal()
```

#### Measures of spread: range, variance, and standard deviation

##### Range

The **range** is the difference between the largest and smallest value.

```{r}
max(perc_edu$value, na.rm = T) - min(perc_edu$value, na.rm = T)
```

##### Variance

The **variance** measures how spread out your values are. Take a look at these two plots. Both have the same center point (0) and number of observations (1,000,000). However, the data are much more spread out around that center point in the top graph. 

```{r}
wide_dist <- tibble(z = rnorm(1e6, sd = 2))

p1 <- ggplot(wide_dist, aes(x = z)) + 
  geom_histogram() + 
  geom_vline(xintercept = 0) + 
  theme_minimal() + 
  scale_x_continuous(limits = c(-4, 4))

narrow_dist <- tibble(z = rnorm(1e6, sd = 1))

p2 <- ggplot(narrow_dist, aes(x = z)) + 
  geom_histogram() + 
  geom_vline(xintercept = 0) + 
  theme_minimal() + 
  scale_x_continuous(limits = c(-4, 4))

p1 / p2
```

The data in the top graph have far more variance than those in the bottom graph. We measure this by calculating the *average of the squares of the deviations of the observations from their mean*.

$$
s^2 = \frac{\Sigma(x_i - \bar{x})^2}{n - 1}
$$

Let's step through this. We will first calculate the variance for `wide_dist`, or the top graph. 

```{r}
wide_var_calc <- wide_dist |> 
  mutate(
    mean = mean(wide_dist$z),
    diff = z - mean,
    diff_2 = diff^2
  )

wide_var_calc
```

We take the sum of square of the difference between each observation and the mean of our whole sample. We then divide that by one less than our number of observations.

```{r}
wide_var <- sum(wide_var_calc$diff_2) / (nrow(wide_var_calc) - 1)

wide_var
```

We can compare this to the variance for our narrower distribution. 

```{r}
narrow_var_calc <- narrow_dist |> 
  mutate(
    mean = mean(narrow_dist$z),
    diff = z - mean,
    diff_2 = diff^2
  )

narrow_var <- sum(narrow_var_calc$diff_2) / (nrow(narrow_var_calc) - 1)

narrow_var
```

It is, in fact, smaller!

We can use `var()` to do this in one step: 

```{r}
var(wide_dist)
```

```{r}
var(narrow_dist)
```

##### Standard deviation

A simpler measure of spread is the **standard deviation**. It is simply the square root of the variance. 

```{r}
sqrt(wide_var)
```

```{r}
sqrt(narrow_var)
```

You can get this directly using `sd()`: 

```{r}
sd(wide_dist$z)
```

```{r}
sd(narrow_dist$z)
```

If you look back to our graphs, you will see that I set the standard deviations explicitly when I generated the data. `rnorm()` takes an `sd` argument. This is great because we can confirm that the standard deviations for the wide and narrow distributions are 2 and 1 respectively (with a little bit of noise). 

```{r}
tibble(
  n = rnorm(1e6, sd = 1),
  w = rnorm(1e6, sd = 2)
) |> 
  ggplot() + 
  geom_density(aes(x = n), colour = "green") + 
  geom_density(aes(x = w), colour = "lightblue") + 
  theme_minimal()
```

#### Normal distributions

Remember that normal distributions share a mean and median. This has very cool and useful consequences. 

```{r}
norm_5_2 <- tibble(x = rnorm(n = 1e6, mean = 5, sd = 2))

ggplot(norm_5_2, aes(x = x)) + 
  stat_slab(
    aes(fill_ramp = after_stat(cut_cdf_qi(cdf))), fill = met.brewer("Egypt")[2]
  ) + 
  scale_fill_ramp_discrete(range = c(1, 0.2), guide = "none") + 
  theme_minimal()
```

-   Approximately 68% of the data fall within one standard deviation of the mean. 

-   Approximately 95% of the data fall within two standard deviations of the mean. 

-   Approximately 99.7% of the data fall within three standard deviations of the mean. 

#### Standardization

Notice how our description of each variable depends on its units of measurement. What do we do if we want to compare across different measurements that have different units? 

##### Z scores

For normal distributions, we can use the *z score*. This gives us a standard way of understanding **how many standard deviations from the mean of a normally distributed variable a value is**. 

$$
z_i = \frac{x_i - \mu_x}{\sigma_x}
$$

We are just transforming our data. We want to center it around 0 and reshape it so that roughly 68% of the data fall within one standard deviation of the mean, 95% of the data fall within two standard deviations of the mean, and 99.7% of the data fall within three standard deviations of the mean.

Let's standardize our data from above. 

```{r}
standard_5_2 <- norm_5_2 |> 
  mutate(mean = mean(x),
         sd = sd(x),
         z_score = (x - mean) / sd)

head(standard_5_2)
```

We can confirm this: 

```{r}
ggplot(standard_5_2, aes(x = z_score)) + 
  stat_slab(
    aes(fill_ramp = after_stat(cut_cdf_qi(cdf))), fill = met.brewer("Egypt")[2]
  ) + 
  scale_fill_ramp_discrete(range = c(1, 0.2), guide = "none") + 
  theme_minimal()
```
