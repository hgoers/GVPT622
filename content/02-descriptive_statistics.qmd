---
title: "Descriptive Statistics"
toc-depth: 4
execute: 
  warning: false
  message: false
  echo: true
  fig-width: 10
server: shiny
---

## Readings

### Class

{{< fa book >}} Pollock & Edwards, Chapters 1-2

### Lab

{{< fa book >}} Pollock & Edwards R Companion, Chapters 2-3

## Section

### Prerequisites

```{r}
#| echo: false

library(scales)
```

```{r}
#| eval: false

install.packages(c("tidyverse",
                   "poliscidata",
                   "wbstats",
                   "janitor",
                   "skimr",
                   "countrycode",
                   "ggridges",
                   "ggdist",
                   "MetBrewer",
                   "patchwork"))
```

```{r}
library(tidyverse)
library(poliscidata)
library(wbstats)
library(janitor)
library(skimr)
library(countrycode)
library(ggridges)
library(ggdist)
library(MetBrewer)
library(patchwork)
```

### How will what you learn this week help your research?

You have an interesting question that you want to explore. You have some data that relate to that question. Included in these data are information on your outcome of interest and information on the things that you think determine or shape that outcome. You think that one (or more) of the drivers is particularly important, but no one has yet written about it or proven its importance. Brilliant! What do you do now?

The first step in any empirical analysis is getting to know your data. I mean, *really* getting to know your data. You want to dig into it with a critical eye. You want to understand any patterns lurking beneath the surface.

Ultimately, you want to get a really good understanding of the **data generation process**. This process can be thought of in two different and important ways. First, you want to understand how, out there in the real world, your outcome and drivers come to be. For example, if you are interested in political voting patterns, you want to know the nitty gritty process of how people actually vote. Do they have to travel long distances, stand in long queues, fill out a lot of paperwork? Are there age restrictions on their ability to vote? Are there more insidious restrictions that might suppress voting for one particular group in the electorate?

You can use the skills we will discuss this week to help you answer these questions. For example, you can determine whether there are relatively few young voters compared to older voters. Then you can explore why this might be. In turn, your growing expertise in and understanding of the data generation process should inform your exploration of the data. You might note that people have to wait in long queues on a Tuesday to vote. Does this impact the number of workers vs. retirees who vote?

Now, this is made slightly more tricky by the second part of your exploration. You need to understand how your variables are actually measured. How do we know who turns out to vote? Did you get access to the voter file, which records each individual who voted and some interesting and potentially relevant demographic information about them? Or are you relying on exit polls, that only include a portion of those who voted? Were the people included in the polls reflective of the total voting population? What or whom is missing from this survey? Of course, if your sample is not representative, you might find some patterns that appear to be very important to your outcome of interest but are, in fact, just an artifact of a poorly drawn sample.

This week you will be introduced to the first part of this process: data exploration. We use descriptive statistics to *describe patterns in our data*. These are incredibly powerful tools that will arm you with an intimate knowledge of the shape of your variables of interest. With this knowledge, you will be able to start to answer your important question and potentially identify new ones. You will also be able to sense-check your more complex models and pick up on odd or incorrect relationships that they may find.

As you make your frequency tables and histograms and very elaborate dot plots and box charts, keep in mind that these tools are useful for your interrogation of the data generation process. *Be critical.* Continue to ask whether your data allow you to detect true relationships between your variables of interest. Build your intuition for what is really going on and what factors are really driving your outcome of interest.

Let's get started.

### Describing your data

Descriptive statistics can be used to understand single variables and how any number of variables of interest relate to one another. The level of complexity involved in understanding these data and their relationships tend to increase with the number of variables you include. Today, we will look at single variables (for example, the age of respondents to a survey or how much each country spends on education in a year). Next week we will explore the relationship between two variables (for example, an individual's party affiliation and their level of support for abortion access).

Your variables are defined in terms of a **unit of observation or analysis**. These could include individuals, households, congressional districts, states, or countries. **The unit of analysis you adopt should be relevant to your theory.** For example, if you are interested in understanding which individuals are more susceptable to the different strategies rebel groups use to recruit individuals to fight, you probably want individual-level data. If you are interested in determining what drives countries to war with one another, you probably want country-level data (or leader-level data, or voter-level data?).

The important take-away here is that you should start with your theory. You should build a data set that reflects that theory.

### Different types of variables

A variable is an empirical measurement of a characteristic of each observation. The ways of describing your data depend on the type of variable. You can have categorical or continuous variables. **Categorical variables** are discrete. They can be unordered (**nominal**) - for example, the colour of cars - or ordered (**ordinal**) - for example, whether you strongly dislike, dislike, are neutral about, like, or strongly like Taylor Swift.

::: callout-note
**Dichotomous (or binary) variables** are a special type of categorical variable. They take on one of two values. For example: yes or no; at war or not at war; is a Swifty, or is not a Swifty.
:::

**Continuous variables** are, well, continuous. For example, your height or weight, a country's GDP or population, or the number of fatalities in a battle.

::: callout-note
Continuous variables can be made into (usually ordered) categorical variables. This process is called **binning**. For example, you can take individuals' ages and reduce them to 0 - 18 years old, 18 - 45 years old, 45 - 65 years old, and 65+ years old.

You lose information in this process: you cannot go from 45 - 65 years old back to the individuals' precise age. In other words, you cannot go from a categorical to continuous variable.
:::

Let's take a look at how you can describe these different types of variables in turn, using real-world political science examples.

### Describing categorical variables

Simply put, for categorical variables we are interested in the count and/or percentage of cases that fall into each category.

::: {.callout-note collapse="true"}
Later, we will ask interesting questions using these summaries. These include whether differences between the counts and/or percentages of cases that fall into each category are meaningfully (and/or statistically significantly) different from one another. This deceptively simple question serves as the foundation for a lot of political science (particularly comparative) research.
:::

::: callout-note
We can represent categorical variables numerically whilest still letting R know not to treat them as continuous variables. We use [factors](https://forcats.tidyverse.org/) to do this.
:::

#### Case study: American National Election Survey

For this section, we will be working with the [American National Election Survey](https://electionstudies.org/) to explore how to produce useful descriptive statistics for categorical variables using R. The ANES polls annually individual Americans about their political beliefs and behavior.

We can access the 2012 survey using the `poliscidata` package:

```{r}
#| eval: false

poliscidata::nes
```

::: {.callout-caution title="Exercise"}
Take a look at the many different bits of information collected about each respondent using `?nes`.
:::

Let's look at how many individuals of different ages took part in the survey in 2012. The survey records the age of each respondent within six different brackets and reports that information in the `dem_age6` variable.

#### Categorical variables as factors

Remember, there are many different data types that R recognizes. These include characters (`"A"`, `"B"`, `"C"`), integers (`1`, `2`, `3`), and logical values (`TRUE` or `FALSE`).

Factors are another data type. R uses factors to deal with categorical variables.

To illustrate, let's look at the `dem_age6` variable in the `nes` data set. This value takes on six different values, recording the age range into which the respondent to the NES falls. These six categories are:

```{r}
distinct(nes, dem_age6)
```

This is an ordinal categorical variable. It is discrete and ordered. We can take a look at the variable itself using the helpful `skimr::skim()` function:

```{r}
skimr::skim(nes$dem_age6)
```

Happily, the authors of the `poliscidata` package have already converted this variable into a factor. However, they have left it unordered (see the `ordered` variable in the printed out summary). We can easily convert this into an ordered factor:

```{r}
nes <- mutate(nes, dem_age6 = factor(dem_age6, ordered = T, levels = c("17-29",
                                                                       "30-39", 
                                                                       "40-49", 
                                                                       "50-59",
                                                                       "60-69",
                                                                       "70-older")))

skimr::skim(nes, dem_age6)
```

Here, we used the `ordered` argument to tell R that this factor is ordinal. We then used the `levels` argument to tell R the order of the categories.

::: callout-tip
By ensuring that R understands which variables are categorical, we prevent it from giving us nonsense statistics (such as a mean or median for our categories).
:::

#### Frequency distribution

We can take advantage of `janitor::tabyl()` to quickly calculate the number and proportion of respondents in each age bracket.

```{r}
tabyl(nes, dem_age6)
```

::: callout-tip
`valid_percent` provides the proportion of respondents in each age bracket **with missing values removed from the denominator**. For example, the NES survey had `r nes |> nrow() |> comma()` respondents in 2012, but only `r nes |> drop_na(dem_age6) |> nrow() |> comma()` of them provided their age. `r nes |> filter(dem_age6 == "17-29") |> nrow()` responded that they are 17 - 29 years old. Therefore, the `17-29` proportion (which is bounded by 0 and 1, whereas percents are bounded by 0 and 100) is `r nes |> filter(dem_age6 == "17-29") |> nrow()` / `r nes |> nrow() |> comma()` and its valid proportion is `r nes |> filter(dem_age6 == "17-29") |> nrow()` / `r nes |> drop_na(dem_age6) |> nrow() |> comma()`.
:::

#### Visualizing this frequency

It is a bit difficult to quickly determine relative counts. Which age bracket has the most respondents? Which has the least? Are these counts very different from each other.

I highly recommend visualizing your data. You will get a much better sense of it. We can easily visualize this frequency table. I recommend using a bar chart to show clearly relative counts.

```{r}
nes |> 
  tabyl(dem_age6) |> 
  ggplot(aes(x = n, y = dem_age6)) + 
  geom_col() +
  geom_text(aes(label = n), hjust = -0.5) + 
  theme_minimal() + 
  labs(
    x = "Count of respondents",
    y = "Age bracket"
  ) + 
  scale_x_continuous(limits = c(0, 1500))
```

::: callout-tip
You can use `geom_text()` to add labels to your plots. Remember to include `label` as a `aes()` argument to specify which variable you would like to use as your label. The `hjust` argument provided to the `geom_text()` function moves the label slightly to the right to avoid overlap with the bars. Vertical adjustments can be made using the `vjust` argument.
:::

::: callout-tip
Another useful side-effect of properly specifying our ordinal categorical variable as an ordered factor is that it will correctly order our plots according to the order of our variables.
:::

### Describing continuous variables

We need to treat continuous variables differently from categorical ones because they cannot be meaningfully bound together and compared. For example, imagine making a frequency table or bar chart that counts the number of countries with each observed GDP. You would have 193 different counts of one. Not very helpful!

We can get a much better sense of our continuous variables by looking at characteristics of the **distribution** of these variables across the range of all possible values they could take on. Phew! Let's make sense of this using some real-world data.

#### Case study: Comparing countries' spending on education

For this section, we will look at each country's spending on education as a percent of its gross domestic product. We will use `wbstats::wb_data()` to collect these data.

```{r}
perc_edu <- wb_data(
  "SE.XPD.TOTL.GD.ZS", start_date = 2020, end_date = 2020, return_wide = F
) |> 
  transmute(
    country, 
    region = countrycode(country, "country.name", "region"),
    year = date, 
    value = value / 100
  )

perc_edu
```

I have converted these percentages (0 - 100) to proportions (0 - 1) for ease of interpretation. I have also added each country's region (using `countrycode::countrycode()`) so that we can explore regional trends in our data.

We can get a good sense of how expenditure varied by country by looking at the **center**, **spread**, and **shape** of the distribution.

#### Histogram

```{r}
ggplot(perc_edu, aes(x = value)) + 
  geom_histogram() + 
  theme_minimal() + 
  labs(
    x = "Expenditure on education as a proportion of GDP",
    y = "Count"
  ) + 
  scale_x_continuous(labels = label_percent())
```

::: callout-tip
The `scales` package includes a lot of helpful functions that allow you to format numbers. Here, I am using `scales::label_percent()` to convert our raw proportions into the percentage format on the x-axis of the plot.
:::

::: {.callout-caution title="Exercise"}
Take a look at `?geom_histogram` to find the arguments needed to change the bin width of your histograms.
:::

#### Density curves

```{r}
ggplot(perc_edu, aes(x = value)) + 
  geom_density() + 
  theme_minimal() + 
  labs(
    x = "Expenditure on education as a proportion of GDP",
    y = "Density"
  ) + 
  scale_x_continuous(labels = label_percent())
```

#### Understanding distributions

Because continuous variables are best described using their distribution, we can use the shape of that distribution to better understand our individual variables and compare them to others. Is the distribution symmetric or skewed? Where are the majority of observations clustered? Are there multiple distinct clusters, or high points, in the distribution?

##### Normal distribution

```{r}
tibble(x = rnorm(n = 1e6)) |> 
  ggplot(aes(x = x)) + 
  geom_histogram() + 
  theme_minimal()
```

##### Right skewed distribution

```{r}
tibble(x = rbeta(1e6, 1, 10)) |> 
  ggplot(aes(x = x)) + 
  geom_histogram() + 
  theme_minimal()
```

##### Left skewed distribution

```{r}
tibble(x = rbeta(1e6, 10, 1)) |> 
  ggplot(aes(x = x)) + 
  geom_histogram() + 
  theme_minimal()
```

#### Measures of central tendency: mean, median, and mode

We can also use **measures of central tendency** to quickly describe and compare our variables.

##### Mean

The **mean** is the average of all values:

$$
\bar{x} = \frac{\Sigma x_i}{n}
$$

In other words, add all of your values together and then divide that total by the number of values you have.

In R:

```{r}
mean(perc_edu$value, na.rm = T) |> 
  percent()
```

::: callout-tip
If you do not use the argument `na.rm` (read NA remove!), you will get an `NA` if any exist in your vector of values. This is a good default! You should be very aware of missing data points.
:::

##### Median

The **median** is the mid-point of all values.

To calculate it, put all of your values in order from smallest to largest. Identify the value in the middle. That's your median.

In R:

```{r}
median(perc_edu$value, na.rm = T) |> 
  percent()
```

##### Mode

The **mode** is the most frequent of all values.

To calculate it, count how many times each value occurs in your data set. The one that occurs the most is your mode.

::: callout-note
This is usually a more useful summary statistic for categorical variables than continuous ones. For example, which colour of car is most popular? Which political party has the most members?
:::

In R:

```{r}
x <- c(1, 1, 2, 4, 5, 32, 5, 1, 10, 3, 4, 6, 10)

table(x)
```

##### Using central tendency to describe and understand distributions

Normally distributed values *have the same [mean]{style="color:#EEABC4;"} and [median]{style="color:#4582EC;"}*.

```{r}
norm_dist <- tibble(x = rnorm(n = 1e6))

ggplot(norm_dist, aes(x = x)) + 
  geom_histogram() + 
  geom_vline(xintercept = mean(norm_dist$x), colour = "#EEABC4", size = 2) + 
  geom_vline(xintercept = median(norm_dist$x), colour = "#4582EC", size = 2) + 
  theme_minimal()
```

For right skewed data, *the [mean]{style="color:#EEABC4;"} is greater than the [median]{style="color:#4582EC;"}*.

```{r}
right_dist <- tibble(x = rbeta(1e6, 2, 10))

ggplot(right_dist, aes(x = x)) + 
  geom_histogram() + 
  geom_vline(xintercept = mean(right_dist$x), colour = "#EEABC4", size = 2) + 
  geom_vline(xintercept = median(right_dist$x), colour = "#4582EC", size = 2) + 
  theme_minimal()
```

For left skewed data, *the [mean]{style="color:#EEABC4;"} is smaller than the [median]{style="color:#4582EC;"}*.

```{r}
left_dist <- tibble(x = rbeta(1e6, 10, 2))

ggplot(left_dist, aes(x = x)) + 
  geom_histogram() + 
  geom_vline(xintercept = mean(left_dist$x), colour = "#EEABC4", size = 2) + 
  geom_vline(xintercept = median(left_dist$x), colour = "#4582EC", size = 2) + 
  theme_minimal()
```

::: {.callout-caution title="Exercise"}
How is the percentage spent on education data skewed?
:::

#### Five number summary

As you can see, we are attempting to summarise our continuous data to give us a meaningful but manageable sense of it. Means and medians are useful for continuous data.

We can provide more context to our understanding using more summary statistics. A common approach is the **five number summary**. This includes:

-   The smallest value;

-   The 25th percentile value, or the median of the lower half of the data;

-   The median;

-   The 75th percentile value, or the median of the upper half of the data;

-   The largest value.

We can use `skimr::skim()` to quickly get useful information about our continuous variable.

```{r}
skim(perc_edu$value)
```

We have `r nrow(perc_edu)` rows (because our unit of observation is a country, we can read this as 217 countries). We are missing education spending values for `r skim(perc_edu$value) |> pull(n_missing)` of those countries (see `n_missing`), giving us a complete rate of `r skim(perc_edu$value) |> pull(complete_rate) |> percent()` (see `complete_rate`).

The country that spent the least on education as a percent of its GDP in 2020 was `r perc_edu |> slice_min(value) |> pull(country)`, which spent `r skim(perc_edu$value) |> pull(numeric.p0) |> percent(accuracy = 0.1)` (see `p0`). The country that spent the most was the `r perc_edu |> slice_max(value) |> pull(country)`, which spent `r skim(perc_edu$value) |> pull(numeric.p100) |> percent(accuracy = 0.1)` (see `p100`). The average percent of GDP spent on education in 2020 was `r skim(perc_edu$value) |> pull(numeric.mean) |> percent(accuracy = 0.1)` (see `mean`) and the median was `r skim(perc_edu$value) |> pull(numeric.p50) |> percent(accuracy = 0.1)` (see `p50`).

This description was a bit unwieldy. To get a better sense of our data, we can visualize it.

#### Box plots

Box plots (sometimes referred to as box and whisker plots) visualize the five number summary (with bonus features) nicely.

```{r}
ggplot(perc_edu, aes(x = value)) + 
  geom_boxplot() + 
  theme_minimal() + 
  theme(
    axis.text.y = element_blank()
  ) + 
  labs(
    x = "Expenditure on education as a proportion of GDP",
    y = NULL
  ) + 
  scale_x_continuous(labels = label_percent())
```

Note that some values are displayed as dots. The box plot is providing you with a bit more information than the five number summary alone. The box itself displays the 25th percentile, the median, and the 75th percentile values. The tails show you all the data up to a range 1.5 times the **interquartile range (IQR)**, or the 75th percentile minus the 25th percentile. If the smallest or largest values fall below or above (respectively) 1.5 times the IQR, the tail ends at that value. If, however, these values fall outside that range, they are displayed as dots. These are (very rule of thumb, take with a grain of salt, please rely on your theory and data generation process instead!) candidates for **outliers**.

#### Outliers

**Outliers** fall so far away from the majority of the other values that they should be examined closely and perhaps excluded from your analysis. Outliers can distort your mean. They do not, however, distort your median.

::: callout-note
We will talk more about how to deal with outliers later in the course.
:::

#### Measures of spread: range, variance, and standard deviation

We now have a good sense of some of the features of our data. Another useful thing to know is the shape of the distribution. Here, **measures of spread** are useful.

##### Range

The **range** is the difference between the largest and smallest value.

$$
range = min - max
$$

```{r}
max(perc_edu$value, na.rm = T) - min(perc_edu$value, na.rm = T)
```

##### Variance

The **variance** measures how spread out your values are. On average, how far are your observations from the mean? Take a look at these two plots. Both have the same center point (0) and number of observations (1,000,000). However, the data are much more spread out around that center point in the top graph.

```{r}
wide_dist <- tibble(x = rnorm(1e6, sd = 2))

p1 <- ggplot(wide_dist, aes(x = x)) + 
  geom_histogram() + 
  geom_vline(xintercept = 0) + 
  theme_minimal() + 
  scale_x_continuous(limits = c(-4, 4))

narrow_dist <- tibble(x = rnorm(1e6, sd = 1))

p2 <- ggplot(narrow_dist, aes(x = x)) + 
  geom_histogram() + 
  geom_vline(xintercept = 0) + 
  theme_minimal() + 
  scale_x_continuous(limits = c(-4, 4))

p1 / p2
```

The data in the top graph have higher variance (are more spread out) than those in the bottom graph. We measure this by calculating the *average of the squares of the deviations of the observations from their mean*.

$$
s^2 = \frac{\Sigma(x_i - \bar{x})^2}{n - 1}
$$

Let's step through this. We will first calculate the variance for `wide_dist`, or the top graph. To do this:

1.  Calculate the mean of your values.

2.  Calculate the difference between each individual value and that mean (how far from the mean is every value?).

3.  Square those differences.

::: callout-tip
We do not care whether the value is higher or lower than the mean. We only care how far from the mean it is. Squaring a value removes its sign (positive or negative) allowing us to concentrate on this difference.
:::

4.  Add all of those squared differences to get a single number.

5.  Divide that single number by the number of observations you have minus 1.

You now have your variance!

In R:

```{r}
wide_dist_mean <- mean(wide_dist$x)

wide_var_calc <- wide_dist |> 
  mutate(
    mean = wide_dist_mean,
    diff = x - mean,
    diff_2 = diff^2
  )

wide_var_calc
```

We the add those squared differences between each observation and the mean of our whole sample together. Finally, we divide that by one less than our number of observations.

```{r}
wide_var <- sum(wide_var_calc$diff_2) / (nrow(wide_var_calc) - 1)

wide_var
```

We can compare this to the variance for our narrower distribution.

```{r}
narrow_var_calc <- narrow_dist |> 
  mutate(
    mean = mean(narrow_dist$x),
    diff = x - mean,
    diff_2 = diff^2
  )

narrow_var <- sum(narrow_var_calc$diff_2) / (nrow(narrow_var_calc) - 1)

narrow_var
```

It is, in fact, smaller!

That was painful. Happily we can use `var()` to do this in one step:

```{r}
var(wide_dist)
```

```{r}
var(narrow_dist)
```

```{r}
var(wide_dist) > var(narrow_dist)
```

##### Standard deviation

A simpler measure of spread is the **standard deviation**. It is simply the square root of the variance.

```{r}
sqrt(wide_var)
```

```{r}
sqrt(narrow_var)
```

You can get this directly using `sd()`:

```{r}
sd(wide_dist$x)
```

```{r}
sd(narrow_dist$x)
```

If you look back to our graphs, you will see that I set the standard deviations explicitly when I generated the data: `rnorm()` takes an `sd` argument. This is great because we can confirm that the standard deviations for the wide and narrow distributions are 2 and 1 respectively (with a little bit of randomnoise).

```{r}
tibble(
  n = rnorm(1e6, sd = 1),
  w = rnorm(1e6, sd = 2)
) |> 
  ggplot() + 
  geom_density(aes(x = n), colour = "green", size = 2) + 
  geom_density(aes(x = w), colour = "lightblue", size = 2) + 
  theme_minimal()
```

::: callout-tip
Remember that the standard deviation is a measure of how spread out our data are. Therefore, data with no spread (are all the exact same number) will have a standard deviation of 0.
:::

#### Normal distributions

Remember that normal distributions share a mean and median. This has very cool and useful side effects.

Let's explore these with some simulated data that have a mean of 5 and a standard deviation of 2.

```{r}
norm_5_2 <- tibble(x = rnorm(n = 1e6, mean = 5, sd = 2))

head(norm_5_2)
```

::: callout-note
Because we are taking random draws of numbers using `rnorm()`, you will get a different set of numbers to me if you run this command. They will still be centered around a mean of 5 and have a standard deviation of 2.
:::

Let's plot these numbers:

```{r}
ggplot(norm_5_2, aes(x = x)) + 
  stat_slab(
    aes(fill_ramp = after_stat(cut_cdf_qi(cdf))), fill = met.brewer("Egypt")[2]
  ) + 
  scale_fill_ramp_discrete(range = c(1, 0.2), guide = "none") + 
  theme_minimal()
```

Because these numbers are normally distributed (share a mean and median), the following are true:

-   Approximately 68% of the data fall within one standard deviation of the mean (the dark blue shaded area);

-   Approximately 95% of the data fall within two standard deviations of the mean (the medium blue shaded area);

-   Approximately 99.7% of the data fall within three standard deviations of the mean (the light blue shaded area).

::: {.callout-caution title="Exercise"}
Repeat this process with normally distributed data centered around different means and with different standard deviations.
:::

#### Standardization

Notice how our description of each variable depends on its units of measurement. What do we do if we want to compare across different measurements that have different units?

##### Z scores

For normal distributions, we can use the *z score*. This gives us a standard way of understanding **how many standard deviations from the mean of a normally distributed variable a value is**.

$$
z_i = \frac{x_i - \mu_x}{\sigma_x}
$$

We are just transforming our data. We want to center it around 0 and reshape it so that roughly 68% of the data fall within one standard deviation of the mean, 95% of the data fall within two standard deviations of the mean, and 99.7% of the data fall within three standard deviations of the mean.

Let's standardize our data from above.

```{r}
standard_5_2 <- norm_5_2 |> 
  mutate(mean = mean(x),
         sd = sd(x),
         z_score = (x - mean) / sd)

head(standard_5_2)
```

We can confirm this:

```{r}
ggplot(standard_5_2, aes(x = z_score)) + 
  stat_slab(
    aes(fill_ramp = after_stat(cut_cdf_qi(cdf))), fill = met.brewer("Egypt")[2]
  ) + 
  scale_fill_ramp_discrete(range = c(1, 0.2), guide = "none") + 
  theme_minimal() + 
  scale_x_continuous(breaks = seq(-5, 5, 1))
```

We can use the z-score of a value to determine what percentage of values of our normally distributed data are less than that value. To do this, we can either reference a [z table](https://www.z-table.com/), or use the following function in R:

```         
pnorm()
```

What percentage of the simulated data fall below 1.5 standard deviations from the mean? First, let's visualize this question:

```{r}
ggplot(standard_5_2, aes(x = z_score)) +
  stat_halfeye(aes(fill = after_stat(x < 1.5)), .width = c(0.95, 0.68)) + 
  theme_minimal() +
  theme(legend.position = "none") + 
  scale_x_continuous(breaks = seq(-5, 5, 1)) + 
  labs(x = "Z-Score",
       y = "Density",
       caption = "Median and mean shown with point. One and two standard deviations are shown by the black bars.")
  
```

Next, we can use the function above to work out the precise percentage of the data that fall below this value:

```{r}
pnorm(1.5)
```

`r pnorm(1.5) |> percent(accuracy = 0.01)` of the data fall below 1.5 standard deviations above the mean of our simulated data.

::: {.callout-caution title="Exercise"}
Pick a different number of standard deviations from the mean and have a go at visualizing and calculating the percentage of the data that fall below that value.
:::

::: callout-note
To work out the percentage of the data that fall above that value, you simply subtract the proportion that falls below from 1 (or the percentage subtracted from 100%).

What percentage of the data are greater than 1.5 standard deviations from the mean? `1 - pnorm(1.5)` or `r 1 - round(pnorm(1.5), 3)`.
:::

### Next week

A focus on techniques for examining relationships between variables.
